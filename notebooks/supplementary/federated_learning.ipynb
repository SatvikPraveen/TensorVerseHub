{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"TFF version: {tff.__version__}\")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b09563",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Federated Learning Fundamentals\n",
    "\n",
    "### Traditional vs. Federated Learning\n",
    "\n",
    "**Centralized Learning:**\n",
    "- All data collected to central server\n",
    "- Train on complete dataset\n",
    "- Privacy concerns: data exposure\n",
    "- Communication: One-way (download model)\n",
    "\n",
    "**Federated Learning:**\n",
    "- Data stays on client devices\n",
    "- Train locally on each client\n",
    "- Aggregate updates at server\n",
    "- Privacy: Only model updates shared\n",
    "- Communication: Model gradients exchanged\n",
    "\n",
    "### Federated Averaging (FedAvg) Algorithm\n",
    "\n",
    "1. **Server initialization:** Send initial model to all clients\n",
    "2. **Client training:** Each client trains locally on own data\n",
    "3. **Model aggregation:** Server averages weight updates\n",
    "4. **Update distribution:** Send aggregated model back to clients\n",
    "5. **Repeat:** Multiple communication rounds\n",
    "\n",
    "### Key Challenges\n",
    "- **Non-IID Data:** Clients have different data distributions\n",
    "- **Communication Cost:** Limited bandwidth between clients and server\n",
    "- **Privacy:** Protecting individual client data\n",
    "- **Convergence:** Proving theoretical convergence guarantees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396fe04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate federated learning dataset\n",
    "def generate_federated_dataset(n_clients: int = 10, samples_per_client: int = 100,\n",
    "                              feature_dim: int = 20, n_classes: int = 3,\n",
    "                              non_iid: bool = True):\n",
    "    \"\"\"\n",
    "    Generate federated learning dataset.\n",
    "    \n",
    "    Args:\n",
    "        n_clients: Number of clients\n",
    "        samples_per_client: Samples per client\n",
    "        feature_dim: Feature dimension\n",
    "        n_classes: Number of classes\n",
    "        non_iid: Whether data is non-IID across clients\n",
    "    \n",
    "    Returns:\n",
    "        List of (X, y) tuples for each client\n",
    "    \"\"\"\n",
    "    client_data = []\n",
    "    \n",
    "    if non_iid:\n",
    "        # Non-IID: Each client specializes in 1-2 classes\n",
    "        for client_id in range(n_clients):\n",
    "            # Client specializes in specific classes\n",
    "            assigned_classes = [\n",
    "                client_id % n_classes,\n",
    "                (client_id + 1) % n_classes\n",
    "            ]\n",
    "            \n",
    "            X = np.random.randn(samples_per_client, feature_dim)\n",
    "            y = np.random.choice(assigned_classes, samples_per_client)\n",
    "            \n",
    "            # Add class-specific signal\n",
    "            for class_id in assigned_classes:\n",
    "                mask = y == class_id\n",
    "                X[mask] += class_id * 2\n",
    "            \n",
    "            client_data.append((X.astype(np.float32), y.astype(np.int32)))\n",
    "    else:\n",
    "        # IID: Random samples for each client\n",
    "        for _ in range(n_clients):\n",
    "            X = np.random.randn(samples_per_client, feature_dim).astype(np.float32)\n",
    "            y = np.random.randint(0, n_classes, samples_per_client).astype(np.int32)\n",
    "            client_data.append((X, y))\n",
    "    \n",
    "    return client_data\n",
    "\n",
    "# Generate federated dataset\n",
    "n_clients = 10\n",
    "client_data = generate_federated_dataset(n_clients=n_clients, non_iid=True)\n",
    "\n",
    "print(f\"‚úÖ Federated Dataset Generated\")\n",
    "print(f\"Number of clients: {n_clients}\")\n",
    "print(f\"Samples per client: {len(client_data[0][0])}\")\n",
    "print(f\"Feature dimension: {client_data[0][0].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efec774",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Centralized Model Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2626d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(feature_dim: int = 20, n_classes: int = 3):\n",
    "    \"\"\"Create a simple Keras model.\"\"\"\n",
    "    return keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(feature_dim,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "# Centralized training baseline\n",
    "print(\"üöÄ Training Centralized Model...\\n\")\n",
    "\n",
    "# Combine all client data\n",
    "all_X = np.concatenate([X for X, _ in client_data])\n",
    "all_y = np.concatenate([y for _, y in client_data])\n",
    "\n",
    "# Create and train model\n",
    "central_model = create_keras_model()\n",
    "central_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "central_history = central_model.fit(\n",
    "    all_X, all_y,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "central_val_acc = central_history.history['val_sparse_categorical_accuracy'][-1]\n",
    "print(f\"\\n‚úÖ Centralized Model Training Complete\")\n",
    "print(f\"Final validation accuracy: {central_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add0ea67",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Federated Learning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d392c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create federated datasets\n",
    "def create_tf_dataset(X, y, batch_size=20):\n",
    "    \"\"\"Convert numpy arrays to tf.data.Dataset.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    return dataset.batch(batch_size).repeat()\n",
    "\n",
    "# Create federated data for TFF\n",
    "federated_train_data = [\n",
    "    create_tf_dataset(X, y, batch_size=20)\n",
    "    for X, y in client_data\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Federated datasets created\")\n",
    "print(f\"Number of client datasets: {len(federated_train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated model simulation (simplified without full TFF)\n",
    "class FederatedLearner:\n",
    "    \"\"\"Simplified Federated Learning implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_clients: int, model_fn):\n",
    "        self.n_clients = n_clients\n",
    "        self.model_fn = model_fn\n",
    "        self.global_model = model_fn()\n",
    "        self.global_model.compile(\n",
    "            optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "            metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    "        )\n",
    "    \n",
    "    def client_update(self, client_model, X, y, epochs=1):\n",
    "        \"\"\"\n",
    "        Perform local training on client.\n",
    "        \n",
    "        Returns:\n",
    "            Updated weights\n",
    "        \"\"\"\n",
    "        client_model.fit(X, y, epochs=epochs, verbose=0)\n",
    "        return client_model.get_weights()\n",
    "    \n",
    "    def aggregate_weights(self, client_weights_list):\n",
    "        \"\"\"\n",
    "        Average weights across clients.\n",
    "        \n",
    "        Returns:\n",
    "            Aggregated weights\n",
    "        \"\"\"\n",
    "        # Federated averaging\n",
    "        n_layers = len(client_weights_list[0])\n",
    "        aggregated_weights = []\n",
    "        \n",
    "        for layer_idx in range(n_layers):\n",
    "            layer_weights = [w[layer_idx] for w in client_weights_list]\n",
    "            avg_weight = np.mean(layer_weights, axis=0)\n",
    "            aggregated_weights.append(avg_weight)\n",
    "        \n",
    "        return aggregated_weights\n",
    "    \n",
    "    def train_round(self, client_data_list, epochs=1):\n",
    "        \"\"\"\n",
    "        Execute one federated learning round.\n",
    "        \"\"\"\n",
    "        client_weights_list = []\n",
    "        \n",
    "        # Local training on each client\n",
    "        for client_idx in range(self.n_clients):\n",
    "            # Create local copy of global model\n",
    "            client_model = self.model_fn()\n",
    "            client_model.set_weights(self.global_model.get_weights())\n",
    "            client_model.compile(\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    "            )\n",
    "            \n",
    "            # Local training\n",
    "            X, y = client_data_list[client_idx]\n",
    "            weights = self.client_update(client_model, X, y, epochs=epochs)\n",
    "            client_weights_list.append(weights)\n",
    "        \n",
    "        # Aggregate weights\n",
    "        aggregated_weights = self.aggregate_weights(client_weights_list)\n",
    "        self.global_model.set_weights(aggregated_weights)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluate global model.\"\"\"\n",
    "        loss, accuracy = self.global_model.evaluate(X_test, y_test, verbose=0)\n",
    "        return accuracy\n",
    "\n",
    "print(\"‚úÖ FederatedLearner class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb0dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train federated model\n",
    "print(\"üöÄ Training Federated Model...\\n\")\n",
    "\n",
    "fl = FederatedLearner(n_clients=n_clients, model_fn=lambda: create_keras_model())\n",
    "\n",
    "# Generate test set\n",
    "X_test = np.random.randn(500, 20).astype(np.float32)\n",
    "y_test = np.random.randint(0, 3, 500).astype(np.int32)\n",
    "# Add signal\n",
    "for i in range(500):\n",
    "    X_test[i] += y_test[i] * 2\n",
    "\n",
    "federated_accuracies = []\n",
    "centralized_accuracies = []\n",
    "\n",
    "# Federated training rounds\n",
    "for round_num in range(20):\n",
    "    # Federated training\n",
    "    fl.train_round(client_data, epochs=1)\n",
    "    fed_acc = fl.evaluate(X_test, y_test)\n",
    "    federated_accuracies.append(fed_acc)\n",
    "    \n",
    "    # Centralized model for comparison\n",
    "    central_model.fit(all_X, all_y, epochs=1, verbose=0)\n",
    "    cent_acc = central_model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "    centralized_accuracies.append(cent_acc)\n",
    "    \n",
    "    if (round_num + 1) % 5 == 0:\n",
    "        print(f\"Round {round_num + 1}/20 | Federated: {fed_acc:.4f} | Centralized: {cent_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Federated Learning Complete\")\n",
    "print(f\"Final federated accuracy: {federated_accuracies[-1]:.4f}\")\n",
    "print(f\"Final centralized accuracy: {centralized_accuracies[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4b9bc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Privacy-Preserving Learning with Differential Privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d570edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentiallyPrivateFL:\n",
    "    \"\"\"Federated Learning with Differential Privacy.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_clients: int, model_fn, noise_multiplier: float = 0.5):\n",
    "        self.n_clients = n_clients\n",
    "        self.model_fn = model_fn\n",
    "        self.noise_multiplier = noise_multiplier\n",
    "        self.global_model = model_fn()\n",
    "        self.global_model.compile(\n",
    "            optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "            metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    "        )\n",
    "    \n",
    "    def add_gaussian_noise(self, weights, noise_scale: float):\n",
    "        \"\"\"\n",
    "        Add Gaussian noise to model weights for differential privacy.\n",
    "        \"\"\"\n",
    "        noisy_weights = []\n",
    "        for w in weights:\n",
    "            noise = np.random.normal(0, noise_scale, w.shape)\n",
    "            noisy_weights.append(w + noise)\n",
    "        return noisy_weights\n",
    "    \n",
    "    def clip_and_aggregate(self, client_weights_list, clip_norm: float = 1.0):\n",
    "        \"\"\"\n",
    "        Clip gradients and aggregate with noise for DP.\n",
    "        \"\"\"\n",
    "        # Compute weight differences (gradients)\n",
    "        reference_weights = self.global_model.get_weights()\n",
    "        \n",
    "        clipped_updates = []\n",
    "        for client_weights in client_weights_list:\n",
    "            update = []\n",
    "            total_norm = 0\n",
    "            \n",
    "            # Compute L2 norm\n",
    "            for ref_w, client_w in zip(reference_weights, client_weights):\n",
    "                diff = client_w - ref_w\n",
    "                total_norm += np.sum(diff ** 2)\n",
    "            total_norm = np.sqrt(total_norm)\n",
    "            \n",
    "            # Clip updates\n",
    "            clip_factor = min(1.0, clip_norm / (total_norm + 1e-10))\n",
    "            \n",
    "            for ref_w, client_w in zip(reference_weights, client_weights):\n",
    "                diff = client_w - ref_w\n",
    "                clipped_diff = diff * clip_factor\n",
    "                update.append(clipped_diff)\n",
    "            \n",
    "            clipped_updates.append(update)\n",
    "        \n",
    "        # Average clipped updates\n",
    "        avg_update = []\n",
    "        for layer_idx in range(len(reference_weights)):\n",
    "            layer_updates = [u[layer_idx] for u in clipped_updates]\n",
    "            avg_layer_update = np.mean(layer_updates, axis=0)\n",
    "            avg_update.append(avg_layer_update)\n",
    "        \n",
    "        # Add Gaussian noise\n",
    "        noise_scale = self.noise_multiplier * clip_norm\n",
    "        noisy_update = self.add_gaussian_noise(avg_update, noise_scale)\n",
    "        \n",
    "        # Apply update\n",
    "        new_weights = [\n",
    "            ref_w + noisy_u\n",
    "            for ref_w, noisy_u in zip(reference_weights, noisy_update)\n",
    "        ]\n",
    "        \n",
    "        return new_weights\n",
    "    \n",
    "    def train_round(self, client_data_list, epochs=1):\n",
    "        \"\"\"Execute DP-FL training round.\"\"\"\n",
    "        client_weights_list = []\n",
    "        \n",
    "        for client_idx in range(self.n_clients):\n",
    "            client_model = self.model_fn()\n",
    "            client_model.set_weights(self.global_model.get_weights())\n",
    "            client_model.compile(\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    "            )\n",
    "            \n",
    "            X, y = client_data_list[client_idx]\n",
    "            client_model.fit(X, y, epochs=epochs, verbose=0)\n",
    "            client_weights_list.append(client_model.get_weights())\n",
    "        \n",
    "        # Clip, aggregate, and add noise\n",
    "        aggregated_weights = self.clip_and_aggregate(client_weights_list, clip_norm=1.0)\n",
    "        self.global_model.set_weights(aggregated_weights)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluate model.\"\"\"\n",
    "        loss, accuracy = self.global_model.evaluate(X_test, y_test, verbose=0)\n",
    "        return accuracy\n",
    "\n",
    "print(\"‚úÖ Differentially Private FL class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DP-FL model\n",
    "print(\"üöÄ Training Differentially Private FL Model...\\n\")\n",
    "\n",
    "dp_fl = DifferentiallyPrivateFL(\n",
    "    n_clients=n_clients,\n",
    "    model_fn=lambda: create_keras_model(),\n",
    "    noise_multiplier=0.1\n",
    ")\n",
    "\n",
    "dp_fl_accuracies = []\n",
    "\n",
    "for round_num in range(20):\n",
    "    dp_fl.train_round(client_data, epochs=1)\n",
    "    acc = dp_fl.evaluate(X_test, y_test)\n",
    "    dp_fl_accuracies.append(acc)\n",
    "    \n",
    "    if (round_num + 1) % 5 == 0:\n",
    "        print(f\"Round {round_num + 1}/20 | DP-FL Accuracy: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ DP-FL Training Complete\")\n",
    "print(f\"Final DP-FL accuracy: {dp_fl_accuracies[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86576b1f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Comparison & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d101e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Convergence comparison\n",
    "ax = axes[0]\n",
    "ax.plot(federated_accuracies, label='Federated Learning', linewidth=2, marker='o')\n",
    "ax.plot(centralized_accuracies, label='Centralized Learning', linewidth=2, marker='s')\n",
    "ax.plot(dp_fl_accuracies, label='DP-Federated Learning', linewidth=2, marker='^')\n",
    "ax.set_xlabel('Communication Round')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Learning Convergence Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Final performance\n",
    "ax = axes[1]\n",
    "methods = ['Centralized', 'Federated', 'DP-Federated']\n",
    "accuracies = [\n",
    "    centralized_accuracies[-1],\n",
    "    federated_accuracies[-1],\n",
    "    dp_fl_accuracies[-1]\n",
    "]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "bars = ax.bar(methods, accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Final Accuracy')\n",
    "ax.set_title('Final Model Performance')\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d2605",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Key Takeaways\n",
    "\n",
    "### Federated Learning Advantages\n",
    "1. **Privacy:** Raw data never leaves client devices\n",
    "2. **Communication Efficiency:** Only model updates transmitted\n",
    "3. **Personalization:** Models can be tailored to local data\n",
    "4. **Decentralization:** No single point of failure\n",
    "5. **Regulatory Compliance:** Easier to meet privacy regulations\n",
    "\n",
    "### Challenges & Solutions\n",
    "\n",
    "| Challenge | Solution |\n",
    "|-----------|----------|\n",
    "| **Non-IID Data** | Use adaptive learning rates, local epochs |\n",
    "| **Communication Cost** | Quantization, compression, selective updates |\n",
    "| **Privacy** | Differential privacy, secure aggregation |\n",
    "| **System Heterogeneity** | Client selection, asynchronous updates |\n",
    "\n",
    "### Differential Privacy\n",
    "- **Mechanism:** Add noise to gradients\n",
    "- **Privacy Budget (Œµ):** Lower = stronger privacy, higher = better accuracy\n",
    "- **Gradient Clipping:** Bound sensitivity of updates\n",
    "- **Noise Multiplier:** Controls noise magnitude\n",
    "\n",
    "### Practical Applications\n",
    "- Mobile keyboard prediction (Gboard)\n",
    "- Medical data analysis\n",
    "- Edge device learning\n",
    "- Personalized recommendation systems\n",
    "- Collaborative learning across organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "üìö Federated Learning - Summary\n",
    "================================\n",
    "\n",
    "‚úÖ Topics Covered:\n",
    "  ‚Ä¢ Federated learning fundamentals\n",
    "  ‚Ä¢ FedAvg algorithm and implementation\n",
    "  ‚Ä¢ Centralized vs. federated comparison\n",
    "  ‚Ä¢ Non-IID data challenges\n",
    "  ‚Ä¢ Differential privacy integration\n",
    "  ‚Ä¢ Gradient clipping and noise addition\n",
    "  ‚Ä¢ Privacy-accuracy tradeoffs\n",
    "\n",
    "üí° Key Insights:\n",
    "  ‚Ä¢ FL enables privacy-preserving collaborative learning\n",
    "  ‚Ä¢ Federated models can match centralized performance\n",
    "  ‚Ä¢ Privacy and utility have inherent tradeoffs\n",
    "  ‚Ä¢ Communication efficiency is critical\n",
    "  ‚Ä¢ Non-IID data requires careful algorithm design\n",
    "\n",
    "üéØ Next Steps:\n",
    "  1. Implement client sampling for heterogeneous systems\n",
    "  2. Add model compression (quantization, pruning)\n",
    "  3. Experiment with asynchronous aggregation\n",
    "  4. Analyze privacy guarantees formally\n",
    "  5. Apply to real federated datasets (FEMNIST, etc.)\n",
    "\n",
    "üîê Privacy Considerations:\n",
    "  ‚Ä¢ Even aggregated updates can leak information\n",
    "  ‚Ä¢ Differential privacy provides formal guarantees\n",
    "  ‚Ä¢ Privacy-utility tradeoff is fundamental\n",
    "  ‚Ä¢ Regular audits recommended for sensitive applications\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
