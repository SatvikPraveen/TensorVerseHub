{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62553069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import Tuple, List, Optional\n",
    "import datetime\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31347f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Time Series Data Preparation\n",
    "\n",
    "### Key Concepts\n",
    "- **Stationarity:** Statistical properties don't change over time\n",
    "- **Normalization:** Scale data to reasonable range\n",
    "- **Sliding Windows:** Create input-output pairs for supervised learning\n",
    "- **Train-Test Split:** Temporal order must be preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7719bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic time series data\n",
    "np.random.seed(42)\n",
    "n_points = 1000\n",
    "t = np.arange(n_points)\n",
    "\n",
    "# Synthetic time series: trend + seasonality + noise\n",
    "trend = t * 0.1\n",
    "seasonality = 10 * np.sin(2 * np.pi * t / 100)\n",
    "noise = np.random.normal(0, 1, n_points)\n",
    "\n",
    "time_series = trend + seasonality + noise\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'time': pd.date_range('2020-01-01', periods=n_points, freq='D'),\n",
    "    'value': time_series\n",
    "})\n",
    "\n",
    "print(\"Time Series Data:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(df['time'], df['value'], linewidth=0.8)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Synthetic Time Series Data')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesPreprocessor:\n",
    "    \"\"\"Preprocessing utilities for time series data.\"\"\"\n",
    "    \n",
    "    def __init__(self, lookback: int = 50, forecast_horizon: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize preprocessor.\n",
    "        \n",
    "        Args:\n",
    "            lookback: Number of historical steps to use\n",
    "            forecast_horizon: Number of steps to forecast\n",
    "        \"\"\"\n",
    "        self.lookback = lookback\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.data_mean = None\n",
    "        self.data_std = None\n",
    "    \n",
    "    def normalize(self, data: np.ndarray, fit: bool = True) -> np.ndarray:\n",
    "        \"\"\"Normalize data to [0, 1].\"\"\"\n",
    "        if fit:\n",
    "            return self.scaler.fit_transform(data.reshape(-1, 1)).flatten()\n",
    "        else:\n",
    "            return self.scaler.transform(data.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    def denormalize(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Reverse normalization.\"\"\"\n",
    "        return self.scaler.inverse_transform(data.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    def create_sliding_windows(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Create sliding window dataset.\n",
    "        \n",
    "        Args:\n",
    "            data: Time series data\n",
    "        \n",
    "        Returns:\n",
    "            X: Input sequences (n_samples, lookback)\n",
    "            y: Target values (n_samples, forecast_horizon)\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        for i in range(len(data) - self.lookback - self.forecast_horizon + 1):\n",
    "            X.append(data[i:i + self.lookback])\n",
    "            y.append(data[i + self.lookback:i + self.lookback + self.forecast_horizon])\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def train_test_split(self, X: np.ndarray, y: np.ndarray, \n",
    "                        test_ratio: float = 0.2) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Split data maintaining temporal order.\"\"\"\n",
    "        split_idx = int(len(X) * (1 - test_ratio))\n",
    "        \n",
    "        X_train = X[:split_idx]\n",
    "        y_train = y[:split_idx]\n",
    "        X_test = X[split_idx:]\n",
    "        y_test = y[split_idx:]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Preprocess data\n",
    "preprocessor = TimeSeriesPreprocessor(lookback=50, forecast_horizon=10)\n",
    "\n",
    "# Normalize\n",
    "normalized_data = preprocessor.normalize(df['value'].values, fit=True)\n",
    "\n",
    "# Create sliding windows\n",
    "X, y = preprocessor.create_sliding_windows(normalized_data)\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"y: {y.shape}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = preprocessor.train_test_split(X, y, test_ratio=0.2)\n",
    "print(f\"\\nTrain set: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test set: X={X_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c17e39e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: LSTM for Time Series Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d82631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(lookback: int, forecast_horizon: int) -> keras.Model:\n",
    "    \"\"\"Build LSTM model for time series forecasting.\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.LSTM(64, return_sequences=True, input_shape=(lookback, 1)),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.LSTM(32, return_sequences=False),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(forecast_horizon)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Reshape data for LSTM (samples, timesteps, features)\n",
    "X_train_lstm = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_lstm = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Build and compile model\n",
    "lstm_model = create_lstm_model(preprocessor.lookback, preprocessor.forecast_horizon)\n",
    "lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(lstm_model.summary())\n",
    "\n",
    "# Train\n",
    "print(\"\\nðŸš€ Training LSTM model...\")\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train_lstm, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=0,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "print(\"âœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM model\n",
    "train_loss = lstm_model.evaluate(X_train_lstm, y_train, verbose=0)\n",
    "test_loss = lstm_model.evaluate(X_test_lstm, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\nðŸ“Š LSTM Model Performance:\")\n",
    "print(f\"Train Loss (MSE): {train_loss[0]:.6f}\")\n",
    "print(f\"Test Loss (MSE): {test_loss[0]:.6f}\")\n",
    "print(f\"Test MAE: {test_loss[1]:.6f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lstm_model.predict(X_test_lstm, verbose=0)\n",
    "\n",
    "# Denormalize predictions\n",
    "y_pred_denorm = preprocessor.denormalize(y_pred.flatten()).reshape(y_pred.shape)\n",
    "y_test_denorm = preprocessor.denormalize(y_test.flatten()).reshape(y_test.shape)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean((y_pred_denorm - y_test_denorm) ** 2))\n",
    "print(f\"Test RMSE (denormalized): {rmse:.4f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# Training history\n",
    "axes[0, 0].plot(lstm_history.history['loss'], label='Train Loss')\n",
    "axes[0, 0].plot(lstm_history.history['val_loss'], label='Val Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('LSTM Training History')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Predictions vs Actual (first sample)\n",
    "sample_idx = 0\n",
    "axes[0, 1].plot(y_test_denorm[sample_idx], 'o-', label='Actual', linewidth=2, markersize=6)\n",
    "axes[0, 1].plot(y_pred_denorm[sample_idx], 's-', label='Predicted', linewidth=2, markersize=6)\n",
    "axes[0, 1].set_xlabel('Forecast Step')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].set_title(f'Sample {sample_idx}: Actual vs Predicted')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Predictions vs Actual (multiple samples)\n",
    "sample_range = range(min(50, len(y_pred_denorm)))\n",
    "axes[1, 0].scatter(y_test_denorm[sample_range].flatten(), \n",
    "                   y_pred_denorm[sample_range].flatten(), alpha=0.6)\n",
    "# Perfect prediction line\n",
    "min_val = min(y_test_denorm[sample_range].min(), y_pred_denorm[sample_range].min())\n",
    "max_val = max(y_test_denorm[sample_range].max(), y_pred_denorm[sample_range].max())\n",
    "axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect')\n",
    "axes[1, 0].set_xlabel('Actual')\n",
    "axes[1, 0].set_ylabel('Predicted')\n",
    "axes[1, 0].set_title('Predictions vs Actual (Multiple Samples)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test_denorm[sample_range].flatten() - y_pred_denorm[sample_range].flatten()\n",
    "axes[1, 1].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Residual')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution of Residuals')\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--', label='Zero Error')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7695f5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Transformer for Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347bbaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"Transformer block for time series.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def create_transformer_model(lookback: int, forecast_horizon: int) -> keras.Model:\n",
    "    \"\"\"Build Transformer model for time series forecasting.\"\"\"\n",
    "    inputs = keras.Input(shape=(lookback, 1))\n",
    "    \n",
    "    x = layers.Dense(64)(inputs)\n",
    "    x = TransformerBlock(embed_dim=64, num_heads=4, ff_dim=128, rate=0.1)(x)\n",
    "    x = TransformerBlock(embed_dim=64, num_heads=4, ff_dim=128, rate=0.1)(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(forecast_horizon)(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Build and compile Transformer model\n",
    "transformer_model = create_transformer_model(preprocessor.lookback, preprocessor.forecast_horizon)\n",
    "transformer_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"\\nTransformer Model Summary:\")\n",
    "print(transformer_model.summary())\n",
    "\n",
    "# Train\n",
    "print(\"\\nðŸš€ Training Transformer model...\")\n",
    "transformer_history = transformer_model.fit(\n",
    "    X_train_lstm, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    verbose=0,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "print(\"âœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f77160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Transformer model\n",
    "train_loss_tf = transformer_model.evaluate(X_train_lstm, y_train, verbose=0)\n",
    "test_loss_tf = transformer_model.evaluate(X_test_lstm, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\nðŸ“Š Transformer Model Performance:\")\n",
    "print(f\"Train Loss (MSE): {train_loss_tf[0]:.6f}\")\n",
    "print(f\"Test Loss (MSE): {test_loss_tf[0]:.6f}\")\n",
    "print(f\"Test MAE: {test_loss_tf[1]:.6f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_tf = transformer_model.predict(X_test_lstm, verbose=0)\n",
    "y_pred_tf_denorm = preprocessor.denormalize(y_pred_tf.flatten()).reshape(y_pred_tf.shape)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_tf = np.sqrt(np.mean((y_pred_tf_denorm - y_test_denorm) ** 2))\n",
    "print(f\"Test RMSE (denormalized): {rmse_tf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d94ce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e828cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training history comparison\n",
    "axes[0].plot(lstm_history.history['val_loss'], label='LSTM', linewidth=2)\n",
    "axes[0].plot(transformer_history.history['val_loss'], label='Transformer', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Validation Loss')\n",
    "axes[0].set_title('Model Comparison: Training Progress')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Performance metrics\n",
    "models = ['LSTM', 'Transformer']\n",
    "losses = [test_loss[0], test_loss_tf[0]]\n",
    "rmses = [rmse, rmse_tf]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "# Normalize RMSE to similar scale for visualization\n",
    "rmse_normalized = [r / max(rmses) * max(losses) for r in rmses]\n",
    "\n",
    "bars1 = axes[1].bar(x - width/2, losses, width, label='MSE Loss', alpha=0.8, color='skyblue')\n",
    "bars2 = axes[1].bar(x + width/2, rmse_normalized, width, label='RMSE (normalized)', alpha=0.8, color='orange')\n",
    "\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_title('Model Performance Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(models)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, loss in zip(bars1, losses):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{loss:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Summary of Model Comparison:\")\n",
    "print(f\"{'Model':<15} {'Test MSE':<15} {'RMSE':<15}\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"{'LSTM':<15} {test_loss[0]:<15.6f} {rmse:<15.4f}\")\n",
    "print(f\"{'Transformer':<15} {test_loss_tf[0]:<15.6f} {rmse_tf:<15.4f}\")\n",
    "print(\"\\nBest Model: \", \"LSTM\" if rmse < rmse_tf else \"Transformer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bdc074",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Key Takeaways\n",
    "\n",
    "### Time Series Forecasting Best Practices\n",
    "\n",
    "1. **Data Preparation**\n",
    "   - Always preserve temporal order in train-test split\n",
    "   - Normalize data to improve convergence\n",
    "   - Use appropriate sliding window size\n",
    "\n",
    "2. **Model Selection**\n",
    "   - LSTM: Good balance of performance and simplicity\n",
    "   - Transformer: Better for long-range dependencies\n",
    "   - Start with LSTM, escalate if needed\n",
    "\n",
    "3. **Evaluation Metrics**\n",
    "   - MAE: Average absolute error (interpretable)\n",
    "   - RMSE: Penalizes large errors more\n",
    "   - MAPE: Percentage error (good for interpretation)\n",
    "\n",
    "4. **Common Challenges**\n",
    "   - Non-stationary data: Use differencing\n",
    "   - Missing values: Interpolate or forward fill\n",
    "   - Multiple seasonality: Use decomposition\n",
    "\n",
    "5. **Advanced Techniques**\n",
    "   - Ensemble methods combining multiple models\n",
    "   - Multi-task learning with related series\n",
    "   - Attention mechanisms for important time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "ðŸ“š Time Series Forecasting - Summary\n",
    "====================================\n",
    "\n",
    "âœ… Topics Covered:\n",
    "  â€¢ Time series data preprocessing\n",
    "  â€¢ Sliding window dataset creation\n",
    "  â€¢ LSTM networks for forecasting\n",
    "  â€¢ Transformer architectures\n",
    "  â€¢ Multi-step forecasting\n",
    "  â€¢ Model evaluation and comparison\n",
    "\n",
    "ðŸ’¡ Key Insights:\n",
    "  â€¢ LSTM is excellent for sequential data with long-term dependencies\n",
    "  â€¢ Transformers excel at capturing complex temporal patterns\n",
    "  â€¢ Always validate temporal properties (stationarity, seasonality)\n",
    "  â€¢ Proper normalization is crucial for convergence\n",
    "\n",
    "ðŸŽ¯ Practical Applications:\n",
    "  â€¢ Stock price prediction\n",
    "  â€¢ Weather forecasting\n",
    "  â€¢ Demand forecasting\n",
    "  â€¢ Anomaly detection\n",
    "  â€¢ Sensor data analysis\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ… Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
