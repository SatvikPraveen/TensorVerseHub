{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f314b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "from typing import Tuple, List, Dict, Optional, Callable\n",
    "import time\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Gym version: {gym.__version__}\")\n",
    "\n",
    "# Enable GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"\\nGPU available: {len(gpus)}\")\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"\\nNo GPU available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a0fcf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Reinforcement Learning Fundamentals\n",
    "\n",
    "### Key Concepts\n",
    "- **Agent:** Takes actions in environment\n",
    "- **Environment:** Responds to actions with rewards and new states\n",
    "- **State (s):** Current observation\n",
    "- **Action (a):** Agent's decision\n",
    "- **Reward (r):** Feedback signal\n",
    "- **Policy (Ï€):** Mapping from states to actions\n",
    "- **Value Function (V):** Expected cumulative reward\n",
    "- **Q-Function (Q):** Expected cumulative reward for action in state\n",
    "\n",
    "### Markov Decision Process (MDP)\n",
    "```\n",
    "Agent                Environment\n",
    "  |                      |\n",
    "  |------ action(a) ---->|\n",
    "  |                      |\n",
    "  |<-- state(s), reward(r) --|\n",
    "  |\n",
    "  v\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024878e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Experience tuple for replay buffer\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for storing and sampling experiences.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 10000):\n",
    "        \"\"\"\n",
    "        Initialize replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            capacity: Maximum number of experiences to store\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def add(self, experience: Experience):\n",
    "        \"\"\"Add experience to buffer.\"\"\"\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Sample random batch from buffer.\"\"\"\n",
    "        batch = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "        \n",
    "        states = np.array([exp.state for exp in batch])\n",
    "        actions = np.array([exp.action for exp in batch])\n",
    "        rewards = np.array([exp.reward for exp in batch])\n",
    "        next_states = np.array([exp.next_state for exp in batch])\n",
    "        dones = np.array([exp.done for exp in batch])\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "print(\"âœ… Replay buffer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a73fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Deep Q-Learning (DQN)\n",
    "\n",
    "### Key Ideas\n",
    "- Use neural network to approximate Q-function\n",
    "- Experience replay to break correlation\n",
    "- Target network to stabilize learning\n",
    "- Epsilon-greedy exploration strategy\n",
    "\n",
    "### Algorithm\n",
    "```\n",
    "1. Initialize Q-network and target network\n",
    "2. For each episode:\n",
    "   a. Reset environment, get initial state\n",
    "   b. For each step:\n",
    "      - Select action: epsilon-greedy on Q-values\n",
    "      - Take action, observe reward and next state\n",
    "      - Store (state, action, reward, next_state, done) in replay buffer\n",
    "      - Sample batch from replay buffer\n",
    "      - Calculate target Q-value: r + Î³*max_a Q(s', a)\n",
    "      - Update Q-network by minimizing TD error\n",
    "      - Periodically update target network\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f1804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"Deep Q-Network agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, learning_rate: float = 0.001):\n",
    "        \"\"\"\n",
    "        Initialize DQN agent.\n",
    "        \n",
    "        Args:\n",
    "            state_dim: Dimension of state space\n",
    "            action_dim: Number of actions\n",
    "            learning_rate: Learning rate for optimizer\n",
    "        \"\"\"\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.update_frequency = 4  # Update Q-network every N steps\n",
    "        self.target_update_frequency = 1000  # Update target network every N steps\n",
    "        \n",
    "        # Networks\n",
    "        self.q_network = self._build_q_network()\n",
    "        self.target_network = self._build_q_network()\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(capacity=10000)\n",
    "        \n",
    "        # Tracking\n",
    "        self.total_steps = 0\n",
    "        self.training_loss_history = []\n",
    "    \n",
    "    def _build_q_network(self) -> keras.Model:\n",
    "        \"\"\"Build Q-network.\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', input_shape=(self.state_dim,)),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(self.action_dim)\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    def select_action(self, state: np.ndarray, training: bool = True) -> int:\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        state_tensor = tf.expand_dims(state, axis=0)\n",
    "        q_values = self.q_network(state_tensor, training=False)[0]\n",
    "        return tf.argmax(q_values).numpy()\n",
    "    \n",
    "    def store_experience(self, state: np.ndarray, action: int, reward: float, \n",
    "                        next_state: np.ndarray, done: bool):\n",
    "        \"\"\"Store experience in replay buffer.\"\"\"\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.replay_buffer.add(experience)\n",
    "    \n",
    "    def train_step(self, batch_size: int = 32) -> Optional[float]:\n",
    "        \"\"\"Perform training step.\"\"\"\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return None\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Current Q-values\n",
    "            q_values = self.q_network(states)\n",
    "            q_values = tf.reduce_sum(q_values * tf.one_hot(actions, self.action_dim), axis=1)\n",
    "            \n",
    "            # Target Q-values\n",
    "            next_q_values = self.target_network(next_states)\n",
    "            max_next_q_values = tf.reduce_max(next_q_values, axis=1)\n",
    "            target_q_values = rewards + self.gamma * max_next_q_values * (1 - dones)\n",
    "            \n",
    "            # TD error (loss)\n",
    "            loss = tf.reduce_mean(tf.square(target_q_values - q_values))\n",
    "        \n",
    "        # Backpropagation\n",
    "        gradients = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.q_network.trainable_variables))\n",
    "        \n",
    "        self.training_loss_history.append(loss.numpy())\n",
    "        self.total_steps += 1\n",
    "        \n",
    "        # Update target network\n",
    "        if self.total_steps % self.target_update_frequency == 0:\n",
    "            self.target_network.set_weights(self.q_network.get_weights())\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.total_steps % 100 == 0:\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        return loss.numpy()\n",
    "\n",
    "print(\"âœ… DQN Agent defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522b0a59",
   "metadata": {},
   "source": [
    "### Training Deep Q-Networks\n",
    "\n",
    "Let's train a DQN agent on the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "\n",
    "# Create agent\n",
    "agent = DQNAgent(state_dim, action_dim, learning_rate=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 100\n",
    "batch_size = 32\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "print(f\"\\nðŸš€ Training DQN for {num_episodes} episodes...\\n\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Select and take action\n",
    "        action = agent.select_action(state, training=True)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        done = done or truncated\n",
    "        \n",
    "        # Store experience\n",
    "        agent.store_experience(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Train on batch\n",
    "        if len(agent.replay_buffer) >= batch_size:\n",
    "            agent.train_step(batch_size)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        state = next_state\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_lengths.append(episode_length)\n",
    "    \n",
    "    if (episode + 1) % 10 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-10:])\n",
    "        avg_length = np.mean(episode_lengths[-10:])\n",
    "        print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward: {avg_reward:.1f} | Avg Length: {avg_length:.1f} | Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "print(f\"\\nâœ… Training complete!\")\n",
    "print(f\"Final average reward (last 10 episodes): {np.mean(episode_rewards[-10:]):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf77eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Episode rewards\n",
    "axes[0].plot(episode_rewards, label='Episode Reward', alpha=0.6)\n",
    "axes[0].plot(np.convolve(episode_rewards, np.ones(10)/10, mode='valid'), \n",
    "             label='10-Episode Moving Average', linewidth=2)\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Total Reward')\n",
    "axes[0].set_title('DQN Training: Episode Rewards')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Episode lengths\n",
    "axes[1].plot(episode_lengths, label='Episode Length', alpha=0.6, color='orange')\n",
    "axes[1].plot(np.convolve(episode_lengths, np.ones(10)/10, mode='valid'), \n",
    "             label='10-Episode Moving Average', linewidth=2, color='darkorange')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Episode Length')\n",
    "axes[1].set_title('DQN Training: Episode Lengths')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Training Summary:\")\n",
    "print(f\"  Initial avg reward (first 10): {np.mean(episode_rewards[:10]):.1f}\")\n",
    "print(f\"  Final avg reward (last 10): {np.mean(episode_rewards[-10:]):.1f}\")\n",
    "print(f\"  Improvement: {np.mean(episode_rewards[-10:]) - np.mean(episode_rewards[:10]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809dbf0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Policy Gradient Methods\n",
    "\n",
    "### Key Ideas\n",
    "- Directly parameterize policy (actor)\n",
    "- Optimize expected return using gradient ascent\n",
    "- Works with continuous and discrete actions\n",
    "\n",
    "### Algorithm (REINFORCE)\n",
    "```\n",
    "1. Initialize policy network Ï€(a|s; Î¸)\n",
    "2. For each episode:\n",
    "   a. Collect trajectory (s, a, r, s', ...)\n",
    "   b. Calculate discounted returns\n",
    "   c. For each timestep:\n",
    "      - Loss = -log(Ï€(a|s)) * R_t\n",
    "   d. Update policy: Î¸ = Î¸ + Î± * âˆ‡ Loss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e36f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientAgent:\n",
    "    \"\"\"Policy Gradient Agent (REINFORCE).\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, learning_rate: float = 0.01):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # Policy network\n",
    "        self.policy_network = self._build_policy_network()\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        \n",
    "        # Trajectory storage\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        # Tracking\n",
    "        self.training_loss_history = []\n",
    "    \n",
    "    def _build_policy_network(self) -> keras.Model:\n",
    "        \"\"\"Build policy network.\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', input_shape=(self.state_dim,)),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(self.action_dim, activation='softmax')\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Select action from policy.\"\"\"\n",
    "        state_tensor = tf.expand_dims(state, axis=0)\n",
    "        action_probs = self.policy_network(state_tensor)[0]\n",
    "        action = tf.random.categorical(tf.math.log(action_probs + 1e-10), num_samples=1).numpy()[0]\n",
    "        return action\n",
    "    \n",
    "    def store_experience(self, state: np.ndarray, action: int, reward: float):\n",
    "        \"\"\"Store experience for batch update.\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def compute_returns(self) -> np.ndarray:\n",
    "        \"\"\"Compute discounted returns.\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        \n",
    "        for reward in reversed(self.rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        returns = np.array(returns)\n",
    "        # Normalize returns\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        return returns\n",
    "    \n",
    "    def train_on_episode(self):\n",
    "        \"\"\"Train on collected episode.\"\"\"\n",
    "        if not self.states:\n",
    "            return None\n",
    "        \n",
    "        states = np.array(self.states)\n",
    "        actions = np.array(self.actions)\n",
    "        returns = self.compute_returns()\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get action probabilities\n",
    "            action_probs = self.policy_network(states)\n",
    "            \n",
    "            # Get probabilities of taken actions\n",
    "            action_probs_selected = tf.reduce_sum(\n",
    "                action_probs * tf.one_hot(actions, self.action_dim), axis=1\n",
    "            )\n",
    "            \n",
    "            # Policy gradient loss\n",
    "            loss = -tf.reduce_sum(tf.math.log(action_probs_selected + 1e-10) * returns)\n",
    "        \n",
    "        # Update policy\n",
    "        gradients = tape.gradient(loss, self.policy_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.policy_network.trainable_variables))\n",
    "        \n",
    "        self.training_loss_history.append(loss.numpy())\n",
    "        \n",
    "        # Clear trajectory\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return loss.numpy()\n",
    "\n",
    "print(\"âœ… Policy Gradient Agent defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dabe9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Policy Gradient Agent\n",
    "env = gym.make('CartPole-v1')\n",
    "pg_agent = PolicyGradientAgent(state_dim, action_dim, learning_rate=0.01)\n",
    "\n",
    "num_episodes = 100\n",
    "pg_episode_rewards = []\n",
    "pg_episode_lengths = []\n",
    "\n",
    "print(f\"ðŸš€ Training Policy Gradient Agent for {num_episodes} episodes...\\n\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Select action\n",
    "        action = pg_agent.select_action(state)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        done = done or truncated\n",
    "        \n",
    "        # Store experience\n",
    "        pg_agent.store_experience(state, action, reward)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        state = next_state\n",
    "    \n",
    "    # Train on episode\n",
    "    pg_agent.train_on_episode()\n",
    "    \n",
    "    pg_episode_rewards.append(episode_reward)\n",
    "    pg_episode_lengths.append(episode_length)\n",
    "    \n",
    "    if (episode + 1) % 10 == 0:\n",
    "        avg_reward = np.mean(pg_episode_rewards[-10:])\n",
    "        avg_length = np.mean(pg_episode_lengths[-10:])\n",
    "        print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward: {avg_reward:.1f} | Avg Length: {avg_length:.1f}\")\n",
    "\n",
    "print(f\"\\nâœ… Training complete!\")\n",
    "print(f\"Final average reward (last 10 episodes): {np.mean(pg_episode_rewards[-10:]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2e7e72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Actor-Critic Methods\n",
    "\n",
    "### Key Ideas\n",
    "- **Actor:** Policy network that selects actions\n",
    "- **Critic:** Value network that estimates returns\n",
    "- Critic provides baseline for variance reduction\n",
    "- More stable than pure policy gradients\n",
    "\n",
    "### Algorithm (A2C - Advantage Actor-Critic)\n",
    "```\n",
    "Actor loss: -log(Ï€(a|s)) * (R - V(s))\n",
    "Critic loss: (R - V(s))^2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76af22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    \"\"\"Advantage Actor-Critic (A2C) Agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, \n",
    "                 actor_lr: float = 0.001, critic_lr: float = 0.001):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # Actor (Policy) Network\n",
    "        self.actor = self._build_actor_network()\n",
    "        self.actor_optimizer = keras.optimizers.Adam(learning_rate=actor_lr)\n",
    "        \n",
    "        # Critic (Value) Network\n",
    "        self.critic = self._build_critic_network()\n",
    "        self.critic_optimizer = keras.optimizers.Adam(learning_rate=critic_lr)\n",
    "        \n",
    "        # Tracking\n",
    "        self.actor_loss_history = []\n",
    "        self.critic_loss_history = []\n",
    "    \n",
    "    def _build_actor_network(self) -> keras.Model:\n",
    "        \"\"\"Build actor (policy) network.\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', input_shape=(self.state_dim,)),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(self.action_dim, activation='softmax')\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    def _build_critic_network(self) -> keras.Model:\n",
    "        \"\"\"Build critic (value) network.\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', input_shape=(self.state_dim,)),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Select action from actor.\"\"\"\n",
    "        state_tensor = tf.expand_dims(state, axis=0)\n",
    "        action_probs = self.actor(state_tensor)[0]\n",
    "        action = tf.random.categorical(tf.math.log(action_probs + 1e-10), num_samples=1).numpy()[0]\n",
    "        return action\n",
    "    \n",
    "    def get_value(self, state: np.ndarray) -> float:\n",
    "        \"\"\"Get state value from critic.\"\"\"\n",
    "        state_tensor = tf.expand_dims(state, axis=0)\n",
    "        value = self.critic(state_tensor)[0, 0]\n",
    "        return value.numpy()\n",
    "    \n",
    "    def train_step(self, state: np.ndarray, action: int, reward: float, \n",
    "                   next_state: np.ndarray, done: bool):\n",
    "        \"\"\"Perform one training step.\"\"\"\n",
    "        state_tensor = tf.expand_dims(state, axis=0)\n",
    "        next_state_tensor = tf.expand_dims(next_state, axis=0)\n",
    "        \n",
    "        # Get current value estimate\n",
    "        current_value = self.critic(state_tensor)[0, 0]\n",
    "        \n",
    "        # Get next value estimate\n",
    "        next_value = self.critic(next_state_tensor)[0, 0]\n",
    "        if done:\n",
    "            next_value = 0\n",
    "        \n",
    "        # Calculate target and advantage\n",
    "        target_value = reward + self.gamma * next_value\n",
    "        advantage = target_value - current_value\n",
    "        \n",
    "        # Train critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            critic_value = self.critic(state_tensor)[0, 0]\n",
    "            critic_loss = tf.square(target_value - critic_value)\n",
    "        \n",
    "        critic_gradients = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))\n",
    "        \n",
    "        # Train actor\n",
    "        with tf.GradientTape() as tape:\n",
    "            action_probs = self.actor(state_tensor)[0]\n",
    "            action_prob_selected = action_probs[action]\n",
    "            actor_loss = -tf.math.log(action_prob_selected + 1e-10) * advantage\n",
    "        \n",
    "        actor_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_gradients, self.actor.trainable_variables))\n",
    "        \n",
    "        self.actor_loss_history.append(actor_loss.numpy())\n",
    "        self.critic_loss_history.append(critic_loss.numpy())\n",
    "\n",
    "print(\"âœ… Actor-Critic Agent defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd53561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Actor-Critic Agent\n",
    "env = gym.make('CartPole-v1')\n",
    "ac_agent = ActorCriticAgent(state_dim, action_dim, actor_lr=0.001, critic_lr=0.001)\n",
    "\n",
    "num_episodes = 100\n",
    "ac_episode_rewards = []\n",
    "ac_episode_lengths = []\n",
    "\n",
    "print(f\"ðŸš€ Training Actor-Critic Agent for {num_episodes} episodes...\\n\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Select action\n",
    "        action = ac_agent.select_action(state)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        done = done or truncated\n",
    "        \n",
    "        # Train on step\n",
    "        ac_agent.train_step(state, action, reward, next_state, done)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        state = next_state\n",
    "    \n",
    "    ac_episode_rewards.append(episode_reward)\n",
    "    ac_episode_lengths.append(episode_length)\n",
    "    \n",
    "    if (episode + 1) % 10 == 0:\n",
    "        avg_reward = np.mean(ac_episode_rewards[-10:])\n",
    "        avg_length = np.mean(ac_episode_lengths[-10:])\n",
    "        print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward: {avg_reward:.1f} | Avg Length: {avg_length:.1f}\")\n",
    "\n",
    "print(f\"\\nâœ… Training complete!\")\n",
    "print(f\"Final average reward (last 10 episodes): {np.mean(ac_episode_rewards[-10:]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683221f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Comparison of RL Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three methods\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Rewards comparison\n",
    "axes[0].plot(episode_rewards, label='DQN', alpha=0.7, linewidth=1)\n",
    "axes[0].plot(np.convolve(episode_rewards, np.ones(10)/10, mode='valid'), \n",
    "             label='DQN (10-ep avg)', linewidth=2)\n",
    "\n",
    "axes[0].plot(pg_episode_rewards, label='Policy Gradient', alpha=0.7, linewidth=1)\n",
    "axes[0].plot(np.convolve(pg_episode_rewards, np.ones(10)/10, mode='valid'), \n",
    "             label='Policy Gradient (10-ep avg)', linewidth=2)\n",
    "\n",
    "axes[0].plot(ac_episode_rewards, label='Actor-Critic', alpha=0.7, linewidth=1)\n",
    "axes[0].plot(np.convolve(ac_episode_rewards, np.ones(10)/10, mode='valid'), \n",
    "             label='Actor-Critic (10-ep avg)', linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Episode Reward')\n",
    "axes[0].set_title('RL Methods Comparison: Episode Rewards')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Final performance comparison\n",
    "methods = ['DQN', 'Policy Gradient', 'Actor-Critic']\n",
    "final_rewards = [\n",
    "    np.mean(episode_rewards[-10:]),\n",
    "    np.mean(pg_episode_rewards[-10:]),\n",
    "    np.mean(ac_episode_rewards[-10:])\n",
    "]\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "bars = axes[1].bar(methods, final_rewards, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, reward in zip(bars, final_rewards):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{reward:.1f}',\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "axes[1].set_ylabel('Average Reward')\n",
    "axes[1].set_title('Final Performance Comparison (Last 10 Episodes)')\n",
    "axes[1].set_ylim(0, max(final_rewards) * 1.2)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Performance Summary:\")\n",
    "print(f\"{'Method':<20} {'Final Avg Reward':<20} {'Max Reward':<15}\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'DQN':<20} {np.mean(episode_rewards[-10:]):<20.1f} {max(episode_rewards):<15.1f}\")\n",
    "print(f\"{'Policy Gradient':<20} {np.mean(pg_episode_rewards[-10:]):<20.1f} {max(pg_episode_rewards):<15.1f}\")\n",
    "print(f\"{'Actor-Critic':<20} {np.mean(ac_episode_rewards[-10:]):<20.1f} {max(ac_episode_rewards):<15.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf2cdab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Summary & Key Takeaways\n",
    "\n",
    "### RL Methods Comparison\n",
    "\n",
    "| Method | Type | Pros | Cons | Use Case |\n",
    "|--------|------|------|------|----------|\n",
    "| **Q-Learning / DQN** | Value-Based | Works well with discrete actions, stable | Doesn't scale to continuous actions | Game AI, Discrete control |\n",
    "| **Policy Gradient** | Policy-Based | Works with continuous actions, simple | High variance, sample inefficient | Continuous control, Robotics |\n",
    "| **Actor-Critic** | Both | Combines benefits of both | More complex to implement | Most real-world applications |\n",
    "\n",
    "### Key Concepts\n",
    "1. **Exploration vs Exploitation:** Balance between trying new actions and using best known\n",
    "2. **Experience Replay:** Store and resample experiences to break correlation\n",
    "3. **Target Networks:** Stabilize training by using separate networks for targets\n",
    "4. **Policy Gradient:** Directly optimize expected return using gradient ascent\n",
    "5. **Advantage:** Reduces variance of policy gradient using critic baseline\n",
    "\n",
    "### Important Hyperparameters\n",
    "- **Learning Rate:** Controls update magnitude\n",
    "- **Discount Factor (Î³):** Balance between immediate and future rewards\n",
    "- **Epsilon (Îµ):** Exploration rate in epsilon-greedy\n",
    "- **Replay Buffer Size:** Memory for experience replay\n",
    "- **Network Architecture:** Hidden layer sizes affect capacity\n",
    "\n",
    "### When to Use Each Method\n",
    "- **DQN:** Discrete actions, computational budget allows\n",
    "- **Policy Gradient:** Continuous actions, simplicity preferred\n",
    "- **Actor-Critic:** Best performance, moderate complexity acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dfd60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "ðŸŽ“ Reinforcement Learning with TensorFlow - Summary\n",
    "==================================================\n",
    "\n",
    "âœ… Concepts Covered:\n",
    "  â€¢ Markov Decision Processes (MDPs)\n",
    "  â€¢ Q-Learning and Deep Q-Networks (DQN)\n",
    "  â€¢ Experience Replay and Target Networks\n",
    "  â€¢ Policy Gradient Methods (REINFORCE)\n",
    "  â€¢ Actor-Critic Algorithms (Advantage Actor-Critic)\n",
    "  â€¢ Practical implementations with TensorFlow/Keras\n",
    "  â€¢ Comparison of methods and when to use each\n",
    "\n",
    "âœ… Hands-On Examples:\n",
    "  â€¢ CartPole environment solutions\n",
    "  â€¢ Performance visualization and comparison\n",
    "  â€¢ Training dynamics and convergence patterns\n",
    "\n",
    "ðŸ“š Next Steps:\n",
    "  1. Experiment with different network architectures\n",
    "  2. Try different hyperparameters\n",
    "  3. Apply to more complex environments (Atari, MuJoCo)\n",
    "  4. Explore TF-Agents framework for production use\n",
    "  5. Implement multi-agent scenarios\n",
    "\n",
    "ðŸ’¡ Key Takeaway:\n",
    "  Reinforcement Learning is a powerful paradigm for learning optimal\n",
    "  control policies. Start with simple environments (CartPole) and\n",
    "  gradually increase complexity as you master the concepts.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d64d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "env.close()\n",
    "print(\"âœ… Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
