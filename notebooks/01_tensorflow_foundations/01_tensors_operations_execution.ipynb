{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 tensors operations execution\n",
    "**Location: TensorVerseHub/notebooks/01_tensorflow_foundations/01_tensors_operations_execution.ipynb**\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "- Understand TensorFlow tensor fundamentals and data types\n",
    "- Master eager execution vs graph execution modes\n",
    "- Implement efficient operations with `tf.function` decorators\n",
    "- Work with tensor shapes, broadcasting, and mathematical operations\n",
    "- Debug and profile TensorFlow operations\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python programming knowledge\n",
    "- Understanding of NumPy arrays (helpful but not required)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE - Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "# Display versions and GPU availability\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Eager execution enabled: {tf.executing_eagerly()}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "print(f\"GPU device name: {tf.test.gpu_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensor Fundamentals\n",
    "\n",
    "Tensors are the fundamental data structure in TensorFlow - multi-dimensional arrays with a uniform data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Basic Tensors\n",
    "# Scalar (0-D tensor)\n",
    "scalar = tf.constant(42)\n",
    "print(f\"Scalar: {scalar}\")\n",
    "print(f\"Shape: {scalar.shape}\")\n",
    "print(f\"Rank: {tf.rank(scalar)}\")\n",
    "print(f\"Data type: {scalar.dtype}\")\n",
    "print()\n",
    "\n",
    "# Vector (1-D tensor)\n",
    "vector = tf.constant([1, 2, 3, 4, 5])\n",
    "print(f\"Vector: {vector}\")\n",
    "print(f\"Shape: {vector.shape}\")\n",
    "print(f\"Rank: {tf.rank(vector)}\")\n",
    "print()\n",
    "\n",
    "# Matrix (2-D tensor)\n",
    "matrix = tf.constant([[1, 2, 3], \n",
    "                      [4, 5, 6]])\n",
    "print(f\"Matrix: {matrix}\")\n",
    "print(f\"Shape: {matrix.shape}\")\n",
    "print(f\"Rank: {tf.rank(matrix)}\")\n",
    "print()\n",
    "\n",
    "# 3-D tensor (common for images: height x width x channels)\n",
    "tensor_3d = tf.random.normal([2, 3, 4])  # batch_size=2, height=3, width=4\n",
    "print(f\"3D Tensor shape: {tensor_3d.shape}\")\n",
    "print(f\"3D Tensor rank: {tf.rank(tensor_3d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Creation Methods\n",
    "# From Python lists/numpy arrays\n",
    "from_list = tf.constant([1, 2, 3, 4])\n",
    "from_numpy = tf.constant(np.array([1, 2, 3, 4]))\n",
    "\n",
    "print(f\"From list: {from_list}\")\n",
    "print(f\"From numpy: {from_numpy}\")\n",
    "print(f\"Are equal: {tf.reduce_all(tf.equal(from_list, from_numpy))}\")\n",
    "print()\n",
    "\n",
    "# Zeros and ones\n",
    "zeros = tf.zeros([3, 4])\n",
    "ones = tf.ones([2, 3, 4])\n",
    "print(f\"Zeros shape: {zeros.shape}\")\n",
    "print(f\"Ones shape: {ones.shape}\")\n",
    "print()\n",
    "\n",
    "# Random tensors\n",
    "tf.random.set_seed(42)  # For reproducibility\n",
    "random_uniform = tf.random.uniform([3, 3], minval=0, maxval=10, dtype=tf.int32)\n",
    "random_normal = tf.random.normal([3, 3], mean=0.0, stddev=1.0)\n",
    "\n",
    "print(f\"Random uniform (0-10): \\n{random_uniform}\")\n",
    "print(f\"Random normal (μ=0, σ=1): \\n{random_normal}\")\n",
    "print()\n",
    "\n",
    "# Identity matrix\n",
    "identity = tf.eye(4)\n",
    "print(f\"Identity matrix: \\n{identity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Types\n",
    "# Different data types\n",
    "float_tensor = tf.constant([1.0, 2.0, 3.0])\n",
    "int_tensor = tf.constant([1, 2, 3])\n",
    "bool_tensor = tf.constant([True, False, True])\n",
    "string_tensor = tf.constant([\"Hello\", \"TensorFlow\"])\n",
    "\n",
    "print(f\"Float tensor: {float_tensor.dtype}\")\n",
    "print(f\"Int tensor: {int_tensor.dtype}\")\n",
    "print(f\"Bool tensor: {bool_tensor.dtype}\")\n",
    "print(f\"String tensor: {string_tensor.dtype}\")\n",
    "print()\n",
    "\n",
    "# Type conversion\n",
    "converted_to_float = tf.cast(int_tensor, tf.float32)\n",
    "converted_to_int = tf.cast(float_tensor, tf.int32)\n",
    "\n",
    "print(f\"Int to float: {converted_to_float} (dtype: {converted_to_float.dtype})\")\n",
    "print(f\"Float to int: {converted_to_int} (dtype: {converted_to_int.dtype})\")\n",
    "\n",
    "# Precision considerations\n",
    "high_precision = tf.constant([1.123456789], dtype=tf.float64)\n",
    "low_precision = tf.cast(high_precision, tf.float32)\n",
    "\n",
    "print(f\"High precision (float64): {high_precision}\")\n",
    "print(f\"Low precision (float32): {low_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tensor Operations\n",
    "\n",
    "TensorFlow provides a comprehensive set of operations for tensor manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Mathematical Operations\n",
    "a = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "b = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
    "\n",
    "print(f\"Tensor A: \\n{a}\")\n",
    "print(f\"Tensor B: \\n{b}\")\n",
    "print()\n",
    "\n",
    "# Element-wise operations\n",
    "addition = tf.add(a, b)  # or a + b\n",
    "subtraction = tf.subtract(a, b)  # or a - b\n",
    "multiplication = tf.multiply(a, b)  # or a * b\n",
    "division = tf.divide(a, b)  # or a / b\n",
    "\n",
    "print(f\"Addition: \\n{addition}\")\n",
    "print(f\"Subtraction: \\n{subtraction}\")\n",
    "print(f\"Element-wise multiplication: \\n{multiplication}\")\n",
    "print(f\"Division: \\n{division}\")\n",
    "print()\n",
    "\n",
    "# Matrix operations\n",
    "matrix_mult = tf.matmul(a, b)  # or a @ b\n",
    "transpose = tf.transpose(a)\n",
    "\n",
    "print(f\"Matrix multiplication: \\n{matrix_mult}\")\n",
    "print(f\"Transpose of A: \\n{transpose}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduction operations\n",
    "tensor = tf.constant([[1, 2, 3], \n",
    "                      [4, 5, 6]])\n",
    "\n",
    "print(f\"Original tensor: \\n{tensor}\")\n",
    "print(f\"Sum all elements: {tf.reduce_sum(tensor)}\")\n",
    "print(f\"Sum along axis 0: {tf.reduce_sum(tensor, axis=0)}\")\n",
    "print(f\"Sum along axis 1: {tf.reduce_sum(tensor, axis=1)}\")\n",
    "print(f\"Mean: {tf.reduce_mean(tensor, dtype=tf.float32)}\")\n",
    "print(f\"Maximum: {tf.reduce_max(tensor)}\")\n",
    "print(f\"Minimum: {tf.reduce_min(tensor)}\")\n",
    "print()\n",
    "\n",
    "# Statistical operations\n",
    "random_data = tf.random.normal([1000], mean=5.0, stddev=2.0)\n",
    "mean = tf.reduce_mean(random_data)\n",
    "std = tf.math.reduce_std(random_data)\n",
    "variance = tf.math.reduce_variance(random_data)\n",
    "\n",
    "print(f\"Random data statistics:\")\n",
    "print(f\"Mean: {mean:.4f} (expected: 5.0)\")\n",
    "print(f\"Std: {std:.4f} (expected: 2.0)\")\n",
    "print(f\"Variance: {variance:.4f} (expected: 4.0)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping tensors\n",
    "original = tf.range(12)\n",
    "print(f\"Original shape: {original.shape}, tensor: {original}\")\n",
    "\n",
    "reshaped_2d = tf.reshape(original, [3, 4])\n",
    "reshaped_3d = tf.reshape(original, [2, 2, 3])\n",
    "\n",
    "print(f\"Reshaped to [3, 4]: \\n{reshaped_2d}\")\n",
    "print(f\"Reshaped to [2, 2, 3]: \\n{reshaped_3d}\")\n",
    "print()\n",
    "\n",
    "# Expanding and squeezing dimensions\n",
    "vector = tf.constant([1, 2, 3, 4])\n",
    "expanded = tf.expand_dims(vector, axis=0)  # Add dimension at position 0\n",
    "expanded_col = tf.expand_dims(vector, axis=1)  # Add dimension at position 1\n",
    "\n",
    "print(f\"Original vector: {vector.shape}\")\n",
    "print(f\"Expanded (axis=0): {expanded.shape}\")\n",
    "print(f\"Expanded (axis=1): {expanded_col.shape}\")\n",
    "\n",
    "# Squeeze removes dimensions of size 1\n",
    "squeezed = tf.squeeze(expanded)\n",
    "print(f\"Squeezed back: {squeezed.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Broadcasting\n",
    "\n",
    "Broadcasting allows operations between tensors of different shapes by automatically expanding smaller tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting examples\n",
    "matrix = tf.constant([[1, 2, 3],\n",
    "                      [4, 5, 6]])\n",
    "vector = tf.constant([10, 20, 30])\n",
    "scalar = tf.constant(100)\n",
    "\n",
    "print(f\"Matrix shape: {matrix.shape}\")\n",
    "print(f\"Vector shape: {vector.shape}\")\n",
    "print(f\"Scalar shape: {scalar.shape}\")\n",
    "print()\n",
    "\n",
    "# Broadcasting in action\n",
    "matrix_plus_vector = matrix + vector  # [2,3] + [3] -> [2,3]\n",
    "matrix_plus_scalar = matrix + scalar  # [2,3] + [] -> [2,3]\n",
    "\n",
    "print(f\"Matrix: \\n{matrix}\")\n",
    "print(f\"Vector: {vector}\")\n",
    "print(f\"Matrix + Vector (broadcasting): \\n{matrix_plus_vector}\")\n",
    "print(f\"Matrix + Scalar (broadcasting): \\n{matrix_plus_scalar}\")\n",
    "\n",
    "# More complex broadcasting\n",
    "a = tf.constant([[1], [2], [3]])  # Shape: [3, 1]\n",
    "b = tf.constant([[1, 2, 3, 4]])   # Shape: [1, 4]\n",
    "result = a * b  # Broadcasting: [3, 1] * [1, 4] -> [3, 4]\n",
    "\n",
    "print(f\"A shape: {a.shape}, B shape: {b.shape}\")\n",
    "print(f\"A * B result shape: {result.shape}\")\n",
    "print(f\"Result: \\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Eager vs Graph Execution\n",
    "\n",
    "TensorFlow 2.x uses eager execution by default, but graph execution can provide performance benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eager execution (default in TF 2.x)\n",
    "print(\"=== Eager Execution ===\")\n",
    "print(f\"Eager execution enabled: {tf.executing_eagerly()}\")\n",
    "\n",
    "def eager_computation(x, y):\n",
    "    \"\"\"Simple computation in eager mode\"\"\"\n",
    "    result = tf.add(x, y)\n",
    "    print(f\"Intermediate result: {result}\")  # This prints immediately\n",
    "    return tf.multiply(result, 2)\n",
    "\n",
    "# Test eager execution\n",
    "a = tf.constant([1, 2, 3])\n",
    "b = tf.constant([4, 5, 6])\n",
    "eager_result = eager_computation(a, b)\n",
    "print(f\"Final result: {eager_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph execution with tf.function\n",
    "print(\"\\n=== Graph Execution with tf.function ===\")\n",
    "\n",
    "@tf.function\n",
    "def graph_computation(x, y):\n",
    "    \"\"\"Same computation but compiled to a graph\"\"\"\n",
    "    result = tf.add(x, y)\n",
    "    # print statements don't work the same way in graph mode\n",
    "    return tf.multiply(result, 2)\n",
    "\n",
    "# Test graph execution\n",
    "graph_result = graph_computation(a, b)\n",
    "print(f\"Graph execution result: {graph_result}\")\n",
    "\n",
    "# Performance comparison\n",
    "def performance_test():\n",
    "    \"\"\"Compare performance of eager vs graph execution\"\"\"\n",
    "    data = tf.random.normal([1000, 1000])\n",
    "    \n",
    "    # Eager execution timing\n",
    "    start_time = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = eager_computation(data, data)\n",
    "    eager_time = time.time() - start_time\n",
    "    \n",
    "    # Graph execution timing\n",
    "    start_time = time.time()\n",
    "    for _ in range(100):\n",
    "        _ = graph_computation(data, data)\n",
    "    graph_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Eager execution time: {eager_time:.4f}s\")\n",
    "    print(f\"Graph execution time: {graph_time:.4f}s\")\n",
    "    print(f\"Speedup: {eager_time/graph_time:.2f}x\")\n",
    "\n",
    "performance_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced tf.function Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def flexible_function(x):\n",
    "    \"\"\"tf.function that can handle different input types\"\"\"\n",
    "    return x * x + 1\n",
    "\n",
    "# Test with different input types\n",
    "print(\"=== tf.function with Different Input Types ===\")\n",
    "\n",
    "# Integer tensor\n",
    "int_result = flexible_function(tf.constant([1, 2, 3]))\n",
    "print(f\"Integer input: {int_result}\")\n",
    "\n",
    "# Float tensor\n",
    "float_result = flexible_function(tf.constant([1.5, 2.5, 3.5]))\n",
    "print(f\"Float input: {float_result}\")\n",
    "\n",
    "# Different shapes trigger recompilation\n",
    "small_result = flexible_function(tf.constant([1, 2]))\n",
    "large_result = flexible_function(tf.constant([1, 2, 3, 4, 5]))\n",
    "print(f\"Small tensor: {small_result}\")\n",
    "print(f\"Large tensor: {large_result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Variables and Assignment\n",
    "\n",
    "TensorFlow Variables are mutable tensors that can be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating variables\n",
    "initial_value = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "my_variable = tf.Variable(initial_value)\n",
    "\n",
    "print(f\"Variable: \\n{my_variable}\")\n",
    "print(f\"Variable shape: {my_variable.shape}\")\n",
    "print(f\"Variable dtype: {my_variable.dtype}\")\n",
    "print()\n",
    "\n",
    "# Variables can be updated\n",
    "print(\"=== Variable Updates ===\")\n",
    "print(f\"Before update: \\n{my_variable}\")\n",
    "\n",
    "# Assign new values\n",
    "my_variable.assign([[5.0, 6.0], [7.0, 8.0]])\n",
    "print(f\"After assign: \\n{my_variable}\")\n",
    "\n",
    "# Add to existing values\n",
    "my_variable.assign_add([[1.0, 1.0], [1.0, 1.0]])\n",
    "print(f\"After assign_add: \\n{my_variable}\")\n",
    "\n",
    "# Subtract from existing values\n",
    "my_variable.assign_sub([[0.5, 0.5], [0.5, 0.5]])\n",
    "print(f\"After assign_sub: \\n{my_variable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable in tf.function\n",
    "@tf.function\n",
    "def update_variable(var, increment):\n",
    "    \"\"\"Update a variable within a tf.function\"\"\"\n",
    "    var.assign_add(increment)\n",
    "    return var\n",
    "\n",
    "# Test variable updates in graph mode\n",
    "counter = tf.Variable(0.0)\n",
    "print(f\"Initial counter: {counter}\")\n",
    "\n",
    "for i in range(5):\n",
    "    updated_counter = update_variable(counter, 1.0)\n",
    "    print(f\"Counter after step {i+1}: {updated_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Debugging and Profiling\n",
    "\n",
    "TensorFlow provides tools for debugging and profiling tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tf.print for debugging in graph mode\n",
    "@tf.function\n",
    "def debug_function(x):\n",
    "    \"\"\"Function with debugging prints\"\"\"\n",
    "    tf.print(\"Input shape:\", tf.shape(x))\n",
    "    tf.print(\"Input values:\", x)\n",
    "    \n",
    "    result = tf.square(x)\n",
    "    tf.print(\"After squaring:\", result)\n",
    "    \n",
    "    final_result = tf.reduce_sum(result)\n",
    "    tf.print(\"Final sum:\", final_result)\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "# Test debugging\n",
    "debug_input = tf.constant([1, 2, 3, 4])\n",
    "debug_output = debug_function(debug_input)\n",
    "print(f\"Function output: {debug_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape Debugging\n",
    "def debug_shapes():\n",
    "    \"\"\"Common shape debugging techniques\"\"\"\n",
    "    \n",
    "    # Create tensors with different shapes\n",
    "    tensor_a = tf.random.normal([3, 4, 5])\n",
    "    tensor_b = tf.random.normal([4, 5])\n",
    "    \n",
    "    print(\"=== Shape Debugging ===\")\n",
    "    print(f\"Tensor A shape: {tensor_a.shape}\")\n",
    "    print(f\"Tensor B shape: {tensor_b.shape}\")\n",
    "    \n",
    "    # Safe shape checking\n",
    "    try:\n",
    "        result = tensor_a + tensor_b  # This should work with broadcasting\n",
    "        print(f\"Addition result shape: {result.shape}\")\n",
    "    except tf.errors.InvalidArgumentError as e:\n",
    "        print(f\"Shape error: {e}\")\n",
    "    \n",
    "    # Dynamic shape information\n",
    "    print(f\"Tensor A dynamic shape: {tf.shape(tensor_a)}\")\n",
    "    print(f\"Tensor A static shape: {tensor_a.shape}\")\n",
    "    print(f\"Tensor A rank: {tf.rank(tensor_a)}\")\n",
    "\n",
    "debug_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Tips for Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_tips():\n",
    "    \"\"\"Demonstrate performance best practices\"\"\"\n",
    "    \n",
    "    print(\"=== Performance Best Practices ===\")\n",
    "    \n",
    "    # Tip 1: Use appropriate data types\n",
    "    large_tensor = tf.random.uniform([1000, 1000])\n",
    "    \n",
    "    # Float32 is often faster than float64\n",
    "    start_time = time.time()\n",
    "    result_32 = tf.matmul(tf.cast(large_tensor, tf.float32), tf.cast(large_tensor, tf.float32))\n",
    "    time_32 = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result_64 = tf.matmul(tf.cast(large_tensor, tf.float64), tf.cast(large_tensor, tf.float64))\n",
    "    time_64 = time.time() - start_time\n",
    "    \n",
    "    print(f\"Float32 multiplication time: {time_32:.4f}s\")\n",
    "    print(f\"Float64 multiplication time: {time_64:.4f}s\")\n",
    "    print(f\"Float32 is {time_64/time_32:.2f}x faster\")\n",
    "    \n",
    "    # Tip 2: Vectorize operations instead of loops\n",
    "    data = tf.random.normal([10000])\n",
    "    \n",
    "    # Bad: Python loop\n",
    "    start_time = time.time()\n",
    "    result_loop = []\n",
    "    for i in range(10000):\n",
    "        result_loop.append(data[i] * data[i])\n",
    "    loop_time = time.time() - start_time\n",
    "    \n",
    "    # Good: Vectorized operation\n",
    "    start_time = time.time()\n",
    "    result_vectorized = tf.square(data)\n",
    "    vectorized_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Loop time: {loop_time:.4f}s\")\n",
    "    print(f\"Vectorized time: {vectorized_time:.4f}s\")\n",
    "    print(f\"Vectorization is {loop_time/vectorized_time:.2f}x faster\")\n",
    "\n",
    "performance_tips()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Tensors, Operations & Execution\n",
    "\n",
    "Welcome to the foundational notebook for TensorFlow! This notebook covers the core concepts of tensors, operations, and execution modes in TensorFlow 2.x. You'll learn about eager execution, graph execution, and the powerful `tf.function` decorator that bridges both worlds.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand TensorFlow tensors and their properties\n",
    "- Master tensor operations and mathematical computations\n",
    "- Learn the differences between eager and graph execution\n",
    "- Implement efficient code using `tf.function`\n",
    "- Explore automatic differentiation with `tf.GradientTape`\n",
    "- Practice tensor manipulation and broadcasting\n",
    "\n",
    "---\n",
    "\n",
    "## 1. TensorFlow Basics and Tensor Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Eager execution enabled: {tf.executing_eagerly()}\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU available: {tf.test.is_gpu_available()}\")\n",
    "if tf.test.is_gpu_available():\n",
    "    print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors from different sources\n",
    "# 1. From Python lists\n",
    "tensor_from_list = tf.constant([1, 2, 3, 4, 5])\n",
    "print(f\"From list: {tensor_from_list}\")\n",
    "\n",
    "# 2. From numpy arrays\n",
    "numpy_array = np.array([[1, 2], [3, 4]], dtype=np.float32)\n",
    "tensor_from_numpy = tf.constant(numpy_array)\n",
    "print(f\"From numpy: {tensor_from_numpy}\")\n",
    "\n",
    "# 3. Using tensor creation functions\n",
    "zeros_tensor = tf.zeros((3, 3))\n",
    "ones_tensor = tf.ones((2, 4))\n",
    "random_tensor = tf.random.normal((3, 3), mean=0, stddev=1)\n",
    "\n",
    "print(f\"Zeros tensor:\\n{zeros_tensor}\")\n",
    "print(f\"Random tensor:\\n{random_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor properties and attributes\n",
    "sample_tensor = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.float32)\n",
    "\n",
    "print(f\"Shape: {sample_tensor.shape}\")\n",
    "print(f\"Data type: {sample_tensor.dtype}\")\n",
    "print(f\"Number of dimensions: {sample_tensor.ndim}\")\n",
    "print(f\"Size (total elements): {tf.size(sample_tensor)}\")\n",
    "print(f\"Device placement: {sample_tensor.device}\")\n",
    "\n",
    "# Converting to numpy\n",
    "numpy_equivalent = sample_tensor.numpy()\n",
    "print(f\"As numpy array: {numpy_equivalent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensor Operations and Mathematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic mathematical operations\n",
    "a = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "b = tf.constant([[5, 6], [7, 8]], dtype=tf.float32)\n",
    "\n",
    "# Element-wise operations\n",
    "addition = tf.add(a, b)  # or a + b\n",
    "subtraction = tf.subtract(a, b)  # or a - b\n",
    "multiplication = tf.multiply(a, b)  # or a * b\n",
    "division = tf.divide(a, b)  # or a / b\n",
    "\n",
    "print(f\"Addition:\\n{addition}\")\n",
    "print(f\"Element-wise multiplication:\\n{multiplication}\")\n",
    "\n",
    "# Matrix operations\n",
    "matrix_mult = tf.matmul(a, b)  # or a @ b\n",
    "print(f\"Matrix multiplication:\\n{matrix_mult}\")\n",
    "\n",
    "# Reduction operations\n",
    "tensor_sum = tf.reduce_sum(a)\n",
    "tensor_mean = tf.reduce_mean(a, axis=1)\n",
    "tensor_max = tf.reduce_max(a, axis=0)\n",
    "\n",
    "print(f\"Sum of all elements: {tensor_sum}\")\n",
    "print(f\"Mean along axis 1: {tensor_mean}\")\n",
    "print(f\"Max along axis 0: {tensor_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced tensor operations\n",
    "# 1. Broadcasting\n",
    "scalar = tf.constant(10.0)\n",
    "vector = tf.constant([1, 2, 3], dtype=tf.float32)\n",
    "matrix = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "\n",
    "broadcast_result = matrix + vector[:2]  # Broadcasting vector to match matrix\n",
    "print(f\"Broadcasting result:\\n{broadcast_result}\")\n",
    "\n",
    "# 2. Reshaping and transposing\n",
    "original = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "reshaped = tf.reshape(original, (3, 2))\n",
    "transposed = tf.transpose(original)\n",
    "\n",
    "print(f\"Original:\\n{original}\")\n",
    "print(f\"Reshaped:\\n{reshaped}\")\n",
    "print(f\"Transposed:\\n{transposed}\")\n",
    "\n",
    "# 3. Indexing and slicing\n",
    "sample = tf.constant([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "print(f\"Element at (1,2): {sample[1, 2]}\")\n",
    "print(f\"First row: {sample[0, :]}\")\n",
    "print(f\"Last two columns:\\n{sample[:, -2:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Eager vs Graph Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eager execution (default in TF 2.x)\n",
    "def eager_function(x, y):\n",
    "    \"\"\"Function executed eagerly - operations run immediately\"\"\"\n",
    "    result = x * 2 + y\n",
    "    print(f\"Intermediate result: {result}\")  # This prints immediately\n",
    "    return result\n",
    "\n",
    "# Test eager execution\n",
    "x = tf.constant(5.0)\n",
    "y = tf.constant(3.0)\n",
    "eager_result = eager_function(x, y)\n",
    "print(f\"Eager result: {eager_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph execution with tf.function\n",
    "@tf.function\n",
    "def graph_function(x, y):\n",
    "    \"\"\"Function converted to TensorFlow graph for optimization\"\"\"\n",
    "    result = x * 2 + y\n",
    "    tf.print(f\"Graph intermediate result: {result}\")  # Use tf.print in graphs\n",
    "    return result\n",
    "\n",
    "# Test graph execution\n",
    "graph_result = graph_function(x, y)\n",
    "print(f\"Graph result: {graph_result}\")\n",
    "\n",
    "# Inspect the graph\n",
    "print(f\"Concrete function: {graph_function.get_concrete_function(x, y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: Eager vs Graph\n",
    "import time\n",
    "\n",
    "def time_execution(func, x, y, num_iterations=1000):\n",
    "    \"\"\"Time function execution\"\"\"\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_iterations):\n",
    "        result = func(x, y)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time, result\n",
    "\n",
    "# Create larger tensors for meaningful comparison\n",
    "large_x = tf.random.normal((1000, 1000))\n",
    "large_y = tf.random.normal((1000, 1000))\n",
    "\n",
    "def complex_eager_operation(x, y):\n",
    "    return tf.reduce_mean(tf.matmul(x, y) + tf.square(x))\n",
    "\n",
    "@tf.function\n",
    "def complex_graph_operation(x, y):\n",
    "    return tf.reduce_mean(tf.matmul(x, y) + tf.square(x))\n",
    "\n",
    "eager_time, eager_result = time_execution(complex_eager_operation, large_x, large_y, 10)\n",
    "graph_time, graph_result = time_execution(complex_graph_operation, large_x, large_y, 10)\n",
    "\n",
    "print(f\"Eager execution time: {eager_time:.4f} seconds\")\n",
    "print(f\"Graph execution time: {graph_time:.4f} seconds\")\n",
    "print(f\"Speedup: {eager_time / graph_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced tf.function Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.function with control flow\n",
    "@tf.function\n",
    "def conditional_function(x):\n",
    "    if tf.greater(x, 0):\n",
    "        return tf.square(x)\n",
    "    else:\n",
    "        return tf.abs(x)\n",
    "\n",
    "# Test with different inputs\n",
    "positive_input = tf.constant(5.0)\n",
    "negative_input = tf.constant(-3.0)\n",
    "\n",
    "print(f\"Positive input result: {conditional_function(positive_input)}\")\n",
    "print(f\"Negative input result: {conditional_function(negative_input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.function with loops\n",
    "@tf.function\n",
    "def fibonacci_tf(n):\n",
    "    \"\"\"Compute Fibonacci sequence using TensorFlow operations\"\"\"\n",
    "    a = tf.constant(0, dtype=tf.int32)\n",
    "    b = tf.constant(1, dtype=tf.int32)\n",
    "    \n",
    "    for _ in tf.range(n):\n",
    "        a, b = b, a + b\n",
    "    \n",
    "    return a\n",
    "\n",
    "# Generate Fibonacci numbers\n",
    "fib_numbers = [fibonacci_tf(i) for i in range(10)]\n",
    "print(f\"Fibonacci sequence: {fib_numbers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function signatures and tracing\n",
    "@tf.function\n",
    "def polymorphic_function(x):\n",
    "    \"\"\"Function that works with different tensor shapes and types\"\"\"\n",
    "    return tf.reduce_sum(x) * 2\n",
    "\n",
    "# Different input signatures will create different graphs\n",
    "int_input = tf.constant([1, 2, 3])\n",
    "float_input = tf.constant([1.0, 2.0, 3.0])\n",
    "matrix_input = tf.constant([[1, 2], [3, 4]])\n",
    "\n",
    "print(f\"Int result: {polymorphic_function(int_input)}\")\n",
    "print(f\"Float result: {polymorphic_function(float_input)}\")\n",
    "print(f\"Matrix result: {polymorphic_function(matrix_input)}\")\n",
    "\n",
    "# Check how many concrete functions were created\n",
    "print(f\"Number of traces: {len(polymorphic_function._list_all_concrete_functions())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Automatic Differentiation with tf.GradientTape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic gradient computation\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x ** 2 + 2 * x + 1  # y = x² + 2x + 1\n",
    "\n",
    "# Compute gradient dy/dx = 2x + 2\n",
    "gradient = tape.gradient(y, x)\n",
    "print(f\"x = {x.numpy()}, y = {y.numpy()}, dy/dx = {gradient.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple variables and gradients\n",
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = x**2 + y**2 + 2*x*y  # z = x² + y² + 2xy\n",
    "\n",
    "# Compute partial derivatives\n",
    "gradients = tape.gradient(z, [x, y])\n",
    "dz_dx, dz_dy = gradients\n",
    "\n",
    "print(f\"z = {z.numpy()}\")\n",
    "print(f\"∂z/∂x = {dz_dx.numpy()}\")\n",
    "print(f\"∂z/∂y = {dz_dy.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher-order derivatives\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as inner_tape:\n",
    "        y = x**3  # y = x³\n",
    "    \n",
    "    first_derivative = inner_tape.gradient(y, x)  # dy/dx = 3x²\n",
    "\n",
    "second_derivative = outer_tape.gradient(first_derivative, x)  # d²y/dx² = 6x\n",
    "\n",
    "print(f\"y = {y.numpy()}\")\n",
    "print(f\"First derivative: {first_derivative.numpy()}\")\n",
    "print(f\"Second derivative: {second_derivative.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent optimization example\n",
    "# Minimize f(x) = (x - 2)² + 1\n",
    "x = tf.Variable(0.0)\n",
    "learning_rate = 0.1\n",
    "history = []\n",
    "\n",
    "for step in range(50):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = (x - 2)**2 + 1\n",
    "    \n",
    "    gradient = tape.gradient(loss, x)\n",
    "    x.assign_sub(learning_rate * gradient)  # x = x - learning_rate * gradient\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}: x = {x.numpy():.4f}, loss = {loss.numpy():.4f}\")\n",
    "    \n",
    "    history.append([step, x.numpy(), loss.numpy()])\n",
    "\n",
    "# Visualize optimization\n",
    "history = np.array(history)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history[:, 0], history[:, 1])\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('x value')\n",
    "plt.title('Parameter Evolution')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history[:, 0], history[:, 2])\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Minimization')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tensor Manipulation and Advanced Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced indexing and masking\n",
    "data = tf.constant([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=tf.float32)\n",
    "\n",
    "# Boolean masking\n",
    "mask = data > 5\n",
    "filtered_data = tf.boolean_mask(data, mask)\n",
    "print(f\"Original data:\\n{data}\")\n",
    "print(f\"Mask (elements > 5):\\n{mask}\")\n",
    "print(f\"Filtered data: {filtered_data}\")\n",
    "\n",
    "# Fancy indexing\n",
    "indices = tf.constant([0, 2])\n",
    "selected_rows = tf.gather(data, indices, axis=0)\n",
    "print(f\"Selected rows:\\n{selected_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor stacking and concatenation\n",
    "a = tf.constant([[1, 2], [3, 4]])\n",
    "b = tf.constant([[5, 6], [7, 8]])\n",
    "c = tf.constant([[9, 10], [11, 12]])\n",
    "\n",
    "# Stack along new dimension\n",
    "stacked = tf.stack([a, b, c], axis=0)\n",
    "print(f\"Stacked shape: {stacked.shape}\")\n",
    "print(f\"Stacked tensor:\\n{stacked}\")\n",
    "\n",
    "# Concatenate along existing dimension\n",
    "concatenated = tf.concat([a, b, c], axis=0)\n",
    "print(f\"Concatenated shape: {concatenated.shape}\")\n",
    "print(f\"Concatenated tensor:\\n{concatenated}\")\n",
    "\n",
    "# Split tensor\n",
    "split_tensors = tf.split(concatenated, 3, axis=0)\n",
    "print(f\"Split back into {len(split_tensors)} tensors\")\n",
    "for i, tensor in enumerate(split_tensors):\n",
    "    print(f\"Tensor {i}:\\n{tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting and tile operations\n",
    "base_tensor = tf.constant([[1, 2], [3, 4]])\n",
    "\n",
    "# Tile (repeat) tensor\n",
    "tiled = tf.tile(base_tensor, [2, 3])\n",
    "print(f\"Original shape: {base_tensor.shape}\")\n",
    "print(f\"Tiled shape: {tiled.shape}\")\n",
    "print(f\"Tiled tensor:\\n{tiled}\")\n",
    "\n",
    "# Broadcasting example\n",
    "vector = tf.constant([10, 20])\n",
    "broadcasted_sum = base_tensor + vector  # Broadcasting happens automatically\n",
    "print(f\"Broadcasted addition:\\n{broadcasted_sum}\")\n",
    "\n",
    "# Explicit broadcasting\n",
    "broadcast_shape = tf.broadcast_dynamic_shape(tf.shape(base_tensor), tf.shape(vector))\n",
    "print(f\"Broadcast shape would be: {broadcast_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Device Placement and Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available devices\n",
    "print(\"Available devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(f\"  {device}\")\n",
    "\n",
    "# Manual device placement\n",
    "with tf.device('/CPU:0'):\n",
    "    cpu_tensor = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "    cpu_result = tf.matmul(cpu_tensor, cpu_tensor)\n",
    "\n",
    "print(f\"CPU tensor device: {cpu_tensor.device}\")\n",
    "print(f\"CPU result: {cpu_result}\")\n",
    "\n",
    "# If GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    with tf.device('/GPU:0'):\n",
    "        gpu_tensor = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "        gpu_result = tf.matmul(gpu_tensor, gpu_tensor)\n",
    "    \n",
    "    print(f\"GPU tensor device: {gpu_tensor.device}\")\n",
    "    print(f\"GPU result: {gpu_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory growth configuration (run this at the beginning of your program)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth for GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth enabled for GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Memory growth must be set before GPUs have been initialized: {e}\")\n",
    "\n",
    "# Monitor memory usage\n",
    "def check_memory_usage():\n",
    "    if tf.test.is_gpu_available():\n",
    "        gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "        for device in gpu_devices:\n",
    "            details = tf.config.experimental.get_device_details(device)\n",
    "            print(f\"GPU device: {device}\")\n",
    "            print(f\"Device details: {details}\")\n",
    "\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practical Examples and Mini-Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Linear regression using raw TensorFlow operations\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X_data = np.random.randn(n_samples, 1).astype(np.float32)\n",
    "y_data = 3 * X_data + 2 + 0.1 * np.random.randn(n_samples, 1).astype(np.float32)\n",
    "\n",
    "# Convert to TensorFlow tensors\n",
    "X = tf.constant(X_data)\n",
    "y = tf.constant(y_data)\n",
    "\n",
    "# Initialize parameters\n",
    "W = tf.Variable(tf.random.normal((1, 1)), name='weight')\n",
    "b = tf.Variable(tf.zeros((1,)), name='bias')\n",
    "\n",
    "# Define loss function and optimization\n",
    "def linear_regression(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = linear_regression(X)\n",
    "        loss = mean_squared_error(y, y_pred)\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, [W, b])\n",
    "    \n",
    "    # Update parameters\n",
    "    W.assign_sub(learning_rate * gradients[0])\n",
    "    b.assign_sub(learning_rate * gradients[1])\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.numpy():.4f}, W = {W.numpy()[0,0]:.4f}, b = {b.numpy()[0]:.4f}\")\n",
    "\n",
    "print(f\"Final parameters: W = {W.numpy()[0,0]:.4f}, b = {b.numpy()[0]:.4f}\")\n",
    "print(f\"True parameters: W = 3.0, b = 2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Custom activation function with automatic differentiation\n",
    "@tf.function\n",
    "def swish_activation(x):\n",
    "    \"\"\"Swish activation function: f(x) = x * sigmoid(x)\"\"\"\n",
    "    return x * tf.nn.sigmoid(x)\n",
    "\n",
    "def plot_activation_and_derivative(activation_fn, x_range=(-5, 5), num_points=1000):\n",
    "    \"\"\"Plot activation function and its derivative\"\"\"\n",
    "    x_vals = tf.linspace(float(x_range[0]), float(x_range[1]), num_points)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x_vals)\n",
    "        y_vals = activation_fn(x_vals)\n",
    "    \n",
    "    derivatives = tape.gradient(y_vals, x_vals)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x_vals.numpy(), y_vals.numpy(), 'b-', linewidth=2, label='Swish(x)')\n",
    "    plt.plot(x_vals.numpy(), tf.nn.relu(x_vals).numpy(), 'r--', alpha=0.7, label='ReLU(x)')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.title('Activation Functions')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x_vals.numpy(), derivatives.numpy(), 'g-', linewidth=2, label=\"Swish'(x)\")\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel(\"f'(x)\")\n",
    "    plt.title('Derivatives')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_activation_and_derivative(swish_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Numerical optimization with constraints\n",
    "def rosenbrock_function(x, y):\n",
    "    \"\"\"Rosenbrock function: f(x,y) = (a-x)² + b(y-x²)²\"\"\"\n",
    "    a, b = 1.0, 100.0\n",
    "    return (a - x)**2 + b * (y - x**2)**2\n",
    "\n",
    "def optimize_rosenbrock():\n",
    "    \"\"\"Minimize Rosenbrock function using gradient descent\"\"\"\n",
    "    x = tf.Variable(0.0)\n",
    "    y = tf.Variable(0.0)\n",
    "    \n",
    "    optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    for step in range(1000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = rosenbrock_function(x, y)\n",
    "        \n",
    "        gradients = tape.gradient(loss, [x, y])\n",
    "        optimizer.apply_gradients(zip(gradients, [x, y]))\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}: x={x.numpy():.4f}, y={y.numpy():.4f}, loss={loss.numpy():.6f}\")\n",
    "        \n",
    "        history.append([step, x.numpy(), y.numpy(), loss.numpy()])\n",
    "    \n",
    "    return np.array(history)\n",
    "\n",
    "# Run optimization\n",
    "optimization_history = optimize_rosenbrock()\n",
    "\n",
    "# Plot convergence\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(optimization_history[:, 0], optimization_history[:, 1], 'b-', label='x')\n",
    "plt.plot(optimization_history[:, 0], optimization_history[:, 2], 'r-', label='y')\n",
    "plt.axhline(y=1.0, color='k', linestyle='--', alpha=0.5, label='optimum')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Parameter Evolution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.semilogy(optimization_history[:, 0], optimization_history[:, 3])\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Loss Convergence')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(optimization_history[:, 1], optimization_history[:, 2], 'g-', alpha=0.7)\n",
    "plt.scatter(optimization_history[0, 1], optimization_history[0, 2], color='red', s=100, label='Start')\n",
    "plt.scatter(optimization_history[-1, 1], optimization_history[-1, 2], color='blue', s=100, label='End')\n",
    "plt.scatter(1.0, 1.0, color='black', s=100, marker='*', label='Optimum')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Optimization Path')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this comprehensive notebook, we've explored the fundamental building blocks of TensorFlow:\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. **Tensor Creation and Properties**: Understanding TensorFlow's core data structure\n",
    "2. **Mathematical Operations**: Element-wise operations, matrix operations, and reductions\n",
    "3. **Execution Modes**: Eager vs graph execution and their performance implications\n",
    "4. **tf.function**: Converting Python functions to optimized TensorFlow graphs\n",
    "5. **Automatic Differentiation**: Computing gradients with tf.GradientTape\n",
    "6. **Advanced Operations**: Broadcasting, indexing, and tensor manipulation\n",
    "7. **Device Management**: CPU/GPU placement and memory optimization\n",
    "8. **Practical Applications**: Linear regression, custom functions, and optimization\n",
    "\n",
    "In this notebook, you also learned:\n",
    "\n",
    "1. **Tensor Fundamentals**: Creating and manipulating tensors of different ranks and data types\n",
    "2. **Mathematical Operations**: Element-wise operations, matrix operations, and reductions\n",
    "3. **Shape Manipulation**: Reshaping, expanding, and broadcasting tensors\n",
    "4. **Execution Modes**: Eager execution vs graph execution with tf.function\n",
    "5. **Variables**: Mutable tensors for storing and updating state\n",
    "6. **Debugging**: Techniques for debugging tensor operations and shapes\n",
    "7. **Performance**: Best practices for efficient tensor computations\n",
    "\n",
    "### Key Takeaways\n",
    "- TensorFlow tensors are immutable unless they are Variables\n",
    "- Broadcasting allows operations between different-shaped tensors\n",
    "- tf.function can significantly improve performance for complex operations\n",
    "- Use appropriate data types (float32 vs float64) for optimal performance\n",
    "- Vectorized operations are much faster than Python loops\n",
    "\n",
    "### Practice Exercises\n",
    "1. Create a function that computes the Euclidean distance between two tensors\n",
    "2. Implement matrix multiplication using only element-wise operations\n",
    "3. Create a tf.function that normalizes a tensor (zero mean, unit variance)\n",
    "4. Experiment with different broadcasting scenarios\n",
    "\n",
    "### Best Practices Learned:\n",
    "- Use `tf.function` for performance-critical code\n",
    "- Leverage automatic differentiation for gradient-based optimization\n",
    "- Understand broadcasting to write efficient tensor operations\n",
    "- Utilize appropriate device placement for computational resources\n",
    "- Practice with real examples to solidify understanding\n",
    "\n",
    "### Next Steps:\n",
    "- Proceed to data pipelines and TFRecords (Notebook 02)\n",
    "- Apply these concepts in neural network construction\n",
    "- Explore advanced TensorFlow features and optimizations\n",
    "\n",
    "This foundation will serve you well as we dive deeper into TensorFlow's ecosystem and build increasingly sophisticated machine learning models!\n",
    "\n",
    "\n",
    "### Additional Resources\n",
    "- [TensorFlow Tensor Guide](https://www.tensorflow.org/guide/tensor)\n",
    "- [tf.function Documentation](https://www.tensorflow.org/guide/function)\n",
    "- [TensorFlow Performance Guide](https://www.tensorflow.org/guide/gpu_performance_analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
