{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 debugging profiling\n",
    "**Location: TensorVerseHub/notebooks/01_tensorflow_foundations/03_debugging_profiling.ipynb**\n",
    "\n",
    "TODO: Implement comprehensive TensorFlow + tf.keras learning content.\n",
    "\n",
    "## Learning Objectives\n",
    "- TODO: Define specific learning objectives\n",
    "- TODO: List key TensorFlow concepts covered\n",
    "- TODO: Outline tf.keras integration points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "# TODO: Add comprehensive implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Debugging & Profiling\n",
    "\n",
    "**File Location:** `notebooks/01_tensorflow_foundations/03_debugging_profiling.ipynb`\n",
    "\n",
    "Master TensorFlow debugging techniques, profiling tools, and reproducibility practices. Learn to identify bottlenecks, debug complex models, and ensure consistent results across different environments using TensorBoard, tf.keras callbacks, and advanced debugging utilities.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master TensorBoard for visualization and debugging\n",
    "- Implement tf.keras callbacks for training monitoring\n",
    "- Ensure reproducibility across different environments\n",
    "- Profile and optimize TensorFlow operations\n",
    "- Debug complex neural network architectures\n",
    "- Handle common TensorFlow errors and performance issues\n",
    "\n",
    "---\n",
    "\n",
    "## 1. TensorBoard Integration and Visualization\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import tempfile\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Setup logging directory\n",
    "log_dir = os.path.join(\"logs\", \"tensorboard\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "print(f\"TensorBoard logs: {log_dir}\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Basic TensorBoard logging with tf.summary\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample data for demonstration\"\"\"\n",
    "    X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, \n",
    "                             n_redundant=0, random_state=42)\n",
    "    return train_test_split(X.astype(np.float32), y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data\n",
    "X_train, X_test, y_train, y_test = create_sample_data()\n",
    "\n",
    "# Custom training loop with TensorBoard logging\n",
    "def train_with_tensorboard_logging():\n",
    "    \"\"\"Demonstrate custom training loop with detailed logging\"\"\"\n",
    "    \n",
    "    # Create model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Setup optimizer and loss\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    \n",
    "    # Metrics\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
    "    \n",
    "    # TensorBoard writers\n",
    "    train_log_dir = os.path.join(log_dir, 'train')\n",
    "    val_log_dir = os.path.join(log_dir, 'validation')\n",
    "    train_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    val_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "    \n",
    "    # Training loop with detailed logging\n",
    "    epochs = 20\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        train_accuracy.reset_states()\n",
    "        train_loss_avg = tf.keras.metrics.Mean()\n",
    "        \n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(train_ds):\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(x_batch, training=True)\n",
    "                loss = loss_fn(y_batch, predictions)\n",
    "            \n",
    "            # Compute gradients and update weights\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "            # Update metrics\n",
    "            train_accuracy.update_state(y_batch, predictions)\n",
    "            train_loss_avg.update_state(loss)\n",
    "            \n",
    "            # Log detailed metrics every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                with train_writer.as_default():\n",
    "                    tf.summary.scalar('batch_loss', loss, step=epoch * len(train_ds) + batch_idx)\n",
    "                    tf.summary.scalar('batch_accuracy', train_accuracy.result(), \n",
    "                                    step=epoch * len(train_ds) + batch_idx)\n",
    "                    \n",
    "                    # Log gradient norms\n",
    "                    for i, grad in enumerate(gradients):\n",
    "                        if grad is not None:\n",
    "                            grad_norm = tf.norm(grad)\n",
    "                            tf.summary.scalar(f'gradient_norm_layer_{i}', grad_norm, \n",
    "                                            step=epoch * len(train_ds) + batch_idx)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_accuracy.reset_states()\n",
    "        val_loss_avg = tf.keras.metrics.Mean()\n",
    "        \n",
    "        for x_batch, y_batch in val_ds:\n",
    "            predictions = model(x_batch, training=False)\n",
    "            loss = loss_fn(y_batch, predictions)\n",
    "            val_accuracy.update_state(y_batch, predictions)\n",
    "            val_loss_avg.update_state(loss)\n",
    "        \n",
    "        # Log epoch metrics\n",
    "        with train_writer.as_default():\n",
    "            tf.summary.scalar('epoch_loss', train_loss_avg.result(), step=epoch)\n",
    "            tf.summary.scalar('epoch_accuracy', train_accuracy.result(), step=epoch)\n",
    "            tf.summary.scalar('learning_rate', optimizer.learning_rate, step=epoch)\n",
    "        \n",
    "        with val_writer.as_default():\n",
    "            tf.summary.scalar('epoch_loss', val_loss_avg.result(), step=epoch)\n",
    "            tf.summary.scalar('epoch_accuracy', val_accuracy.result(), step=epoch)\n",
    "        \n",
    "        # Log model weights distribution\n",
    "        if epoch % 5 == 0:\n",
    "            with train_writer.as_default():\n",
    "                for layer in model.layers:\n",
    "                    if hasattr(layer, 'kernel'):\n",
    "                        tf.summary.histogram(f'{layer.name}/weights', layer.kernel, step=epoch)\n",
    "                    if hasattr(layer, 'bias') and layer.bias is not None:\n",
    "                        tf.summary.histogram(f'{layer.name}/bias', layer.bias, step=epoch)\n",
    "        \n",
    "        print(f\"  Train Acc: {train_accuracy.result():.4f}, Val Acc: {val_accuracy.result():.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run training with logging\n",
    "trained_model = train_with_tensorboard_logging()\n",
    "\n",
    "print(f\"\\nTraining complete! View logs with:\")\n",
    "print(f\"tensorboard --logdir {log_dir}\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Advanced TensorBoard features\n",
    "def advanced_tensorboard_logging():\n",
    "    \"\"\"Demonstrate advanced TensorBoard features\"\"\"\n",
    "    \n",
    "    # Create a more complex model for demonstration\n",
    "    def create_complex_model():\n",
    "        inputs = tf.keras.layers.Input(shape=(20,))\n",
    "        \n",
    "        # Branch 1\n",
    "        x1 = tf.keras.layers.Dense(32, activation='relu', name='branch1_dense1')(inputs)\n",
    "        x1 = tf.keras.layers.Dropout(0.3, name='branch1_dropout')(x1)\n",
    "        x1 = tf.keras.layers.Dense(16, activation='relu', name='branch1_dense2')(x1)\n",
    "        \n",
    "        # Branch 2\n",
    "        x2 = tf.keras.layers.Dense(24, activation='relu', name='branch2_dense1')(inputs)\n",
    "        x2 = tf.keras.layers.Dropout(0.2, name='branch2_dropout')(x2)\n",
    "        x2 = tf.keras.layers.Dense(12, activation='relu', name='branch2_dense2')(x2)\n",
    "        \n",
    "        # Combine branches\n",
    "        combined = tf.keras.layers.Concatenate(name='combine_branches')([x1, x2])\n",
    "        outputs = tf.keras.layers.Dense(3, activation='softmax', name='output')(combined)\n",
    "        \n",
    "        return tf.keras.Model(inputs=inputs, outputs=outputs, name='complex_model')\n",
    "    \n",
    "    model = create_complex_model()\n",
    "    \n",
    "    # Log model architecture\n",
    "    with tf.summary.create_file_writer(os.path.join(log_dir, 'model')).as_default():\n",
    "        tf.summary.graph(model.call.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, 20], dtype=tf.float32)\n",
    "        ).graph)\n",
    "    \n",
    "    # Log model structure\n",
    "    model.summary()\n",
    "    \n",
    "    # Create confusion matrix logging\n",
    "    def log_confusion_matrix(epoch, y_true, y_pred, class_names):\n",
    "        \"\"\"Log confusion matrix to TensorBoard\"\"\"\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import seaborn as sns\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f'Confusion Matrix - Epoch {epoch}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        \n",
    "        # Convert plot to image and log to TensorBoard\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        \n",
    "        with tf.summary.create_file_writer(os.path.join(log_dir, 'images')).as_default():\n",
    "            tf.summary.image(\"Confusion Matrix\", image, step=epoch)\n",
    "        \n",
    "        plt.close()\n",
    "    \n",
    "    # Example of logging custom images and text\n",
    "    with tf.summary.create_file_writer(os.path.join(log_dir, 'custom')).as_default():\n",
    "        # Log text summary\n",
    "        tf.summary.text(\"model_info\", f\"Model has {model.count_params()} parameters\", step=0)\n",
    "        \n",
    "        # Log custom scalar with metadata\n",
    "        tf.summary.scalar(\"custom_metric\", 0.85, step=0, \n",
    "                         description=\"Custom performance metric\")\n",
    "\n",
    "advanced_tensorboard_logging()\n",
    "print(\"Advanced TensorBoard logging complete!\")\n",
    "```\n",
    "\n",
    "## 2. tf.keras Callbacks for Monitoring\n",
    "\n",
    "```python\n",
    "# Comprehensive callback system\n",
    "class CustomCallbacks:\n",
    "    \"\"\"Collection of custom callbacks for training monitoring\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_standard_callbacks(log_dir, patience=5):\n",
    "        \"\"\"Create standard set of callbacks\"\"\"\n",
    "        \n",
    "        callbacks = [\n",
    "            # Early stopping\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            \n",
    "            # Learning rate reduction\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.2,\n",
    "                patience=3,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            \n",
    "            # Model checkpointing\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=os.path.join(log_dir, 'best_model.h5'),\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            \n",
    "            # TensorBoard logging\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=log_dir,\n",
    "                histogram_freq=1,\n",
    "                write_graph=True,\n",
    "                write_images=True,\n",
    "                update_freq='epoch'\n",
    "            ),\n",
    "            \n",
    "            # CSV logging\n",
    "            tf.keras.callbacks.CSVLogger(\n",
    "                filename=os.path.join(log_dir, 'training_log.csv'),\n",
    "                separator=',',\n",
    "                append=False\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        return callbacks\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_custom_monitoring_callback():\n",
    "        \"\"\"Create custom callback for detailed monitoring\"\"\"\n",
    "        \n",
    "        class DetailedMonitoringCallback(tf.keras.callbacks.Callback):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.epoch_times = []\n",
    "                self.batch_times = []\n",
    "                \n",
    "            def on_epoch_begin(self, epoch, logs=None):\n",
    "                self.epoch_start_time = time.time()\n",
    "                print(f\"\\n--- Starting Epoch {epoch + 1} ---\")\n",
    "                \n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                epoch_time = time.time() - self.epoch_start_time\n",
    "                self.epoch_times.append(epoch_time)\n",
    "                \n",
    "                print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f} seconds\")\n",
    "                print(f\"Train Loss: {logs.get('loss', 0):.4f}, Train Acc: {logs.get('accuracy', 0):.4f}\")\n",
    "                print(f\"Val Loss: {logs.get('val_loss', 0):.4f}, Val Acc: {logs.get('val_accuracy', 0):.4f}\")\n",
    "                \n",
    "                # Log to TensorBoard\n",
    "                with tf.summary.create_file_writer(os.path.join(log_dir, 'timing')).as_default():\n",
    "                    tf.summary.scalar('epoch_duration', epoch_time, step=epoch)\n",
    "                    tf.summary.scalar('avg_epoch_duration', np.mean(self.epoch_times), step=epoch)\n",
    "                \n",
    "            def on_batch_begin(self, batch, logs=None):\n",
    "                self.batch_start_time = time.time()\n",
    "                \n",
    "            def on_batch_end(self, batch, logs=None):\n",
    "                batch_time = time.time() - self.batch_start_time\n",
    "                self.batch_times.append(batch_time)\n",
    "                \n",
    "                if batch % 20 == 0:  # Log every 20 batches\n",
    "                    print(f\"  Batch {batch}: loss={logs.get('loss', 0):.4f}, \"\n",
    "                          f\"time={batch_time:.3f}s\")\n",
    "            \n",
    "            def on_train_end(self, logs=None):\n",
    "                print(f\"\\nTraining Summary:\")\n",
    "                print(f\"Total epochs: {len(self.epoch_times)}\")\n",
    "                print(f\"Average epoch time: {np.mean(self.epoch_times):.2f} ± {np.std(self.epoch_times):.2f} seconds\")\n",
    "                print(f\"Total training time: {sum(self.epoch_times):.2f} seconds\")\n",
    "        \n",
    "        return DetailedMonitoringCallback()\n",
    "\n",
    "# Custom learning rate scheduler callback\n",
    "class CustomLRScheduler(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Custom learning rate scheduler with warm-up and decay\"\"\"\n",
    "    \n",
    "    def __init__(self, warmup_epochs=5, decay_epochs=10, initial_lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.decay_epochs = decay_epochs\n",
    "        self.initial_lr = initial_lr\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            # Warm-up phase\n",
    "            lr = self.initial_lr * (epoch + 1) / self.warmup_epochs\n",
    "        elif epoch < self.warmup_epochs + self.decay_epochs:\n",
    "            # Decay phase\n",
    "            decay_epoch = epoch - self.warmup_epochs\n",
    "            lr = self.initial_lr * (0.5 ** (decay_epoch / 5))\n",
    "        else:\n",
    "            # Stable phase\n",
    "            lr = self.initial_lr * 0.1\n",
    "        \n",
    "        tf.keras.backend.set_value(self.model.optimizer.learning_rate, lr)\n",
    "        print(f\"Epoch {epoch + 1}: Learning rate set to {lr:.6f}\")\n",
    "\n",
    "# Gradient monitoring callback\n",
    "class GradientMonitoringCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Monitor gradient statistics during training\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir, log_freq=5):\n",
    "        super().__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.log_freq = log_freq\n",
    "        self.writer = tf.summary.create_file_writer(os.path.join(log_dir, 'gradients'))\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if batch % self.log_freq == 0:\n",
    "            # Get gradients (this is simplified - in practice you'd need to modify training loop)\n",
    "            with self.writer.as_default():\n",
    "                for i, layer in enumerate(self.model.layers):\n",
    "                    if hasattr(layer, 'kernel') and layer.kernel is not None:\n",
    "                        weights = layer.kernel\n",
    "                        tf.summary.scalar(f'layer_{i}/weight_mean', tf.reduce_mean(weights), step=batch)\n",
    "                        tf.summary.scalar(f'layer_{i}/weight_std', tf.math.reduce_std(weights), step=batch)\n",
    "                        tf.summary.histogram(f'layer_{i}/weights', weights, step=batch)\n",
    "\n",
    "# Demonstrate callback usage\n",
    "def train_with_callbacks():\n",
    "    \"\"\"Train model with comprehensive callback monitoring\"\"\"\n",
    "    \n",
    "    # Create model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(20,)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Setup callbacks\n",
    "    callback_log_dir = os.path.join(log_dir, 'callbacks')\n",
    "    os.makedirs(callback_log_dir, exist_ok=True)\n",
    "    \n",
    "    callbacks = CustomCallbacks.create_standard_callbacks(callback_log_dir)\n",
    "    callbacks.extend([\n",
    "        CustomCallbacks.create_custom_monitoring_callback(),\n",
    "        CustomLRScheduler(warmup_epochs=3, decay_epochs=7),\n",
    "        GradientMonitoringCallback(callback_log_dir)\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=15,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0  # Custom callback handles verbose output\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Train with callbacks\n",
    "callback_model, callback_history = train_with_callbacks()\n",
    "print(\"Training with callbacks completed!\")\n",
    "```\n",
    "\n",
    "## 3. Reproducibility and Debugging\n",
    "\n",
    "```python\n",
    "# Comprehensive reproducibility setup\n",
    "def setup_reproducibility(seed=42):\n",
    "    \"\"\"Setup reproducible environment\"\"\"\n",
    "    \n",
    "    # Set random seeds\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    # For Python's random module\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Configure TensorFlow for reproducibility\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    \n",
    "    # Configure GPU memory growth (if available)\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                # Set deterministic operations\n",
    "                tf.config.experimental.enable_op_determinism()\n",
    "        except RuntimeError as e:\n",
    "            print(f\"GPU configuration warning: {e}\")\n",
    "    \n",
    "    print(f\"Reproducibility setup complete with seed: {seed}\")\n",
    "\n",
    "# Environment debugging utilities\n",
    "def debug_environment():\n",
    "    \"\"\"Comprehensive environment debugging information\"\"\"\n",
    "    \n",
    "    print(\"=== TensorFlow Environment Debug Info ===\")\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    print(f\"Keras version: {tf.keras.__version__}\")\n",
    "    print(f\"Python executable: {sys.executable}\")\n",
    "    \n",
    "    # Device information\n",
    "    print(f\"\\nDevice Information:\")\n",
    "    print(f\"GPUs available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "    print(f\"CPUs available: {len(tf.config.list_physical_devices('CPU'))}\")\n",
    "    \n",
    "    for device in tf.config.list_physical_devices():\n",
    "        print(f\"  {device}\")\n",
    "    \n",
    "    # Memory information\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(f\"\\nGPU Memory Info:\")\n",
    "        for gpu in tf.config.list_physical_devices('GPU'):\n",
    "            try:\n",
    "                memory_info = tf.config.experimental.get_memory_info(gpu.name.replace('/physical_device:', ''))\n",
    "                print(f\"  {gpu}: {memory_info}\")\n",
    "            except:\n",
    "                print(f\"  {gpu}: Memory info not available\")\n",
    "    \n",
    "    # Environment variables\n",
    "    important_vars = ['TF_CPP_MIN_LOG_LEVEL', 'TF_DETERMINISTIC_OPS', 'CUDA_VISIBLE_DEVICES']\n",
    "    print(f\"\\nImportant Environment Variables:\")\n",
    "    for var in important_vars:\n",
    "        value = os.environ.get(var, 'Not set')\n",
    "        print(f\"  {var}: {value}\")\n",
    "    \n",
    "    # Test basic operations\n",
    "    print(f\"\\nBasic Operation Tests:\")\n",
    "    try:\n",
    "        # CPU test\n",
    "        with tf.device('/CPU:0'):\n",
    "            cpu_test = tf.reduce_sum(tf.random.normal([100, 100]))\n",
    "            print(f\"  CPU operation: {cpu_test.numpy():.4f}\")\n",
    "        \n",
    "        # GPU test (if available)\n",
    "        if tf.config.list_physical_devices('GPU'):\n",
    "            with tf.device('/GPU:0'):\n",
    "                gpu_test = tf.reduce_sum(tf.random.normal([100, 100]))\n",
    "                print(f\"  GPU operation: {gpu_test.numpy():.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Operation test failed: {e}\")\n",
    "\n",
    "# Setup reproducibility and debug environment\n",
    "setup_reproducibility(42)\n",
    "debug_environment()\n",
    "```\n",
    "\n",
    "```python\n",
    "# Advanced debugging utilities\n",
    "class ModelDebugger:\n",
    "    \"\"\"Comprehensive model debugging utilities\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.layer_outputs = {}\n",
    "        \n",
    "    def create_layer_output_model(self):\n",
    "        \"\"\"Create model that outputs all intermediate layer results\"\"\"\n",
    "        \n",
    "        layer_outputs = []\n",
    "        layer_names = []\n",
    "        \n",
    "        for layer in self.model.layers:\n",
    "            if len(layer.output.shape) > 0:  # Skip layers without output\n",
    "                layer_outputs.append(layer.output)\n",
    "                layer_names.append(layer.name)\n",
    "        \n",
    "        debug_model = tf.keras.Model(\n",
    "            inputs=self.model.input,\n",
    "            outputs=layer_outputs\n",
    "        )\n",
    "        \n",
    "        return debug_model, layer_names\n",
    "    \n",
    "    def analyze_layer_outputs(self, X_sample):\n",
    "        \"\"\"Analyze outputs from each layer\"\"\"\n",
    "        \n",
    "        debug_model, layer_names = self.create_layer_output_model()\n",
    "        layer_outputs = debug_model(X_sample)\n",
    "        \n",
    "        print(\"Layer Output Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, (output, name) in enumerate(zip(layer_outputs, layer_names)):\n",
    "            output_stats = {\n",
    "                'shape': output.shape,\n",
    "                'mean': tf.reduce_mean(output).numpy(),\n",
    "                'std': tf.math.reduce_std(output).numpy(),\n",
    "                'min': tf.reduce_min(output).numpy(),\n",
    "                'max': tf.reduce_max(output).numpy(),\n",
    "                'zeros': tf.reduce_sum(tf.cast(tf.equal(output, 0), tf.float32)).numpy(),\n",
    "                'infs': tf.reduce_sum(tf.cast(tf.math.is_inf(output), tf.float32)).numpy(),\n",
    "                'nans': tf.reduce_sum(tf.cast(tf.math.is_nan(output), tf.float32)).numpy()\n",
    "            }\n",
    "            \n",
    "            print(f\"Layer {i}: {name}\")\n",
    "            print(f\"  Shape: {output_stats['shape']}\")\n",
    "            print(f\"  Stats: mean={output_stats['mean']:.4f}, std={output_stats['std']:.4f}\")\n",
    "            print(f\"  Range: [{output_stats['min']:.4f}, {output_stats['max']:.4f}]\")\n",
    "            \n",
    "            # Warning for potential issues\n",
    "            if output_stats['zeros'] > output.size * 0.5:\n",
    "                print(f\"  ⚠️  Warning: {output_stats['zeros']}/{output.size} outputs are zero (dead neurons?)\")\n",
    "            if output_stats['infs'] > 0:\n",
    "                print(f\"  ❌ Error: {output_stats['infs']} infinite values detected\")\n",
    "            if output_stats['nans'] > 0:\n",
    "                print(f\"  ❌ Error: {output_stats['nans']} NaN values detected\")\n",
    "            \n",
    "            print()\n",
    "    \n",
    "    def check_gradient_flow(self, X_sample, y_sample):\n",
    "        \"\"\"Check gradient flow through the model\"\"\"\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            predictions = self.model(X_sample, training=True)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_sample, predictions)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        print(\"Gradient Flow Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            if hasattr(layer, 'trainable_variables') and layer.trainable_variables:\n",
    "                for j, var in enumerate(layer.trainable_variables):\n",
    "                    grad = tape.gradient(loss, var)\n",
    "                    if grad is not None:\n",
    "                        grad_norm = tf.norm(grad).numpy()\n",
    "                        grad_mean = tf.reduce_mean(tf.abs(grad)).numpy()\n",
    "                        \n",
    "                        print(f\"Layer {i} ({layer.name}) - Variable {j}:\")\n",
    "                        print(f\"  Gradient norm: {grad_norm:.6f}\")\n",
    "                        print(f\"  Gradient mean abs: {grad_mean:.6f}\")\n",
    "                        \n",
    "                        if grad_norm < 1e-7:\n",
    "                            print(f\"  ⚠️  Warning: Very small gradients (vanishing gradient?)\")\n",
    "                        elif grad_norm > 10:\n",
    "                            print(f\"  ⚠️  Warning: Large gradients (exploding gradient?)\")\n",
    "                    else:\n",
    "                        print(f\"Layer {i} ({layer.name}) - Variable {j}: No gradient\")\n",
    "        \n",
    "        del tape  # Clean up persistent tape\n",
    "\n",
    "# Debugging utilities in action\n",
    "def demonstrate_debugging():\n",
    "    \"\"\"Demonstrate comprehensive model debugging\"\"\"\n",
    "    \n",
    "    # Create a potentially problematic model\n",
    "    problematic_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1000, activation='relu', input_shape=(20,)),\n",
    "        tf.keras.layers.Dense(1000, activation='relu'),\n",
    "        tf.keras.layers.Dense(1000, activation='relu'),  # Very deep\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    problematic_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-1),  # High learning rate\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Create debugger\n",
    "    debugger = ModelDebugger(problematic_model)\n",
    "    \n",
    "    # Sample data for debugging\n",
    "    X_debug = X_train[:10]\n",
    "    y_debug = y_train[:10]\n",
    "    \n",
    "    print(\"=== Model Architecture Debug ===\")\n",
    "    problematic_model.summary()\n",
    "    \n",
    "    print(\"\\n=== Layer Output Analysis ===\")\n",
    "    debugger.analyze_layer_outputs(X_debug)\n",
    "    \n",
    "    print(\"\\n=== Gradient Flow Analysis ===\")\n",
    "    debugger.check_gradient_flow(X_debug, y_debug)\n",
    "\n",
    "demonstrate_debugging()\n",
    "```\n",
    "\n",
    "## 4. Performance Profiling\n",
    "\n",
    "```python\n",
    "# TensorFlow Profiler integration\n",
    "def profile_model_training():\n",
    "    \"\"\"Profile model training with TensorFlow Profiler\"\"\"\n",
    "    \n",
    "    # Create model and data\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation='relu', input_shape=(20,)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    dataset = dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Profile training\n",
    "    profile_dir = os.path.join(log_dir, 'profiler')\n",
    "    os.makedirs(profile_dir, exist_ok=True)\n",
    "    \n",
    "    # Start profiling\n",
    "    tf.profiler.experimental.start(profile_dir)\n",
    "    \n",
    "    # Run training steps\n",
    "    print(\"Starting profiled training...\")\n",
    "    for step, (x_batch, y_batch) in enumerate(dataset.take(50)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(x_batch, training=True)\n",
    "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_batch, predictions)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.numpy():.4f}\")\n",
    "    \n",
    "    # Stop profiling\n",
    "    tf.profiler.experimental.stop()\n",
    "    \n",
    "    print(f\"Profiling complete! View with:\")\n",
    "    print(f\"tensorboard --logdir {profile_dir}\")\n",
    "\n",
    "# Custom performance benchmarking\n",
    "class PerformanceBenchmark:\n",
    "    \"\"\"Custom performance benchmarking utilities\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    \n",
    "    def benchmark_operation(self, operation_fn, inputs, num_runs=100, warmup_runs=10):\n",
    "        \"\"\"Benchmark a specific operation\"\"\"\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(warmup_runs):\n",
    "            _ = operation_fn(inputs)\n",
    "        \n",
    "        # Actual timing\n",
    "        times = []\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            result = operation_fn(inputs)\n",
    "            if hasattr(result, 'numpy'):\n",
    "                _ = result.numpy()  # Ensure computation is complete\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "        \n",
    "        return {\n",
    "            'mean_time': np.mean(times),\n",
    "            'std_time': np.std(times),\n",
    "            'min_time': np.min(times),\n",
    "            'max_time': np.max(times),\n",
    "            'median_time': np.median(times)\n",
    "        }\n",
    "    \n",
    "    def benchmark_model_inference(self, model, test_data, batch_sizes=[1, 8, 32, 64]):\n",
    "        \"\"\"Benchmark model inference at different batch sizes\"\"\"\n",
    "        \n",
    "        print(\"Model Inference Benchmarking:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            # Prepare batched data\n",
    "            batched_data = []\n",
    "            for i in range(0, len(test_data), batch_size):\n",
    "                batch = test_data[i:i + batch_size]\n",
    "                if len(batch) == batch_size:  # Only use full batches\n",
    "                    batched_data.append(batch)\n",
    "                if len(batched_data) >= 20:  # Limit number of batches\n",
    "                    break\n",
    "            \n",
    "            # Benchmark\n",
    "            def inference_fn(batch_data):\n",
    "                total_predictions = 0\n",
    "                for batch in batch_data:\n",
    "                    predictions = model(batch, training=False)\n",
    "                    total_predictions += predictions.shape[0]\n",
    "                return total_predictions\n",
    "            \n",
    "            benchmark_result = self.benchmark_operation(inference_fn, batched_data, num_runs=10)\n",
    "            \n",
    "            # Calculate throughput\n",
    "            total_samples = len(batched_data) * batch_size\n",
    "            throughput = total_samples / benchmark_result['mean_time']\n",
    "            \n",
    "            results[batch_size] = {\n",
    "                **benchmark_result,\n",
    "                'throughput': throughput,\n",
    "                'samples_per_batch': batch_size\n",
    "            }\n",
    "            \n",
    "            print(f\"Batch size {batch_size:2d}: {throughput:8.2f} samples/sec, \"\n",
    "                  f\"latency: {benchmark_result['mean_time']*1000:6.2f} ± {benchmark_result['std_time']*1000:5.2f} ms\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def profile_memory_usage(self, operation_fn, inputs):\n",
    "        \"\"\"Profile memory usage of an operation\"\"\"\n",
    "        \n",
    "        if tf.config.list_physical_devices('GPU'):\n",
    "            # GPU memory profiling\n",
    "            initial_memory = tf.config.experimental.get_memory_info('GPU:0')['current']\n",
    "            \n",
    "            result = operation_fn(inputs)\n",
    "            \n",
    "            peak_memory = tf.config.experimental.get_memory_info('GPU:0')['current']\n",
    "            memory_used = peak_memory - initial_memory\n",
    "            \n",
    "            return {\n",
    "                'gpu_memory_used_mb': memory_used / (1024**2),\n",
    "                'gpu_initial_mb': initial_memory / (1024**2),\n",
    "                'gpu_peak_mb': peak_memory / (1024**2)\n",
    "            }\n",
    "        else:\n",
    "            # CPU memory profiling would require additional libraries like psutil\n",
    "            return {'message': 'GPU not available for memory profiling'}\n",
    "\n",
    "# Run performance profiling\n",
    "print(\"=== Performance Profiling ===\")\n",
    "\n",
    "# Simple profiling example\n",
    "benchmark = PerformanceBenchmark()\n",
    "\n",
    "# Benchmark matrix operations\n",
    "def matrix_op(size):\n",
    "    a = tf.random.normal((size, size))\n",
    "    b = tf.random.normal((size, size))\n",
    "    return tf.matmul(a, b)\n",
    "\n",
    "print(\"Matrix multiplication benchmarking:\")\n",
    "for size in [100, 500, 1000]:\n",
    "    result = benchmark.benchmark_operation(lambda _: matrix_op(size), None, num_runs=10)\n",
    "    print(f\"Size {size:4d}: {result['mean_time']*1000:6.2f} ± {result['std_time']*1000:4.2f} ms\")\n",
    "\n",
    "# Benchmark model inference\n",
    "if 'trained_model' in locals():\n",
    "    inference_results = benchmark.benchmark_model_inference(trained_model, X_test)\n",
    "\n",
    "# Profile TensorFlow operations\n",
    "profile_model_training()\n",
    "```\n",
    "\n",
    "## 5. Common Debugging Scenarios\n",
    "\n",
    "```python\n",
    "# Common debugging scenarios and solutions\n",
    "class DebuggingScenarios:\n",
    "    \"\"\"Collection of common debugging scenarios and solutions\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def debug_nan_gradients():\n",
    "        \"\"\"Debug and fix NaN gradients\"\"\"\n",
    "        \n",
    "        print(\"=== Debugging NaN Gradients ===\")\n",
    "        \n",
    "        # Create model that might produce NaN gradients\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(10, input_shape=(5,)),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Use high learning rate that might cause issues\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=10.0),\n",
    "                     loss='binary_crossentropy')\n",
    "        \n",
    "        # Generate data\n",
    "        X = tf.random.normal((100, 5))\n",
    "        y = tf.random.uniform((100, 1)) > 0.5\n",
    "        y = tf.cast(y, tf.float32)\n",
    "        \n",
    "        # Training step that might produce NaN\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(X)\n",
    "            loss = tf.keras.losses.binary_crossentropy(y, predictions)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # Check for NaN gradients\n",
    "        nan_detected = False\n",
    "        for i, grad in enumerate(gradients):\n",
    "            if grad is not None:\n",
    "                nan_count = tf.reduce_sum(tf.cast(tf.math.is_nan(grad), tf.int32))\n",
    "                if nan_count > 0:\n",
    "                    print(f\"NaN gradients detected in layer {i}: {nan_count.numpy()} NaN values\")\n",
    "                    nan_detected = True\n",
    "                else:\n",
    "                    grad_norm = tf.norm(grad)\n",
    "                    print(f\"Layer {i} gradient norm: {grad_norm.numpy():.6f}\")\n",
    "        \n",
    "        if not nan_detected:\n",
    "            print(\"No NaN gradients detected with current settings\")\n",
    "        \n",
    "        # Solutions for NaN gradients\n",
    "        print(\"\\nSolutions for NaN gradients:\")\n",
    "        print(\"1. Reduce learning rate\")\n",
    "        print(\"2. Add gradient clipping\")\n",
    "        print(\"3. Use batch normalization\")\n",
    "        print(\"4. Check for inf/nan in input data\")\n",
    "        print(\"5. Use more stable loss functions\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def debug_vanishing_gradients():\n",
    "        \"\"\"Debug vanishing gradients in deep networks\"\"\"\n",
    "        \n",
    "        print(\"\\n=== Debugging Vanishing Gradients ===\")\n",
    "        \n",
    "        # Create very deep network\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(50, input_shape=(10,)))\n",
    "        \n",
    "        for _ in range(20):  # 20 hidden layers\n",
    "            model.add(tf.keras.layers.Dense(50, activation='sigmoid'))  # Sigmoid can cause vanishing gradients\n",
    "        \n",
    "        model.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        # Generate data\n",
    "        X = tf.random.normal((100, 10))\n",
    "        y = tf.random.normal((100, 1))\n",
    "        \n",
    "        # Analyze gradient magnitudes\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(X)\n",
    "            loss = tf.keras.losses.mse(y, predictions)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        print(\"Gradient analysis for deep network:\")\n",
    "        layer_idx = 0\n",
    "        for i in range(0, len(gradients), 2):  # Every 2 gradients (weights and biases)\n",
    "            if gradients[i] is not None:\n",
    "                grad_norm = tf.norm(gradients[i])\n",
    "                print(f\"Layer {layer_idx:2d} gradient norm: {grad_norm.numpy():.8f}\")\n",
    "                layer_idx += 1\n",
    "        \n",
    "        print(\"\\nSolutions for vanishing gradients:\")\n",
    "        print(\"1. Use ReLU or LeakyReLU activations instead of sigmoid/tanh\")\n",
    "        print(\"2. Use residual connections (ResNet)\")\n",
    "        print(\"3. Use batch normalization\")\n",
    "        print(\"4. Use gradient clipping\")\n",
    "        print(\"5. Use proper weight initialization (He, Xavier)\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def debug_exploding_gradients():\n",
    "        \"\"\"Debug exploding gradients\"\"\"\n",
    "        \n",
    "        print(\"\\n=== Debugging Exploding Gradients ===\")\n",
    "        \n",
    "        # Create model with potential for exploding gradients\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(100, activation='relu', input_shape=(10,),\n",
    "                                kernel_initializer='random_normal'),  # Poor initialization\n",
    "            tf.keras.layers.Dense(100, activation='relu',\n",
    "                                kernel_initializer='random_normal'),\n",
    "            tf.keras.layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "        \n",
    "        # Use high learning rate\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1.0),\n",
    "                     loss='mse')\n",
    "        \n",
    "        # Generate data\n",
    "        X = tf.random.normal((50, 10)) * 10  # Large input values\n",
    "        y = tf.random.normal((50, 1))\n",
    "        \n",
    "        # Check for exploding gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(X)\n",
    "            loss = tf.keras.losses.mse(y, predictions)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        max_grad_norm = 0\n",
    "        total_grad_norm = 0\n",
    "        \n",
    "        for grad in gradients:\n",
    "            if grad is not None:\n",
    "                grad_norm = tf.norm(grad).numpy()\n",
    "                max_grad_norm = max(max_grad_norm, grad_norm)\n",
    "                total_grad_norm += grad_norm\n",
    "        \n",
    "        print(f\"Maximum gradient norm: {max_grad_norm:.4f}\")\n",
    "        print(f\"Total gradient norm: {total_grad_norm:.4f}\")\n",
    "        \n",
    "        if max_grad_norm > 10:\n",
    "            print(\"⚠️  Large gradients detected - potential exploding gradient problem\")\n",
    "        \n",
    "        print(\"\\nSolutions for exploding gradients:\")\n",
    "        print(\"1. Gradient clipping\")\n",
    "        print(\"2. Lower learning rate\")\n",
    "        print(\"3. Better weight initialization\")\n",
    "        print(\"4. Batch normalization\")\n",
    "        print(\"5. L2 regularization\")\n",
    "\n",
    "# Run debugging scenarios\n",
    "debugging_scenarios = DebuggingScenarios()\n",
    "debugging_scenarios.debug_nan_gradients()\n",
    "debugging_scenarios.debug_vanishing_gradients()\n",
    "debugging_scenarios.debug_exploding_gradients()\n",
    "```\n",
    "\n",
    "## 6. Reproducibility Testing\n",
    "\n",
    "```python\n",
    "# Comprehensive reproducibility testing\n",
    "class ReproducibilityTester:\n",
    "    \"\"\"Test and ensure model reproducibility\"\"\"\n",
    "    \n",
    "    def __init__(self, seed=42):\n",
    "        self.seed = seed\n",
    "        \n",
    "    def test_deterministic_training(self, num_runs=3):\n",
    "        \"\"\"Test if training produces identical results across runs\"\"\"\n",
    "        \n",
    "        print(\"=== Reproducibility Testing ===\")\n",
    "        \n",
    "        training_histories = []\n",
    "        model_weights = []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "            print(f\"Run {run + 1}/{num_runs}\")\n",
    "            \n",
    "            # Reset environment\n",
    "            setup_reproducibility(self.seed)\n",
    "            \n",
    "            # Create and train model\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(32, activation='relu', input_shape=(20,)),\n",
    "                tf.keras.layers.Dense(16, activation='relu'),\n",
    "                tf.keras.layers.Dense(3, activation='softmax')\n",
    "            ])\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            # Train for few epochs\n",
    "            history = model.fit(X_train, y_train, epochs=5, verbose=0)\n",
    "            \n",
    "            # Store results\n",
    "            training_histories.append(history.history)\n",
    "            model_weights.append([w.numpy() for w in model.get_weights()])\n",
    "        \n",
    "        # Compare results\n",
    "        self._compare_training_results(training_histories, model_weights)\n",
    "        \n",
    "    def _compare_training_results(self, histories, weights):\n",
    "        \"\"\"Compare training results across runs\"\"\"\n",
    "        \n",
    "        print(\"\\nReproducibility Analysis:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Compare training histories\n",
    "        metric_names = list(histories[0].keys())\n",
    "        \n",
    "        for metric in metric_names:\n",
    "            values_per_epoch = []\n",
    "            for epoch in range(len(histories[0][metric])):\n",
    "                epoch_values = [hist[metric][epoch] for hist in histories]\n",
    "                values_per_epoch.append(epoch_values)\n",
    "                \n",
    "                # Check if all runs produced identical values\n",
    "                if len(set(f\"{v:.6f}\" for v in epoch_values)) == 1:\n",
    "                    status = \"✅ Identical\"\n",
    "                else:\n",
    "                    max_diff = max(epoch_values) - min(epoch_values)\n",
    "                    status = f\"❌ Different (max diff: {max_diff:.6f})\"\n",
    "                \n",
    "                print(f\"{metric} epoch {epoch}: {status}\")\n",
    "        \n",
    "        # Compare final model weights\n",
    "        print(f\"\\nModel Weights Comparison:\")\n",
    "        all_weights_identical = True\n",
    "        \n",
    "        for layer_idx in range(len(weights[0])):\n",
    "            layer_weights_identical = True\n",
    "            \n",
    "            for run_idx in range(1, len(weights)):\n",
    "                if not np.allclose(weights[0][layer_idx], weights[run_idx][layer_idx], rtol=1e-6):\n",
    "                    layer_weights_identical = False\n",
    "                    all_weights_identical = False\n",
    "            \n",
    "            status = \"✅ Identical\" if layer_weights_identical else \"❌ Different\"\n",
    "            print(f\"Layer {layer_idx} weights: {status}\")\n",
    "        \n",
    "        if all_weights_identical:\n",
    "            print(f\"\\n🎉 All runs produced identical results - Reproducibility confirmed!\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️  Results differ between runs - Check reproducibility setup\")\n",
    "    \n",
    "    def test_cross_platform_reproducibility(self):\n",
    "        \"\"\"Test reproducibility across different execution contexts\"\"\"\n",
    "        \n",
    "        print(f\"\\n=== Cross-Platform Reproducibility Test ===\")\n",
    "        \n",
    "        # Test eager vs graph execution\n",
    "        setup_reproducibility(self.seed)\n",
    "        \n",
    "        # Eager execution\n",
    "        @tf.function\n",
    "        def graph_computation(x):\n",
    "            return tf.nn.relu(tf.matmul(x, tf.random.normal([10, 5], seed=self.seed)))\n",
    "        \n",
    "        def eager_computation(x):\n",
    "            tf.random.set_seed(self.seed)\n",
    "            return tf.nn.relu(tf.matmul(x, tf.random.normal([10, 5])))\n",
    "        \n",
    "        test_input = tf.random.normal([5, 10], seed=self.seed)\n",
    "        \n",
    "        # Compare results\n",
    "        setup_reproducibility(self.seed)\n",
    "        eager_result = eager_computation(test_input)\n",
    "        \n",
    "        setup_reproducibility(self.seed)\n",
    "        graph_result = graph_computation(test_input)\n",
    "        \n",
    "        if np.allclose(eager_result.numpy(), graph_result.numpy()):\n",
    "            print(\"✅ Eager and graph execution produce identical results\")\n",
    "        else:\n",
    "            print(\"❌ Eager and graph execution produce different results\")\n",
    "            print(f\"Max difference: {np.max(np.abs(eager_result.numpy() - graph_result.numpy()))}\")\n",
    "\n",
    "# Run reproducibility tests\n",
    "reproducibility_tester = ReproducibilityTester(seed=42)\n",
    "reproducibility_tester.test_deterministic_training(num_runs=3)\n",
    "reproducibility_tester.test_cross_platform_reproducibility()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**File Location:** `notebooks/01_tensorflow_foundations/03_debugging_profiling.ipynb`\n",
    "\n",
    "This comprehensive debugging and profiling notebook covered essential techniques for TensorFlow development:\n",
    "\n",
    "### Key Debugging Skills Mastered:\n",
    "1. **TensorBoard Integration**: Advanced logging, visualization, and monitoring\n",
    "2. **Custom Callbacks**: Detailed training monitoring and automated interventions\n",
    "3. **Environment Setup**: Reproducibility configuration and environment debugging\n",
    "4. **Model Debugging**: Layer-by-layer analysis and gradient flow inspection\n",
    "5. **Performance Profiling**: TensorFlow Profiler and custom benchmarking\n",
    "6. **Common Issues**: NaN gradients, vanishing/exploding gradients solutions\n",
    "7. **Reproducibility Testing**: Cross-run and cross-platform consistency verification\n",
    "\n",
    "### Critical Debugging Tools:\n",
    "- **TensorBoard**: Comprehensive visualization and profiling\n",
    "- **Custom Callbacks**: Real-time monitoring and intervention\n",
    "- **tf.GradientTape**: Gradient analysis and debugging\n",
    "- **tf.profiler**: Performance bottleneck identification\n",
    "- **Environment Configuration**: Reproducible research practices\n",
    "\n",
    "### Production-Ready Practices:\n",
    "- Comprehensive logging strategies\n",
    "- Automated performance monitoring  \n",
    "- Systematic debugging workflows\n",
    "- Cross-platform reproducibility testing\n",
    "- Performance optimization techniques\n",
    "\n",
    "### Next Steps:\n",
    "- Apply these debugging techniques with tf.keras models (Notebook 04)\n",
    "- Use profiling insights for model optimization\n",
    "- Implement monitoring in production deployments\n",
    "\n",
    "These debugging and profiling skills are essential for developing robust, reproducible, and high-performance TensorFlow applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
