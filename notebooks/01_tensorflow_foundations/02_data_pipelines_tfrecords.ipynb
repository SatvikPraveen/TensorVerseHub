{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 data pipelines tfrecords\n",
    "**Location: TensorVerseHub/notebooks/01_tensorflow_foundations/02_data_pipelines_tfrecords.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Data Pipelines & TFRecords\n",
    "\n",
    "Welcome to the essential guide for building efficient data pipelines in TensorFlow! This notebook covers `tf.data` API, TFRecords format, and integration with `tf.keras` preprocessing layers. You'll learn to create scalable, performant data pipelines that can handle large datasets efficiently.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master the `tf.data` API for building input pipelines\n",
    "- Understand and create TFRecord files for efficient data storage\n",
    "- Implement data preprocessing with `tf.keras` preprocessing layers\n",
    "- Optimize data loading performance with prefetching, caching, and parallelization\n",
    "- Handle different data types: images, text, and structured data\n",
    "- Create reusable preprocessing pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction to tf.data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Enable mixed precision for better performance\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic tf.data.Dataset creation methods\n",
    "# 1. From tensors\n",
    "tensor_dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5])\n",
    "print(\"Dataset from tensors:\")\n",
    "for item in tensor_dataset:\n",
    "    print(f\"  {item.numpy()}\")\n",
    "\n",
    "# 2. From numpy arrays\n",
    "numpy_data = np.array([[1, 2], [3, 4], [5, 6]], dtype=np.float32)\n",
    "numpy_dataset = tf.data.Dataset.from_tensor_slices(numpy_data)\n",
    "print(\"\\nDataset from numpy:\")\n",
    "for item in numpy_dataset:\n",
    "    print(f\"  {item.numpy()}\")\n",
    "\n",
    "# 3. From generator function\n",
    "def data_generator():\n",
    "    for i in range(5):\n",
    "        yield i ** 2\n",
    "\n",
    "generator_dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator, \n",
    "    output_signature=tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    ")\n",
    "print(\"\\nDataset from generator:\")\n",
    "for item in generator_dataset:\n",
    "    print(f\"  {item.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset transformations - the power of tf.data\n",
    "base_dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "# Map transformation\n",
    "squared_dataset = base_dataset.map(lambda x: x ** 2)\n",
    "print(\"Squared values:\")\n",
    "print(list(squared_dataset.as_numpy_iterator()))\n",
    "\n",
    "# Filter transformation\n",
    "even_dataset = base_dataset.filter(lambda x: x % 2 == 0)\n",
    "print(\"\\nEven values:\")\n",
    "print(list(even_dataset.as_numpy_iterator()))\n",
    "\n",
    "# Batch transformation\n",
    "batched_dataset = base_dataset.batch(3)\n",
    "print(\"\\nBatched data:\")\n",
    "for batch in batched_dataset:\n",
    "    print(f\"  {batch.numpy()}\")\n",
    "\n",
    "# Shuffle transformation\n",
    "shuffled_dataset = base_dataset.shuffle(buffer_size=5)\n",
    "print(\"\\nShuffled data:\")\n",
    "print(list(shuffled_dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex transformations and chaining\n",
    "# Create a more complex dataset\n",
    "features = np.random.randn(100, 4).astype(np.float32)\n",
    "labels = np.random.randint(0, 3, size=(100,))\n",
    "\n",
    "# Create dataset from features and labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Apply multiple transformations\n",
    "processed_dataset = (dataset\n",
    "                    .shuffle(buffer_size=100)\n",
    "                    .map(lambda x, y: (tf.nn.l2_normalize(x, axis=0), y))  # Normalize features\n",
    "                    .filter(lambda x, y: tf.reduce_sum(tf.abs(x)) > 0.5)  # Filter samples\n",
    "                    .batch(8)\n",
    "                    .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "print(\"Processed dataset structure:\")\n",
    "for batch_features, batch_labels in processed_dataset.take(1):\n",
    "    print(f\"  Batch features shape: {batch_features.shape}\")\n",
    "    print(f\"  Batch labels shape: {batch_labels.shape}\")\n",
    "    print(f\"  Feature stats - mean: {tf.reduce_mean(batch_features):.3f}, std: {tf.math.reduce_std(batch_features):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Optimization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating performance optimizations\n",
    "def create_dummy_dataset(num_samples=1000):\n",
    "    \"\"\"Create a dummy dataset for performance testing\"\"\"\n",
    "    features = tf.random.normal((num_samples, 100))\n",
    "    labels = tf.random.uniform((num_samples,), maxval=10, dtype=tf.int32)\n",
    "    return tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "# Expensive preprocessing function\n",
    "def expensive_preprocessing(features, labels):\n",
    "    \"\"\"Simulate expensive preprocessing\"\"\"\n",
    "    # Simulate computation time\n",
    "    features = tf.nn.relu(features)\n",
    "    features = tf.nn.l2_normalize(features, axis=1)\n",
    "    processed_features = tf.reduce_mean(tf.reshape(features, (-1, 10, 10)), axis=2)\n",
    "    return processed_features, labels\n",
    "\n",
    "# Compare different optimization strategies\n",
    "import time\n",
    "\n",
    "def benchmark_dataset(dataset, num_batches=50):\n",
    "    \"\"\"Benchmark dataset iteration time\"\"\"\n",
    "    start_time = time.time()\n",
    "    for i, (features, labels) in enumerate(dataset):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        # Simulate model consumption\n",
    "        _ = tf.reduce_mean(features)\n",
    "    return time.time() - start_time\n",
    "\n",
    "# 1. Baseline (no optimization)\n",
    "baseline_dataset = (create_dummy_dataset()\n",
    "                   .map(expensive_preprocessing)\n",
    "                   .batch(32))\n",
    "\n",
    "# 2. With caching\n",
    "cached_dataset = (create_dummy_dataset()\n",
    "                 .cache()\n",
    "                 .map(expensive_preprocessing)\n",
    "                 .batch(32))\n",
    "\n",
    "# 3. With prefetching\n",
    "prefetch_dataset = (create_dummy_dataset()\n",
    "                   .map(expensive_preprocessing)\n",
    "                   .batch(32)\n",
    "                   .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# 4. With parallel mapping\n",
    "parallel_dataset = (create_dummy_dataset()\n",
    "                   .map(expensive_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                   .batch(32)\n",
    "                   .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# 5. Full optimization\n",
    "optimized_dataset = (create_dummy_dataset()\n",
    "                    .cache()\n",
    "                    .map(expensive_preprocessing, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                    .batch(32)\n",
    "                    .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "print(\"Performance comparison:\")\n",
    "print(f\"Baseline time: {benchmark_dataset(baseline_dataset):.2f} seconds\")\n",
    "print(f\"With caching: {benchmark_dataset(cached_dataset):.2f} seconds\")\n",
    "print(f\"With prefetching: {benchmark_dataset(prefetch_dataset):.2f} seconds\")\n",
    "print(f\"With parallel mapping: {benchmark_dataset(parallel_dataset):.2f} seconds\")\n",
    "print(f\"Fully optimized: {benchmark_dataset(optimized_dataset):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced performance patterns\n",
    "def create_interleaved_dataset():\n",
    "    \"\"\"Demonstrate dataset interleaving for multiple data sources\"\"\"\n",
    "    \n",
    "    # Create multiple data sources\n",
    "    dataset1 = tf.data.Dataset.range(100).map(lambda x: f\"source1_{x}\")\n",
    "    dataset2 = tf.data.Dataset.range(100).map(lambda x: f\"source2_{x}\")\n",
    "    dataset3 = tf.data.Dataset.range(100).map(lambda x: f\"source3_{x}\")\n",
    "    \n",
    "    # Method 1: Simple concatenation\n",
    "    concat_dataset = dataset1.concatenate(dataset2).concatenate(dataset3)\n",
    "    \n",
    "    # Method 2: Interleaved sampling\n",
    "    interleaved_dataset = tf.data.Dataset.sample_from_datasets(\n",
    "        [dataset1, dataset2, dataset3],\n",
    "        weights=[0.5, 0.3, 0.2]\n",
    "    )\n",
    "    \n",
    "    print(\"Concatenated dataset (first 10):\")\n",
    "    for i, item in enumerate(concat_dataset.take(10)):\n",
    "        print(f\"  {item.numpy().decode()}\")\n",
    "    \n",
    "    print(\"\\nInterleaved dataset (first 20):\")\n",
    "    for i, item in enumerate(interleaved_dataset.take(20)):\n",
    "        print(f\"  {item.numpy().decode()}\")\n",
    "\n",
    "create_interleaved_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Working with TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating TFRecord files\n",
    "def create_tfrecord_examples():\n",
    "    \"\"\"Create sample data for TFRecord demonstration\"\"\"\n",
    "    \n",
    "    # Generate synthetic image data\n",
    "    images = np.random.randint(0, 255, size=(10, 64, 64, 3), dtype=np.uint8)\n",
    "    labels = np.random.randint(0, 5, size=(10,))\n",
    "    texts = [f\"Sample text description {i}\" for i in range(10)]\n",
    "    \n",
    "    return images, labels, texts\n",
    "\n",
    "def serialize_example(image, label, text):\n",
    "    \"\"\"Create a tf.train.Example message\"\"\"\n",
    "    \n",
    "    # Helper functions for different data types\n",
    "    def _bytes_feature(value):\n",
    "        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    def _float_feature(value):\n",
    "        \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "    \n",
    "    # Convert image to bytes\n",
    "    image_string = tf.io.serialize_tensor(image).numpy()\n",
    "    \n",
    "    # Create feature dictionary\n",
    "    feature = {\n",
    "        'image': _bytes_feature(image_string),\n",
    "        'label': _int64_feature(label),\n",
    "        'text': _bytes_feature(text.encode('utf-8')),\n",
    "        'height': _int64_feature(image.shape[0]),\n",
    "        'width': _int64_feature(image.shape[1]),\n",
    "        'channels': _int64_feature(image.shape[2])\n",
    "    }\n",
    "    \n",
    "    # Create Example proto\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "# Write TFRecord file\n",
    "def write_tfrecords(filename, images, labels, texts):\n",
    "    \"\"\"Write data to TFRecord file\"\"\"\n",
    "    \n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        for image, label, text in zip(images, labels, texts):\n",
    "            example = serialize_example(image, label, text)\n",
    "            writer.write(example)\n",
    "    \n",
    "    print(f\"TFRecord file created: {filename}\")\n",
    "\n",
    "# Create and write TFRecord\n",
    "images, labels, texts = create_tfrecord_examples()\n",
    "tfrecord_filename = \"sample_data.tfrecord\"\n",
    "write_tfrecords(tfrecord_filename, images, labels, texts)\n",
    "\n",
    "# Verify file creation\n",
    "if os.path.exists(tfrecord_filename):\n",
    "    file_size = os.path.getsize(tfrecord_filename)\n",
    "    print(f\"TFRecord file size: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading TFRecord files\n",
    "def parse_tfrecord_fn(example):\n",
    "    \"\"\"Parse a single TFRecord example\"\"\"\n",
    "    \n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'text': tf.io.FixedLenFeature([], tf.string),\n",
    "        'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'channels': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    \n",
    "    # Parse the input tf.train.Example proto\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    \n",
    "    # Decode the image\n",
    "    image = tf.io.parse_tensor(example['image'], out_type=tf.uint8)\n",
    "    image = tf.reshape(image, [example['height'], example['width'], example['channels']])\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "    \n",
    "    # Extract other features\n",
    "    label = tf.cast(example['label'], tf.int32)\n",
    "    text = example['text']\n",
    "    \n",
    "    return {'image': image, 'text': text}, label\n",
    "\n",
    "# Create dataset from TFRecord\n",
    "def load_tfrecord_dataset(filename, batch_size=4):\n",
    "    \"\"\"Load and preprocess TFRecord dataset\"\"\"\n",
    "    \n",
    "    raw_dataset = tf.data.TFRecordDataset(filename)\n",
    "    \n",
    "    dataset = (raw_dataset\n",
    "              .map(parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "              .cache()\n",
    "              .shuffle(buffer_size=100)\n",
    "              .batch(batch_size)\n",
    "              .prefetch(tf.data.AUTOTUNE))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Load and inspect the dataset\n",
    "tfrecord_dataset = load_tfrecord_dataset(tfrecord_filename)\n",
    "\n",
    "print(\"TFRecord dataset structure:\")\n",
    "for features, labels in tfrecord_dataset.take(1):\n",
    "    print(f\"  Image batch shape: {features['image'].shape}\")\n",
    "    print(f\"  Text batch shape: {features['text'].shape}\")\n",
    "    print(f\"  Label batch shape: {labels.shape}\")\n",
    "    print(f\"  Sample text: {features['text'][0].numpy().decode()}\")\n",
    "\n",
    "# Visualize some examples\n",
    "def visualize_tfrecord_data(dataset, num_examples=4):\n",
    "    \"\"\"Visualize data from TFRecord dataset\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    for features, labels in dataset.take(1):\n",
    "        for i in range(min(num_examples, features['image'].shape[0])):\n",
    "            plt.subplot(2, 2, i + 1)\n",
    "            plt.imshow(features['image'][i].numpy())\n",
    "            plt.title(f\"Label: {labels[i].numpy()}\")\n",
    "            plt.text(0.5, -0.1, features['text'][i].numpy().decode()[:30] + \"...\", \n",
    "                    ha='center', transform=plt.gca().transAxes)\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_tfrecord_data(tfrecord_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced TFRecord features and sharding\n",
    "def write_sharded_tfrecords(base_filename, images, labels, texts, num_shards=3):\n",
    "    \"\"\"Write data to multiple TFRecord shards\"\"\"\n",
    "    \n",
    "    samples_per_shard = len(images) // num_shards\n",
    "    \n",
    "    for shard_idx in range(num_shards):\n",
    "        shard_filename = f\"{base_filename}-{shard_idx:05d}-of-{num_shards:05d}.tfrecord\"\n",
    "        \n",
    "        start_idx = shard_idx * samples_per_shard\n",
    "        if shard_idx == num_shards - 1:\n",
    "            # Last shard gets remaining samples\n",
    "            end_idx = len(images)\n",
    "        else:\n",
    "            end_idx = (shard_idx + 1) * samples_per_shard\n",
    "        \n",
    "        shard_images = images[start_idx:end_idx]\n",
    "        shard_labels = labels[start_idx:end_idx]\n",
    "        shard_texts = texts[start_idx:end_idx]\n",
    "        \n",
    "        write_tfrecords(shard_filename, shard_images, shard_labels, shard_texts)\n",
    "\n",
    "# Create sharded TFRecords\n",
    "write_sharded_tfrecords(\"sharded_data\", images, labels, texts)\n",
    "\n",
    "# Read from multiple sharded files\n",
    "def load_sharded_tfrecords(file_pattern, batch_size=4):\n",
    "    \"\"\"Load dataset from multiple TFRecord shards\"\"\"\n",
    "    \n",
    "    # Find all matching files\n",
    "    file_list = tf.io.matching_files(file_pattern)\n",
    "    \n",
    "    # Create dataset from multiple files\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(file_list)\n",
    "    \n",
    "    # Interleave reading from multiple files\n",
    "    dataset = dataset.interleave(\n",
    "        tf.data.TFRecordDataset,\n",
    "        cycle_length=tf.data.AUTOTUNE,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # Apply parsing and preprocessing\n",
    "    dataset = (dataset\n",
    "              .map(parse_tfrecord_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "              .cache()\n",
    "              .shuffle(buffer_size=100)\n",
    "              .batch(batch_size)\n",
    "              .prefetch(tf.data.AUTOTUNE))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Load sharded dataset\n",
    "sharded_dataset = load_sharded_tfrecords(\"sharded_data-*-of-*.tfrecord\")\n",
    "\n",
    "print(\"Sharded dataset loaded successfully\")\n",
    "print(f\"Number of batches: {len(list(sharded_dataset))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. tf.keras Preprocessing Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing with tf.keras\n",
    "text_data = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Machine learning is transforming artificial intelligence\",\n",
    "    \"TensorFlow makes deep learning accessible to everyone\",\n",
    "    \"Data preprocessing is crucial for model performance\",\n",
    "    \"Neural networks learn complex patterns from data\"\n",
    "]\n",
    "\n",
    "# TextVectorization layer\n",
    "text_vectorizer = tf.keras.utils.TextVectorization(\n",
    "    max_tokens=1000,\n",
    "    output_sequence_length=10,\n",
    "    output_mode='int'\n",
    ")\n",
    "\n",
    "# Adapt the layer to the text data\n",
    "text_vectorizer.adapt(text_data)\n",
    "\n",
    "# Create dataset and apply preprocessing\n",
    "text_dataset = tf.data.Dataset.from_tensor_slices(text_data)\n",
    "vectorized_dataset = text_dataset.map(text_vectorizer)\n",
    "\n",
    "print(\"Text preprocessing results:\")\n",
    "print(\"Original texts:\")\n",
    "for text in text_data[:3]:\n",
    "    print(f\"  '{text}'\")\n",
    "\n",
    "print(\"\\nVectorized texts:\")\n",
    "for vectorized in vectorized_dataset.take(3):\n",
    "    print(f\"  {vectorized.numpy()}\")\n",
    "\n",
    "print(f\"\\nVocabulary size: {text_vectorizer.vocabulary_size()}\")\n",
    "print(\"Sample vocabulary:\")\n",
    "vocab = text_vectorizer.get_vocabulary()[:20]\n",
    "for i, word in enumerate(vocab):\n",
    "    print(f\"  {i}: '{word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing with tf.keras\n",
    "# Create sample image data\n",
    "sample_images = tf.random.uniform((5, 100, 100, 3), maxval=255, dtype=tf.int32)\n",
    "sample_images = tf.cast(sample_images, tf.uint8)\n",
    "\n",
    "# Image preprocessing layers\n",
    "image_preprocessing = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),  # Normalize to [0, 1]\n",
    "    tf.keras.layers.Resizing(224, 224),  # Resize to standard size\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),  # Data augmentation\n",
    "    tf.keras.layers.RandomRotation(0.1),  # Random rotation\n",
    "    tf.keras.layers.RandomZoom(0.1),  # Random zoom\n",
    "])\n",
    "\n",
    "# Apply preprocessing\n",
    "def preprocess_images(images):\n",
    "    \"\"\"Apply preprocessing pipeline to images\"\"\"\n",
    "    return image_preprocessing(images, training=True)\n",
    "\n",
    "# Create image dataset\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(sample_images)\n",
    "preprocessed_dataset = image_dataset.map(preprocess_images)\n",
    "\n",
    "# Visualize preprocessing effects\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original images\n",
    "image_dataset_iter = iter(image_dataset)\n",
    "preprocessed_iter = iter(preprocessed_dataset)\n",
    "\n",
    "for i in range(3):\n",
    "    # Original image\n",
    "    original = next(image_dataset_iter)\n",
    "    axes[0, i].imshow(original.numpy())\n",
    "    axes[0, i].set_title(f\"Original {i+1}\")\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Preprocessed image\n",
    "    preprocessed = next(preprocessed_iter)\n",
    "    axes[1, i].imshow(preprocessed.numpy())\n",
    "    axes[1, i].set_title(f\"Preprocessed {i+1}\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering with tf.keras preprocessing\n",
    "# Create structured data\n",
    "structured_data = {\n",
    "    'numeric_feature_1': np.random.randn(100),\n",
    "    'numeric_feature_2': np.random.randn(100) * 10 + 5,\n",
    "    'categorical_feature_1': np.random.choice(['A', 'B', 'C', 'D'], 100),\n",
    "    'categorical_feature_2': np.random.choice(['X', 'Y', 'Z'], 100),\n",
    "    'text_feature': [f\"Text sample {i} with random content\" for i in range(100)]\n",
    "}\n",
    "\n",
    "# Create preprocessing layers for different feature types\n",
    "numeric_normalizer_1 = tf.keras.utils.Normalization()\n",
    "numeric_normalizer_2 = tf.keras.utils.Normalization()\n",
    "\n",
    "categorical_encoder_1 = tf.keras.utils.StringLookup(output_mode='one_hot')\n",
    "categorical_encoder_2 = tf.keras.utils.StringLookup(output_mode='one_hot')\n",
    "\n",
    "text_encoder = tf.keras.utils.TextVectorization(\n",
    "    max_tokens=100,\n",
    "    output_sequence_length=5,\n",
    "    output_mode='tf_idf'\n",
    ")\n",
    "\n",
    "# Adapt preprocessing layers\n",
    "numeric_normalizer_1.adapt(structured_data['numeric_feature_1'])\n",
    "numeric_normalizer_2.adapt(structured_data['numeric_feature_2'])\n",
    "categorical_encoder_1.adapt(structured_data['categorical_feature_1'])\n",
    "categorical_encoder_2.adapt(structured_data['categorical_feature_2'])\n",
    "text_encoder.adapt(structured_data['text_feature'])\n",
    "\n",
    "# Create preprocessing function\n",
    "def preprocess_structured_data(features):\n",
    "    \"\"\"Comprehensive preprocessing for structured data\"\"\"\n",
    "    \n",
    "    processed_features = {}\n",
    "    \n",
    "    # Numeric features\n",
    "    processed_features['norm_num_1'] = numeric_normalizer_1(features['numeric_feature_1'])\n",
    "    processed_features['norm_num_2'] = numeric_normalizer_2(features['numeric_feature_2'])\n",
    "    \n",
    "    # Categorical features\n",
    "    processed_features['cat_1_encoded'] = categorical_encoder_1(features['categorical_feature_1'])\n",
    "    processed_features['cat_2_encoded'] = categorical_encoder_2(features['categorical_feature_2'])\n",
    "    \n",
    "    # Text features\n",
    "    processed_features['text_encoded'] = text_encoder(features['text_feature'])\n",
    "    \n",
    "    return processed_features\n",
    "\n",
    "# Create dataset from structured data\n",
    "structured_dataset = tf.data.Dataset.from_tensor_slices(structured_data)\n",
    "preprocessed_structured = structured_dataset.map(preprocess_structured_data)\n",
    "\n",
    "# Inspect preprocessed structured data\n",
    "print(\"Structured data preprocessing results:\")\n",
    "for sample in preprocessed_structured.take(1):\n",
    "    for key, value in sample.items():\n",
    "        print(f\"{key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "        if 'encoded' in key:\n",
    "            print(f\"  Sample values: {value.numpy()[:5]}...\")\n",
    "        else:\n",
    "            print(f\"  Sample values: {value.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom Dataset Classes and Complex Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for complex data loading\n",
    "class CustomImageDataset:\n",
    "    \"\"\"Custom dataset class for image data with metadata\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, metadata=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.metadata = metadata or [{}] * len(image_paths)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def create_tf_dataset(self, batch_size=32, shuffle=True, augment=True):\n",
    "        \"\"\"Create tf.data.Dataset from the custom dataset\"\"\"\n",
    "        \n",
    "        # Create dataset from paths and labels\n",
    "        dataset = tf.data.Dataset.from_tensor_slices({\n",
    "            'image_path': self.image_paths,\n",
    "            'label': self.labels,\n",
    "            'metadata': self.metadata\n",
    "        })\n",
    "        \n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=len(self.image_paths))\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        dataset = dataset.map(\n",
    "            self._preprocess_function,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "        \n",
    "        if augment:\n",
    "            dataset = dataset.map(\n",
    "                self._augment_function,\n",
    "                num_parallel_calls=tf.data.AUTOTUNE\n",
    "            )\n",
    "        \n",
    "        dataset = (dataset\n",
    "                  .batch(batch_size)\n",
    "                  .prefetch(tf.data.AUTOTUNE))\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def _preprocess_function(self, sample):\n",
    "        \"\"\"Preprocess a single sample\"\"\"\n",
    "        # For demo, create random image instead of loading from path\n",
    "        image = tf.random.uniform((224, 224, 3), maxval=255, dtype=tf.int32)\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': sample['label'],\n",
    "            'metadata': sample['metadata']\n",
    "        }\n",
    "    \n",
    "    def _augment_function(self, sample):\n",
    "        \"\"\"Apply data augmentation\"\"\"\n",
    "        image = sample['image']\n",
    "        \n",
    "        # Random augmentations\n",
    "        if tf.random.uniform([]) > 0.5:\n",
    "            image = tf.image.flip_left_right(image)\n",
    "        \n",
    "        if tf.random.uniform([]) > 0.5:\n",
    "            image = tf.image.random_brightness(image, 0.2)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': sample['label'],\n",
    "            'metadata': sample['metadata']\n",
    "        }\n",
    "\n",
    "# Create custom dataset instance\n",
    "dummy_paths = [f\"image_{i}.jpg\" for i in range(50)]\n",
    "dummy_labels = np.random.randint(0, 5, 50)\n",
    "dummy_metadata = [{'source': f'camera_{i%3}', 'quality': np.random.choice(['high', 'medium', 'low'])} \n",
    "                 for i in range(50)]\n",
    "\n",
    "custom_dataset = CustomImageDataset(dummy_paths, dummy_labels, dummy_metadata)\n",
    "tf_dataset = custom_dataset.create_tf_dataset(batch_size=8)\n",
    "\n",
    "print(\"Custom dataset created successfully\")\n",
    "print(\"Dataset structure:\")\n",
    "for batch in tf_dataset.take(1):\n",
    "    print(f\"  Image batch shape: {batch['image'].shape}\")\n",
    "    print(f\"  Label batch shape: {batch['label'].shape}\")\n",
    "    print(f\"  Metadata batch: {len(batch['metadata'])}\")\n",
    "    print(f\"  Sample metadata: {batch['metadata'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-modal dataset pipeline\n",
    "class MultiModalDataset:\n",
    "    \"\"\"Dataset combining images, text, and structured features\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize preprocessing layers\n",
    "        self.text_vectorizer = tf.keras.utils.TextVectorization(\n",
    "            max_tokens=1000,\n",
    "            output_sequence_length=50\n",
    "        )\n",
    "        \n",
    "        self.image_preprocessor = tf.keras.Sequential([\n",
    "            tf.keras.layers.Rescaling(1./255),\n",
    "            tf.keras.layers.Resizing(224, 224)\n",
    "        ])\n",
    "        \n",
    "        self.numeric_normalizer = tf.keras.utils.Normalization()\n",
    "        \n",
    "    def prepare_preprocessing_layers(self, text_data, numeric_data):\n",
    "        \"\"\"Adapt preprocessing layers to data\"\"\"\n",
    "        self.text_vectorizer.adapt(text_data)\n",
    "        self.numeric_normalizer.adapt(numeric_data)\n",
    "    \n",
    "    def create_multimodal_dataset(self, images, texts, numeric_features, labels, batch_size=16):\n",
    "        \"\"\"Create multi-modal dataset\"\"\"\n",
    "        \n",
    "        # Prepare preprocessing layers\n",
    "        self.prepare_preprocessing_layers(texts, numeric_features)\n",
    "        \n",
    "        # Create dataset from all modalities\n",
    "        dataset = tf.data.Dataset.from_tensor_slices({\n",
    "            'image': images,\n",
    "            'text': texts,\n",
    "            'numeric': numeric_features,\n",
    "            'label': labels\n",
    "        })\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        dataset = dataset.map(\n",
    "            self._preprocess_multimodal,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "        \n",
    "        # Shuffle and batch\n",
    "        dataset = (dataset\n",
    "                  .shuffle(buffer_size=1000)\n",
    "                  .batch(batch_size)\n",
    "                  .prefetch(tf.data.AUTOTUNE))\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def _preprocess_multimodal(self, sample):\n",
    "        \"\"\"Preprocess all modalities\"\"\"\n",
    "        return {\n",
    "            'image': self.image_preprocessor(sample['image']),\n",
    "            'text': self.text_vectorizer(sample['text']),\n",
    "            'numeric': self.numeric_normalizer(sample['numeric']),\n",
    "            'label': sample['label']\n",
    "        }\n",
    "\n",
    "# Generate multi-modal data\n",
    "num_samples = 100\n",
    "mm_images = tf.random.uniform((num_samples, 128, 128, 3), maxval=255, dtype=tf.int32)\n",
    "mm_images = tf.cast(mm_images, tf.uint8)\n",
    "\n",
    "mm_texts = [f\"This is sample text description number {i} with content\" for i in range(num_samples)]\n",
    "mm_numeric = np.random.randn(num_samples, 5).astype(np.float32)\n",
    "mm_labels = np.random.randint(0, 3, num_samples)\n",
    "\n",
    "# Create multi-modal dataset\n",
    "mm_dataset_creator = MultiModalDataset()\n",
    "mm_dataset = mm_dataset_creator.create_multimodal_dataset(\n",
    "    mm_images, mm_texts, mm_numeric, mm_labels\n",
    ")\n",
    "\n",
    "print(\"Multi-modal dataset structure:\")\n",
    "for batch in mm_dataset.take(1):\n",
    "    print(f\"  Image shape: {batch['image'].shape}\")\n",
    "    print(f\"  Text shape: {batch['text'].shape}\")\n",
    "    print(f\"  Numeric shape: {batch['numeric'].shape}\")\n",
    "    print(f\"  Label shape: {batch['label'].shape}\")\n",
    "\n",
    "# Visualize multi-modal batch\n",
    "def visualize_multimodal_batch(dataset):\n",
    "    \"\"\"Visualize a batch from multi-modal dataset\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    for batch in dataset.take(1):\n",
    "        for i in range(min(4, batch['image'].shape[0])):\n",
    "            # Image\n",
    "            axes[0, i].imshow(batch['image'][i].numpy())\n",
    "            axes[0, i].set_title(f\"Label: {batch['label'][i].numpy()}\")\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Numeric features visualization\n",
    "            axes[1, i].bar(range(batch['numeric'].shape[1]), batch['numeric'][i].numpy())\n",
    "            axes[1, i].set_title(f\"Numeric Features\")\n",
    "            axes[1, i].set_xlabel(\"Feature Index\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show text samples\n",
    "    for batch in dataset.take(1):\n",
    "        print(\"\\nText samples (first 3 tokens of first 3 samples):\")\n",
    "        for i in range(min(3, batch['text'].shape[0])):\n",
    "            tokens = batch['text'][i].numpy()[:3]\n",
    "            print(f\"  Sample {i}: {tokens}\")\n",
    "\n",
    "visualize_multimodal_batch(mm_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Data Pipeline Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation data splitting\n",
    "def create_cv_datasets(dataset, k_folds=5):\n",
    "    \"\"\"Create k-fold cross-validation datasets\"\"\"\n",
    "    \n",
    "    # Convert dataset to list for splitting\n",
    "    data_list = list(dataset.unbatch())\n",
    "    dataset_size = len(data_list)\n",
    "    fold_size = dataset_size // k_folds\n",
    "    \n",
    "    cv_datasets = []\n",
    "    \n",
    "    for fold in range(k_folds):\n",
    "        # Calculate fold boundaries\n",
    "        start_idx = fold * fold_size\n",
    "        end_idx = start_idx + fold_size if fold < k_folds - 1 else dataset_size\n",
    "        \n",
    "        # Create validation set for this fold\n",
    "        val_data = data_list[start_idx:end_idx]\n",
    "        \n",
    "        # Create training set (everything else)\n",
    "        train_data = data_list[:start_idx] + data_list[end_idx:]\n",
    "        \n",
    "        # Convert back to tf.data.Dataset\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices(val_data)\n",
    "        \n",
    "        cv_datasets.append({\n",
    "            'train': train_dataset,\n",
    "            'val': val_dataset,\n",
    "            'fold': fold\n",
    "        })\n",
    "    \n",
    "    return cv_datasets\n",
    "\n",
    "# Example with simple dataset\n",
    "simple_data = list(range(100))\n",
    "simple_labels = np.random.randint(0, 3, 100)\n",
    "simple_dataset = tf.data.Dataset.from_tensor_slices((simple_data, simple_labels))\n",
    "\n",
    "cv_datasets = create_cv_datasets(simple_dataset)\n",
    "\n",
    "print(\"Cross-validation datasets created:\")\n",
    "for i, cv_data in enumerate(cv_datasets):\n",
    "    train_size = len(list(cv_data['train']))\n",
    "    val_size = len(list(cv_data['val']))\n",
    "    print(f\"  Fold {i}: Train size={train_size}, Val size={val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pipeline with error handling and monitoring\n",
    "class MonitoredDataPipeline:\n",
    "    \"\"\"Data pipeline with comprehensive monitoring and error handling\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'samples_processed': 0,\n",
    "            'errors': 0,\n",
    "            'processing_times': []\n",
    "        }\n",
    "        \n",
    "    def create_monitored_pipeline(self, raw_data, batch_size=32):\n",
    "        \"\"\"Create pipeline with monitoring and error handling\"\"\"\n",
    "        \n",
    "        # Create base dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(raw_data)\n",
    "        \n",
    "        # Add monitoring wrapper\n",
    "        dataset = dataset.map(\n",
    "            self._monitor_and_process,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "        \n",
    "        # Filter out failed samples (None values)\n",
    "        dataset = dataset.filter(lambda x: tf.not_equal(tf.size(x), 0))\n",
    "        \n",
    "        # Batch and optimize\n",
    "        dataset = (dataset\n",
    "                  .batch(batch_size)\n",
    "                  .prefetch(tf.data.AUTOTUNE))\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def _monitor_and_process(self, sample):\n",
    "        \"\"\"Process sample with monitoring and error handling\"\"\"\n",
    "        try:\n",
    "            # Simulate processing\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Potential error-prone processing\n",
    "            if tf.random.uniform([]) < 0.05:  # 5% error rate\n",
    "                return tf.constant([], dtype=tf.float32)  # Return empty tensor for error\n",
    "            \n",
    "            # Normal processing\n",
    "            processed = tf.cast(sample, tf.float32) * 2.0\n",
    "            \n",
    "            # Record metrics\n",
    "            processing_time = time.time() - start_time\n",
    "            self.metrics['processing_times'].append(processing_time)\n",
    "            self.metrics['samples_processed'] += 1\n",
    "            \n",
    "            return processed\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.metrics['errors'] += 1\n",
    "            return tf.constant([], dtype=tf.float32)  # Return empty tensor for error\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Get pipeline metrics\"\"\"\n",
    "        if self.metrics['processing_times']:\n",
    "            avg_time = np.mean(self.metrics['processing_times'])\n",
    "            std_time = np.std(self.metrics['processing_times'])\n",
    "        else:\n",
    "            avg_time = std_time = 0\n",
    "            \n",
    "        return {\n",
    "            'samples_processed': self.metrics['samples_processed'],\n",
    "            'errors': self.metrics['errors'],\n",
    "            'avg_processing_time': avg_time,\n",
    "            'std_processing_time': std_time,\n",
    "            'error_rate': self.metrics['errors'] / max(1, self.metrics['samples_processed'] + self.metrics['errors'])\n",
    "        }\n",
    "\n",
    "# Test monitored pipeline\n",
    "monitor_pipeline = MonitoredDataPipeline()\n",
    "test_data = np.random.randn(200).astype(np.float32)\n",
    "monitored_dataset = monitor_pipeline.create_monitored_pipeline(test_data)\n",
    "\n",
    "# Process some batches\n",
    "processed_batches = 0\n",
    "for batch in monitored_dataset.take(5):\n",
    "    processed_batches += 1\n",
    "    print(f\"Processed batch {processed_batches}, batch size: {batch.shape[0]}\")\n",
    "\n",
    "# Get metrics\n",
    "metrics = monitor_pipeline.get_metrics()\n",
    "print(\"\\nPipeline metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    if 'time' in key:\n",
    "        print(f\"  {key}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Production Pipeline Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration-driven pipeline\n",
    "class ConfigurableDataPipeline:\n",
    "    \"\"\"Production-ready configurable data pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.preprocessing_layers = {}\n",
    "        self._setup_preprocessing()\n",
    "    \n",
    "    def _setup_preprocessing(self):\n",
    "        \"\"\"Setup preprocessing layers based on configuration\"\"\"\n",
    "        \n",
    "        if 'text_preprocessing' in self.config:\n",
    "            text_config = self.config['text_preprocessing']\n",
    "            self.preprocessing_layers['text'] = tf.keras.utils.TextVectorization(\n",
    "                max_tokens=text_config.get('max_tokens', 1000),\n",
    "                output_sequence_length=text_config.get('sequence_length', 50)\n",
    "            )\n",
    "        \n",
    "        if 'image_preprocessing' in self.config:\n",
    "            img_config = self.config['image_preprocessing']\n",
    "            layers = [tf.keras.layers.Rescaling(1./255)]\n",
    "            \n",
    "            if 'target_size' in img_config:\n",
    "                size = img_config['target_size']\n",
    "                layers.append(tf.keras.layers.Resizing(size[0], size[1]))\n",
    "            \n",
    "            if img_config.get('augment', False):\n",
    "                layers.extend([\n",
    "                    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "                    tf.keras.layers.RandomRotation(0.1),\n",
    "                    tf.keras.layers.RandomZoom(0.1)\n",
    "                ])\n",
    "            \n",
    "            self.preprocessing_layers['image'] = tf.keras.Sequential(layers)\n",
    "    \n",
    "    def create_pipeline(self, data_source, mode='train'):\n",
    "        \"\"\"Create optimized pipeline based on configuration\"\"\"\n",
    "        \n",
    "        # Load data based on source type\n",
    "        if self.config['data_source']['type'] == 'tfrecord':\n",
    "            dataset = self._load_tfrecord(data_source)\n",
    "        elif self.config['data_source']['type'] == 'directory':\n",
    "            dataset = self._load_directory(data_source)\n",
    "        else:\n",
    "            dataset = self._load_tensor_slices(data_source)\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        dataset = dataset.map(\n",
    "            lambda x, y: self._preprocess_sample(x, y, mode),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "        \n",
    "        # Configure pipeline based on mode\n",
    "        if mode == 'train':\n",
    "            dataset = dataset.shuffle(self.config['training']['shuffle_buffer'])\n",
    "            dataset = dataset.repeat(self.config['training'].get('repeat', 1))\n",
    "        \n",
    "        # Batch and optimize\n",
    "        batch_size = self.config['training']['batch_size']\n",
    "        dataset = (dataset\n",
    "                  .batch(batch_size)\n",
    "                  .prefetch(tf.data.AUTOTUNE))\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "# Production configuration example\n",
    "production_config = {\n",
    "    'data_source': {\n",
    "        'type': 'tfrecord',\n",
    "        'pattern': 'data/*.tfrecord'\n",
    "    },\n",
    "    'text_preprocessing': {\n",
    "        'max_tokens': 10000,\n",
    "        'sequence_length': 128\n",
    "    },\n",
    "    'image_preprocessing': {\n",
    "        'target_size': [224, 224],\n",
    "        'augment': True\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': 32,\n",
    "        'shuffle_buffer': 1000,\n",
    "        'repeat': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Production pipeline configuration created\")\n",
    "print(f\"Configuration keys: {list(production_config.keys())}\")\n",
    "\n",
    "# Example usage would be:\n",
    "# pipeline = ConfigurableDataPipeline(production_config)\n",
    "# train_dataset = pipeline.create_pipeline(train_data, mode='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Benchmarking and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive pipeline benchmarking\n",
    "class PipelineBenchmark:\n",
    "    \"\"\"Benchmark different pipeline configurations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    \n",
    "    def benchmark_configurations(self, data, configs, num_batches=50):\n",
    "        \"\"\"Benchmark multiple pipeline configurations\"\"\"\n",
    "        \n",
    "        for config_name, config in configs.items():\n",
    "            print(f\"Benchmarking {config_name}...\")\n",
    "            \n",
    "            # Create dataset with configuration\n",
    "            dataset = self._create_dataset_with_config(data, config)\n",
    "            \n",
    "            # Measure performance\n",
    "            times = []\n",
    "            for _ in range(3):  # Multiple runs for accuracy\n",
    "                start_time = time.time()\n",
    "                for i, batch in enumerate(dataset):\n",
    "                    if i >= num_batches:\n",
    "                        break\n",
    "                    # Simulate model consumption\n",
    "                    _ = tf.reduce_mean(batch[0])\n",
    "                end_time = time.time()\n",
    "                times.append(end_time - start_time)\n",
    "            \n",
    "            avg_time = np.mean(times)\n",
    "            self.results[config_name] = {\n",
    "                'avg_time': avg_time,\n",
    "                'batches_per_sec': num_batches / avg_time,\n",
    "                'config': config\n",
    "            }\n",
    "    \n",
    "    def _create_dataset_with_config(self, data, config):\n",
    "        \"\"\"Create dataset with specific configuration\"\"\"\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "        \n",
    "        # Apply transformations based on config\n",
    "        if config.get('map_parallel'):\n",
    "            dataset = dataset.map(\n",
    "                lambda x: x * 2, \n",
    "                num_parallel_calls=tf.data.AUTOTUNE\n",
    "            )\n",
    "        else:\n",
    "            dataset = dataset.map(lambda x: x * 2)\n",
    "        \n",
    "        if config.get('cache'):\n",
    "            dataset = dataset.cache()\n",
    "        \n",
    "        if config.get('shuffle'):\n",
    "            dataset = dataset.shuffle(config.get('shuffle_buffer', 1000))\n",
    "        \n",
    "        dataset = dataset.batch(config.get('batch_size', 32))\n",
    "        \n",
    "        if config.get('prefetch'):\n",
    "            dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def print_results(self):\n",
    "        \"\"\"Print benchmark results\"\"\"\n",
    "        print(\"\\nBenchmark Results:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        sorted_results = sorted(\n",
    "            self.results.items(), \n",
    "            key=lambda x: x[1]['batches_per_sec'], \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        for config_name, result in sorted_results:\n",
    "            print(f\"{config_name:20} | {result['batches_per_sec']:8.2f} batches/sec | {result['avg_time']:6.3f}s total\")\n",
    "\n",
    "# Define benchmark configurations\n",
    "benchmark_configs = {\n",
    "    'baseline': {\n",
    "        'batch_size': 32\n",
    "    },\n",
    "    'with_caching': {\n",
    "        'batch_size': 32,\n",
    "        'cache': True\n",
    "    },\n",
    "    'with_prefetch': {\n",
    "        'batch_size': 32,\n",
    "        'prefetch': True\n",
    "    },\n",
    "    'with_parallel': {\n",
    "        'batch_size': 32,\n",
    "        'map_parallel': True\n",
    "    },\n",
    "    'optimized': {\n",
    "        'batch_size': 32,\n",
    "        'cache': True,\n",
    "        'prefetch': True,\n",
    "        'map_parallel': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run benchmarks\n",
    "benchmark_data = (np.random.randn(1000, 100, 100, 3).astype(np.float32), \n",
    "                 np.random.randint(0, 10, 1000))\n",
    "\n",
    "benchmarker = PipelineBenchmark()\n",
    "benchmarker.benchmark_configurations(benchmark_data, benchmark_configs, num_batches=30)\n",
    "benchmarker.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**File Location:** `notebooks/01_tensorflow_foundations/02_data_pipelines_tfrecords.ipynb`\n",
    "\n",
    "This comprehensive notebook covered the essential aspects of TensorFlow data pipelines:\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "1. **tf.data API Fundamentals**: Dataset creation, transformations, and chaining operations\n",
    "2. **Performance Optimization**: Caching, prefetching, parallel processing, and interleaving\n",
    "3. **TFRecords**: Creating, writing, reading, and sharding for efficient data storage\n",
    "4. **tf.keras Preprocessing**: Text vectorization, image preprocessing, and structured data handling\n",
    "5. **Custom Datasets**: Building flexible, reusable dataset classes\n",
    "6. **Multi-modal Pipelines**: Combining images, text, and structured data\n",
    "7. **Production Patterns**: Cross-validation, monitoring, error handling, and configuration-driven pipelines\n",
    "8. **Performance Benchmarking**: Measuring and optimizing pipeline throughput\n",
    "\n",
    "### Critical Performance Insights:\n",
    "- **Prefetching** overlaps data preprocessing with model training\n",
    "- **Caching** eliminates redundant computations across epochs  \n",
    "- **Parallel mapping** utilizes multiple CPU cores effectively\n",
    "- **TFRecords** provide optimized binary format for large datasets\n",
    "- **Proper batching** maximizes GPU utilization\n",
    "\n",
    "### Production-Ready Features:\n",
    "- Error handling and monitoring capabilities\n",
    "- Configuration-driven pipeline construction\n",
    "- Multi-modal data integration\n",
    "- Cross-validation data splitting\n",
    "- Comprehensive performance benchmarking\n",
    "\n",
    "### Next Steps:\n",
    "- Apply these pipelines in debugging and profiling (Notebook 03)\n",
    "- Integrate with tf.keras models for end-to-end training\n",
    "- Scale to distributed training scenarios\n",
    "\n",
    "This foundation ensures efficient, scalable data handling for any TensorFlow project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
