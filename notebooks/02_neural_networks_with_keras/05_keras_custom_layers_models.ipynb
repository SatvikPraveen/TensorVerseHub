{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 keras custom layers models\n",
    "**Location: TensorVerseHub/notebooks/02_neural_networks_with_keras/05_keras_custom_layers_models.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "# TODO: Add comprehensive implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom tf.keras Layers, Models & Training Loops\n",
    "\n",
    "**File Location:** `notebooks/02_neural_networks_with_keras/05_custom_layers_models_keras.ipynb`\n",
    "\n",
    "Build custom tf.keras layers, implement advanced model architectures, and create custom training loops. Master the art of extending TensorFlow with your own components for cutting-edge research and specialized applications.\n",
    "\n",
    "## Learning Objectives\n",
    "- Build custom tf.keras layers with proper state management\n",
    "- Implement custom loss functions and metrics\n",
    "- Create advanced custom training loops with tf.GradientTape\n",
    "- Handle complex model architectures with custom components\n",
    "- Implement attention mechanisms and advanced layers\n",
    "- Master gradient computation and optimization strategies\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Custom Layer Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Basic custom layer template\n",
    "class BasicCustomLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Template for basic custom layer implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, units=32, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Create layer weights\"\"\"\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "            name='kernel'\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "            name='bias'\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        output = tf.matmul(inputs, self.w) + self.b\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Return layer configuration for serialization\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'units': self.units,\n",
    "            'activation': tf.keras.activations.serialize(self.activation)\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Advanced custom layers\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Residual block with optional bottleneck\"\"\"\n",
    "    \n",
    "    def __init__(self, filters, kernel_size=3, stride=1, use_bottleneck=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.use_bottleneck = use_bottleneck\n",
    "        \n",
    "        if use_bottleneck:\n",
    "            self.conv1 = tf.keras.layers.Dense(filters // 4, activation='relu')\n",
    "            self.conv2 = tf.keras.layers.Dense(filters // 4, activation='relu')\n",
    "            self.conv3 = tf.keras.layers.Dense(filters, activation=None)\n",
    "        else:\n",
    "            self.conv1 = tf.keras.layers.Dense(filters, activation='relu')\n",
    "            self.conv2 = tf.keras.layers.Dense(filters, activation=None)\n",
    "        \n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization() if use_bottleneck else None\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(0.3)\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.activation = tf.keras.layers.Activation('relu')\n",
    "        \n",
    "        # Skip connection adjustment\n",
    "        self.use_projection = False\n",
    "        if stride != 1:\n",
    "            self.use_projection = True\n",
    "            self.projection = tf.keras.layers.Dense(filters)\n",
    "            self.projection_bn = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Main path\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        if self.use_bottleneck:\n",
    "            x = self.conv2(x)\n",
    "            x = self.bn2(x, training=training)\n",
    "            x = self.dropout(x, training=training)\n",
    "            x = self.conv3(x)\n",
    "            x = self.bn3(x, training=training)\n",
    "        else:\n",
    "            x = self.conv2(x)\n",
    "            x = self.bn2(x, training=training)\n",
    "        \n",
    "        # Skip connection\n",
    "        shortcut = inputs\n",
    "        if self.use_projection:\n",
    "            shortcut = self.projection(shortcut)\n",
    "            shortcut = self.projection_bn(shortcut, training=training)\n",
    "        \n",
    "        # Add and activate\n",
    "        x = self.add([x, shortcut])\n",
    "        return self.activation(x)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'stride': self.stride,\n",
    "            'use_bottleneck': self.use_bottleneck\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Custom multi-head self-attention layer\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads=8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.depth = embed_dim // num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(embed_dim)\n",
    "        self.wk = tf.keras.layers.Dense(embed_dim)\n",
    "        self.wv = tf.keras.layers.Dense(embed_dim)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth)\"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        \"\"\"Calculate the attention weights\"\"\"\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        \n",
    "        # Scale matmul_qk\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        \n",
    "        # Linear transformations\n",
    "        q = self.wq(inputs)\n",
    "        k = self.wk(inputs)\n",
    "        v = self.wv(inputs)\n",
    "        \n",
    "        # Split heads\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        # Attention\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention_output, (batch_size, seq_len, self.embed_dim))\n",
    "        \n",
    "        # Final linear layer\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        # Add & norm\n",
    "        output = self.layer_norm(output + inputs)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Test custom layers\n",
    "print(\"=== Testing Custom Layers ===\")\n",
    "\n",
    "# Create sample data\n",
    "X_sample = np.random.randn(100, 20).astype(np.float32)\n",
    "y_sample = np.random.randint(0, 3, 100)\n",
    "\n",
    "# Test basic custom layer\n",
    "basic_layer = BasicCustomLayer(units=64, activation='relu')\n",
    "basic_output = basic_layer(X_sample[:5])\n",
    "print(f\"Basic Custom Layer Output Shape: {basic_output.shape}\")\n",
    "\n",
    "# Test residual block\n",
    "residual_layer = ResidualBlock(filters=64, use_bottleneck=True)\n",
    "residual_output = residual_layer(X_sample[:5])\n",
    "print(f\"Residual Block Output Shape: {residual_output.shape}\")\n",
    "\n",
    "# Test attention layer (reshape data for sequence)\n",
    "X_sequence = X_sample.reshape(100, 4, 5)  # (batch, seq_len, embed_dim)\n",
    "attention_layer = MultiHeadSelfAttention(embed_dim=5, num_heads=1)\n",
    "attention_output = attention_layer(X_sequence[:5])\n",
    "print(f\"Attention Layer Output Shape: {attention_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specialized custom layers\n",
    "class GatedLinearUnit(tf.keras.layers.Layer):\n",
    "    \"\"\"Gated Linear Unit for improved gradient flow\"\"\"\n",
    "    \n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.dense_gate = tf.keras.layers.Dense(units)\n",
    "        self.dense_value = tf.keras.layers.Dense(units)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        gate = tf.nn.sigmoid(self.dense_gate(inputs))\n",
    "        value = self.dense_value(inputs)\n",
    "        return gate * value\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config\n",
    "\n",
    "class SwishActivation(tf.keras.layers.Layer):\n",
    "    \"\"\"Custom Swish activation layer\"\"\"\n",
    "    \n",
    "    def __init__(self, beta=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.beta = beta\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs * tf.nn.sigmoid(self.beta * inputs)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'beta': self.beta})\n",
    "        return config\n",
    "\n",
    "class LayerScale(tf.keras.layers.Layer):\n",
    "    \"\"\"Layer scaling for improved training stability\"\"\"\n",
    "    \n",
    "    def __init__(self, init_value=1e-4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.init_value = init_value\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.scale = self.add_weight(\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=tf.keras.initializers.Constant(self.init_value),\n",
    "            trainable=True,\n",
    "            name='scale'\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs * self.scale\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'init_value': self.init_value})\n",
    "        return config\n",
    "\n",
    "class NoiseRegularization(tf.keras.layers.Layer):\n",
    "    \"\"\"Add noise during training for regularization\"\"\"\n",
    "    \n",
    "    def __init__(self, noise_stddev=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.noise_stddev = noise_stddev\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(inputs), stddev=self.noise_stddev)\n",
    "            return inputs + noise\n",
    "        return inputs\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'noise_stddev': self.noise_stddev})\n",
    "        return config\n",
    "\n",
    "class FeatureSqueezeExcitation(tf.keras.layers.Layer):\n",
    "    \"\"\"Squeeze and Excitation block for feature recalibration\"\"\"\n",
    "    \n",
    "    def __init__(self, reduction_ratio=16, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.feature_dim = input_shape[-1]\n",
    "        reduced_dim = max(1, self.feature_dim // self.reduction_ratio)\n",
    "        \n",
    "        self.squeeze = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.excitation = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(reduced_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.feature_dim, activation='sigmoid')\n",
    "        ])\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # For 2D inputs, add a dummy sequence dimension\n",
    "        if len(inputs.shape) == 2:\n",
    "            x = tf.expand_dims(inputs, axis=1)\n",
    "            squeeze_output = tf.squeeze(self.squeeze(x), axis=1)\n",
    "        else:\n",
    "            squeeze_output = self.squeeze(inputs)\n",
    "        \n",
    "        # Excitation\n",
    "        excitation_output = self.excitation(squeeze_output)\n",
    "        \n",
    "        # Scale original input\n",
    "        if len(inputs.shape) == 2:\n",
    "            return inputs * excitation_output\n",
    "        else:\n",
    "            return inputs * tf.expand_dims(excitation_output, axis=1)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'reduction_ratio': self.reduction_ratio})\n",
    "        return config\n",
    "\n",
    "# Position encoding for transformers\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    \"\"\"Positional encoding for transformer architectures\"\"\"\n",
    "    \n",
    "    def __init__(self, max_position=1000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_position = max_position\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        position = tf.range(self.max_position, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, self.d_model, 2, dtype=tf.float32) * \n",
    "                         -(tf.math.log(10000.0) / self.d_model))\n",
    "        \n",
    "        pos_encoding = tf.zeros((self.max_position, self.d_model))\n",
    "        pos_encoding = tf.concat([\n",
    "            tf.sin(position * div_term),\n",
    "            tf.cos(position * div_term)\n",
    "        ], axis=1)\n",
    "        \n",
    "        if self.d_model % 2 == 1:\n",
    "            pos_encoding = pos_encoding[:, :-1]\n",
    "        \n",
    "        self.pos_encoding = tf.Variable(\n",
    "            pos_encoding, trainable=False, name='positional_encoding'\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encoding[:seq_len, :]\n",
    "\n",
    "# Test advanced layers\n",
    "print(\"\\n=== Testing Advanced Custom Layers ===\")\n",
    "\n",
    "# Test GLU\n",
    "glu_layer = GatedLinearUnit(32)\n",
    "glu_output = glu_layer(X_sample[:5])\n",
    "print(f\"GLU Output Shape: {glu_output.shape}\")\n",
    "\n",
    "# Test Swish activation\n",
    "swish_layer = SwishActivation(beta=1.5)\n",
    "swish_output = swish_layer(X_sample[:5])\n",
    "print(f\"Swish Output Shape: {swish_output.shape}\")\n",
    "\n",
    "# Test Squeeze and Excitation\n",
    "se_layer = FeatureSqueezeExcitation(reduction_ratio=8)\n",
    "se_output = se_layer(X_sample[:5])\n",
    "print(f\"SE Output Shape: {se_output.shape}\")\n",
    "\n",
    "# Test positional encoding\n",
    "pos_enc = PositionalEncoding(max_position=100)\n",
    "pos_output = pos_enc(X_sequence[:5])\n",
    "print(f\"Positional Encoding Output Shape: {pos_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Loss Functions and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss functions\n",
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Focal Loss for addressing class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.25, gamma=2.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # Convert to one-hot if needed\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = tf.one_hot(tf.cast(y_true, tf.int32), tf.shape(y_pred)[-1])\n",
    "        \n",
    "        # Calculate cross entropy\n",
    "        ce_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "        \n",
    "        # Calculate focal weight\n",
    "        p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        alpha_t = tf.where(tf.equal(y_true, 1), self.alpha, 1 - self.alpha)\n",
    "        focal_weight = alpha_t * tf.pow(1 - p_t, self.gamma)\n",
    "        \n",
    "        return tf.reduce_mean(focal_weight * ce_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'alpha': self.alpha, 'gamma': self.gamma})\n",
    "        return config\n",
    "\n",
    "class ContrastiveLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Contrastive loss for similarity learning\"\"\"\n",
    "    \n",
    "    def __init__(self, margin=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.margin = margin\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_pred should be distances, y_true should be 0/1 (similar/dissimilar)\n",
    "        square_pred = tf.square(y_pred)\n",
    "        margin_square = tf.square(tf.maximum(self.margin - y_pred, 0))\n",
    "        \n",
    "        loss = tf.reduce_mean(\n",
    "            y_true * square_pred + (1 - y_true) * margin_square\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'margin': self.margin})\n",
    "        return config\n",
    "\n",
    "class LabelSmoothingLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Label smoothing cross-entropy loss\"\"\"\n",
    "    \n",
    "    def __init__(self, smoothing=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = tf.one_hot(tf.cast(y_true, tf.int32), tf.shape(y_pred)[-1])\n",
    "        \n",
    "        num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
    "        smooth_positives = 1.0 - self.smoothing\n",
    "        smooth_negatives = self.smoothing / num_classes\n",
    "        \n",
    "        smoothed_labels = y_true * smooth_positives + smooth_negatives\n",
    "        \n",
    "        return tf.keras.losses.categorical_crossentropy(smoothed_labels, y_pred)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'smoothing': self.smoothing})\n",
    "        return config\n",
    "\n",
    "# Custom metrics\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    \"\"\"F1 Score metric for classification\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, average='macro', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.average = average\n",
    "        \n",
    "        self.true_positives = self.add_weight(\n",
    "            'true_positives', shape=(num_classes,), initializer='zeros'\n",
    "        )\n",
    "        self.false_positives = self.add_weight(\n",
    "            'false_positives', shape=(num_classes,), initializer='zeros'\n",
    "        )\n",
    "        self.false_negatives = self.add_weight(\n",
    "            'false_negatives', shape=(num_classes,), initializer='zeros'\n",
    "        )\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        if len(y_true.shape) > 1:\n",
    "            y_true = tf.argmax(y_true, axis=-1)\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        \n",
    "        for i in range(self.num_classes):\n",
    "            true_i = tf.equal(y_true, i)\n",
    "            pred_i = tf.equal(y_pred, i)\n",
    "            \n",
    "            tp = tf.reduce_sum(tf.cast(tf.logical_and(true_i, pred_i), tf.float32))\n",
    "            fp = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not(true_i), pred_i), tf.float32))\n",
    "            fn = tf.reduce_sum(tf.cast(tf.logical_and(true_i, tf.logical_not(pred_i)), tf.float32))\n",
    "            \n",
    "            self.true_positives[i].assign_add(tp)\n",
    "            self.false_positives[i].assign_add(fp)\n",
    "            self.false_negatives[i].assign_add(fn)\n",
    "    \n",
    "    def result(self):\n",
    "        precision = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "        \n",
    "        if self.average == 'macro':\n",
    "            return tf.reduce_mean(f1)\n",
    "        elif self.average == 'weighted':\n",
    "            weights = self.true_positives + self.false_negatives\n",
    "            return tf.reduce_sum(f1 * weights) / tf.reduce_sum(weights)\n",
    "        else:\n",
    "            return f1\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(tf.zeros_like(self.true_positives))\n",
    "        self.false_positives.assign(tf.zeros_like(self.false_positives))\n",
    "        self.false_negatives.assign(tf.zeros_like(self.false_negatives))\n",
    "\n",
    "class TopKCategoricalAccuracy(tf.keras.metrics.Metric):\n",
    "    \"\"\"Top-K categorical accuracy\"\"\"\n",
    "    \n",
    "    def __init__(self, k=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.k = k\n",
    "        self.total = self.add_weight('total', initializer='zeros')\n",
    "        self.count = self.add_weight('count', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = tf.one_hot(tf.cast(y_true, tf.int32), tf.shape(y_pred)[-1])\n",
    "        \n",
    "        top_k_pred = tf.nn.top_k(y_pred, k=self.k)\n",
    "        y_true_labels = tf.argmax(y_true, axis=-1)\n",
    "        \n",
    "        matches = tf.reduce_any(\n",
    "            tf.equal(tf.expand_dims(y_true_labels, axis=-1), top_k_pred.indices),\n",
    "            axis=-1\n",
    "        )\n",
    "        \n",
    "        self.total.assign_add(tf.reduce_sum(tf.cast(matches, tf.float32)))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "    \n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.total.assign(0.0)\n",
    "        self.count.assign(0.0)\n",
    "\n",
    "# Test custom losses and metrics\n",
    "print(\"\\n=== Testing Custom Loss Functions and Metrics ===\")\n",
    "\n",
    "# Create sample data for testing\n",
    "y_true_sample = tf.constant([0, 1, 2, 1, 0])\n",
    "y_pred_sample = tf.constant([\n",
    "    [0.8, 0.1, 0.1],\n",
    "    [0.2, 0.7, 0.1],\n",
    "    [0.1, 0.2, 0.7],\n",
    "    [0.3, 0.6, 0.1],\n",
    "    [0.9, 0.05, 0.05]\n",
    "], dtype=tf.float32)\n",
    "\n",
    "# Test Focal Loss\n",
    "focal_loss = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "focal_loss_value = focal_loss(y_true_sample, y_pred_sample)\n",
    "print(f\"Focal Loss: {focal_loss_value:.4f}\")\n",
    "\n",
    "# Test Label Smoothing Loss\n",
    "smooth_loss = LabelSmoothingLoss(smoothing=0.1)\n",
    "smooth_loss_value = smooth_loss(y_true_sample, y_pred_sample)\n",
    "print(f\"Label Smoothing Loss: {smooth_loss_value:.4f}\")\n",
    "\n",
    "# Test F1 Score\n",
    "f1_metric = F1Score(num_classes=3, average='macro')\n",
    "f1_metric.update_state(y_true_sample, y_pred_sample)\n",
    "f1_score = f1_metric.result()\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "# Test Top-K Accuracy\n",
    "topk_metric = TopKCategoricalAccuracy(k=2)\n",
    "topk_metric.update_state(y_true_sample, y_pred_sample)\n",
    "topk_accuracy = topk_metric.result()\n",
    "print(f\"Top-2 Accuracy: {topk_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced custom training loop implementation\n",
    "class CustomTrainer:\n",
    "    \"\"\"Advanced custom training loop with multiple optimizers and schedulers\"\"\"\n",
    "    \n",
    "    def __init__(self, model, loss_fn, optimizer, metrics=None):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics or []\n",
    "        \n",
    "        # Training state\n",
    "        self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        self.val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        \n",
    "        # History\n",
    "        self.history = {'train_loss': [], 'val_loss': []}\n",
    "        for metric in self.metrics:\n",
    "            self.history[f'train_{metric.name}'] = []\n",
    "            self.history[f'val_{metric.name}'] = []\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, x_batch, y_batch):\n",
    "        \"\"\"Single training step with gradient computation\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(x_batch, training=True)\n",
    "            loss = self.loss_fn(y_batch, predictions)\n",
    "            \n",
    "            # Add regularization losses\n",
    "            if self.model.losses:\n",
    "                loss += tf.add_n(self.model.losses)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        \n",
    "        # Apply gradients\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.train_loss.update_state(loss)\n",
    "        for metric in self.metrics:\n",
    "            metric.update_state(y_batch, predictions)\n",
    "        \n",
    "        return loss, predictions\n",
    "    \n",
    "    @tf.function\n",
    "    def val_step(self, x_batch, y_batch):\n",
    "        \"\"\"Single validation step\"\"\"\n",
    "        predictions = self.model(x_batch, training=False)\n",
    "        loss = self.loss_fn(y_batch, predictions)\n",
    "        \n",
    "        self.val_loss.update_state(loss)\n",
    "        for metric in self.metrics:\n",
    "            metric.update_state(y_batch, predictions)\n",
    "        \n",
    "        return loss, predictions\n",
    "    \n",
    "    def fit(self, train_dataset, val_dataset=None, epochs=10, \n",
    "            callbacks=None, verbose=1):\n",
    "        \"\"\"Custom fit method\"\"\"\n",
    "        \n",
    "        callbacks = callbacks or []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if verbose:\n",
    "                print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "            \n",
    "            # Reset metrics\n",
    "            self.train_loss.reset_states()\n",
    "            self.val_loss.reset_states()\n",
    "            for metric in self.metrics:\n",
    "                metric.reset_states()\n",
    "            \n",
    "            # Training phase\n",
    "            train_batches = 0\n",
    "            for x_batch, y_batch in train_dataset:\n",
    "                loss, predictions = self.train_step(x_batch, y_batch)\n",
    "                train_batches += 1\n",
    "                \n",
    "                if verbose and train_batches % 10 == 0:\n",
    "                    print('.', end='', flush=True)\n",
    "            \n",
    "            # Validation phase\n",
    "            if val_dataset is not None:\n",
    "                for x_batch, y_batch in val_dataset:\n",
    "                    val_loss, val_predictions = self.val_step(x_batch, y_batch)\n",
    "            \n",
    "            # Log metrics\n",
    "            epoch_logs = {\n",
    "                'train_loss': self.train_loss.result().numpy(),\n",
    "                'val_loss': self.val_loss.result().numpy() if val_dataset else 0.0\n",
    "            }\n",
    "            \n",
    "            for metric in self.metrics:\n",
    "                epoch_logs[f'train_{metric.name}'] = metric.result().numpy()\n",
    "                if val_dataset is not None:\n",
    "                    # Note: This is simplified - in practice you'd need separate val metrics\n",
    "                    epoch_logs[f'val_{metric.name}'] = metric.result().numpy()\n",
    "            \n",
    "            # Update history\n",
    "            for key, value in epoch_logs.items():\n",
    "                if key in self.history:\n",
    "                    self.history[key].append(value)\n",
    "            \n",
    "            if verbose:\n",
    "                metrics_str = \" - \".join([f\"{k}: {v:.4f}\" for k, v in epoch_logs.items()])\n",
    "                print(f\"\\n{metrics_str}\")\n",
    "            \n",
    "            # Run callbacks\n",
    "            for callback in callbacks:\n",
    "                if hasattr(callback, 'on_epoch_end'):\n",
    "                    callback.on_epoch_end(epoch, epoch_logs)\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "# Advanced training with multiple objectives\n",
    "class MultiObjectiveTrainer:\n",
    "    \"\"\"Training with multiple loss functions and objectives\"\"\"\n",
    "    \n",
    "    def __init__(self, model, loss_functions, loss_weights, optimizers):\n",
    "        self.model = model\n",
    "        self.loss_functions = loss_functions  # Dict of loss functions\n",
    "        self.loss_weights = loss_weights\n",
    "        self.optimizers = optimizers  # Dict of optimizers for different parts\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.loss_trackers = {\n",
    "            name: tf.keras.metrics.Mean(name=f'{name}_loss')\n",
    "            for name in loss_functions.keys()\n",
    "        }\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name='total_loss')\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, inputs, targets):\n",
    "        \"\"\"Multi-objective training step\"\"\"\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            predictions = self.model(inputs, training=True)\n",
    "            \n",
    "            # Compute individual losses\n",
    "            losses = {}\n",
    "            total_loss = 0\n",
    "            \n",
    "            for name, loss_fn in self.loss_functions.items():\n",
    "                if name in targets:\n",
    "                    loss_value = loss_fn(targets[name], predictions[name])\n",
    "                    losses[name] = loss_value\n",
    "                    total_loss += self.loss_weights[name] * loss_value\n",
    "            \n",
    "            # Add regularization\n",
    "            if self.model.losses:\n",
    "                total_loss += tf.add_n(self.model.losses)\n",
    "        \n",
    "        # Compute gradients for each optimizer\n",
    "        for opt_name, optimizer in self.optimizers.items():\n",
    "            if opt_name in losses:\n",
    "                # Get variables for this optimizer\n",
    "                variables = self.get_variables_for_optimizer(opt_name)\n",
    "                gradients = tape.gradient(losses[opt_name], variables)\n",
    "                optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        for name, loss_value in losses.items():\n",
    "            self.loss_trackers[name].update_state(loss_value)\n",
    "        \n",
    "        del tape\n",
    "        return losses, predictions\n",
    "    \n",
    "    def get_variables_for_optimizer(self, optimizer_name):\n",
    "        \"\"\"Get trainable variables for specific optimizer\"\"\"\n",
    "        # This would be customized based on your model architecture\n",
    "        # For example, different optimizers for different parts of the model\n",
    "        if optimizer_name == 'main':\n",
    "            return self.model.trainable_variables\n",
    "        else:\n",
    "            # Return subset of variables\n",
    "            return [v for v in self.model.trainable_variables if optimizer_name in v.name]\n",
    "\n",
    "# Gradient accumulation trainer\n",
    "class GradientAccumulationTrainer:\n",
    "    \"\"\"Training with gradient accumulation for large effective batch sizes\"\"\"\n",
    "    \n",
    "    def __init__(self, model, loss_fn, optimizer, accumulation_steps=4):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        self.accumulated_gradients = []\n",
    "    \n",
    "    def initialize_accumulated_gradients(self):\n",
    "        \"\"\"Initialize accumulated gradients\"\"\"\n",
    "        self.accumulated_gradients = [\n",
    "            tf.Variable(tf.zeros_like(var), trainable=False)\n",
    "            for var in self.model.trainable_variables\n",
    "        ]\n",
    "    \n",
    "    @tf.function\n",
    "    def accumulate_gradients(self, x_batch, y_batch):\n",
    "        \"\"\"Accumulate gradients from a mini-batch\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(x_batch, training=True)\n",
    "            loss = self.loss_fn(y_batch, predictions) / self.accumulation_steps\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        \n",
    "        # Accumulate gradients\n",
    "        for i, grad in enumerate(gradients):\n",
    "            if grad is not None:\n",
    "                self.accumulated_gradients[i].assign_add(grad)\n",
    "        \n",
    "        self.train_loss.update_state(loss * self.accumulation_steps)\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def apply_accumulated_gradients(self):\n",
    "        \"\"\"Apply accumulated gradients and reset\"\"\"\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(self.accumulated_gradients, self.model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        # Reset accumulated gradients\n",
    "        for accumulated_grad in self.accumulated_gradients:\n",
    "            accumulated_grad.assign(tf.zeros_like(accumulated_grad))\n",
    "    \n",
    "    def fit(self, dataset, epochs=10, steps_per_epoch=None):\n",
    "        \"\"\"Training with gradient accumulation\"\"\"\n",
    "        \n",
    "        self.initialize_accumulated_gradients()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "            self.train_loss.reset_states()\n",
    "            \n",
    "            step = 0\n",
    "            for x_batch, y_batch in dataset:\n",
    "                # Accumulate gradients\n",
    "                self.accumulate_gradients(x_batch, y_batch)\n",
    "                step += 1\n",
    "                \n",
    "                # Apply gradients every accumulation_steps\n",
    "                if step % self.accumulation_steps == 0:\n",
    "                    self.apply_accumulated_gradients()\n",
    "                \n",
    "                if steps_per_epoch and step >= steps_per_epoch:\n",
    "                    break\n",
    "                \n",
    "                if step % 50 == 0:\n",
    "                    print('.', end='', flush=True)\n",
    "            \n",
    "            # Apply remaining gradients if any\n",
    "            if step % self.accumulation_steps != 0:\n",
    "                self.apply_accumulated_gradients()\n",
    "            \n",
    "            print(f\"\\nTrain Loss: {self.train_loss.result():.4f}\")\n",
    "\n",
    "# Test custom training loops\n",
    "print(\"\\n=== Testing Custom Training Loops ===\")\n",
    "\n",
    "# Create sample model and data\n",
    "sample_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "X_train, y_train = make_classification(n_samples=800, n_features=20, n_classes=3, random_state=42)\n",
    "X_val, y_val = make_classification(n_samples=200, n_features=20, n_classes=3, random_state=24)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.astype(np.float32), y_train))\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val.astype(np.float32), y_val))\n",
    "val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Test basic custom trainer\n",
    "custom_trainer = CustomTrainer(\n",
    "    model=sample_model,\n",
    "    loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')]\n",
    ")\n",
    "\n",
    "print(\"Training with custom trainer...\")\n",
    "history = custom_trainer.fit(\n",
    "    train_dataset, val_dataset, \n",
    "    epochs=5, verbose=1\n",
    ")\n",
    "\n",
    "print(f\"Final train accuracy: {history['train_accuracy'][-1]:.4f}\")\n",
    "print(f\"Final val accuracy: {history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Test gradient accumulation trainer\n",
    "print(\"\\nTesting gradient accumulation trainer...\")\n",
    "ga_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "ga_trainer = GradientAccumulationTrainer(\n",
    "    model=ga_model,\n",
    "    loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    accumulation_steps=4\n",
    ")\n",
    "\n",
    "ga_trainer.fit(train_dataset, epochs=3, steps_per_epoch=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Model Architectures with Custom Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete custom model using all components\n",
    "class AdvancedTransformerBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Advanced transformer block with custom components\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(embed_dim)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # Layer scaling\n",
    "        self.layer_scale1 = LayerScale()\n",
    "        self.layer_scale2 = LayerScale()\n",
    "        \n",
    "        # Squeeze and excitation\n",
    "        self.se_block = FeatureSqueezeExcitation(reduction_ratio=8)\n",
    "    \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # Multi-head attention with residual connection\n",
    "        attn_output = self.attention(inputs, training=training, mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        attn_output = self.layer_scale1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        # Feed-forward network with residual connection\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        ffn_output = self.layer_scale2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        # Squeeze and excitation\n",
    "        out2 = self.se_block(out2)\n",
    "        \n",
    "        return out2\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class HybridNeuralNetwork(tf.keras.Model):\n",
    "    \"\"\"Hybrid model combining multiple custom components\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, embed_dim=128, num_heads=8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Input processing\n",
    "        self.input_projection = tf.keras.layers.Dense(embed_dim)\n",
    "        self.positional_encoding = PositionalEncoding(max_position=1000)\n",
    "        self.input_dropout = tf.keras.layers.Dropout(0.1)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = [\n",
    "            AdvancedTransformerBlock(embed_dim, num_heads, embed_dim * 4)\n",
    "            for _ in range(3)\n",
    "        ]\n",
    "        \n",
    "        # Residual connections\n",
    "        self.residual_blocks = [\n",
    "            ResidualBlock(embed_dim, use_bottleneck=True)\n",
    "            for _ in range(2)\n",
    "        ]\n",
    "        \n",
    "        # Feature processing\n",
    "        self.feature_squeeze = FeatureSqueezeExcitation(reduction_ratio=16)\n",
    "        self.glu = GatedLinearUnit(embed_dim // 2)\n",
    "        \n",
    "        # Output layers\n",
    "        self.global_pool = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.classifier_dropout = tf.keras.layers.Dropout(0.4)\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes)\n",
    "        self.output_activation = SwishActivation(beta=1.0)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Input processing\n",
    "        x = self.input_projection(inputs)\n",
    "        \n",
    "        # Add sequence dimension if needed\n",
    "        if len(x.shape) == 2:\n",
    "            x = tf.expand_dims(x, axis=1)\n",
    "        \n",
    "        # Positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.input_dropout(x, training=training)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x, training=training)\n",
    "        \n",
    "        # Global pooling to remove sequence dimension\n",
    "        x = self.global_pool(x)\n",
    "        \n",
    "        # Residual blocks (now in 2D)\n",
    "        for residual in self.residual_blocks:\n",
    "            x = residual(x, training=training)\n",
    "        \n",
    "        # Feature processing\n",
    "        x = self.feature_squeeze(x)\n",
    "        x = self.glu(x)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.classifier_dropout(x, training=training)\n",
    "        x = self.classifier(x)\n",
    "        outputs = self.output_activation(x)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_classes': self.num_classes,\n",
    "            'embed_dim': self.embed_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Build and test hybrid model\n",
    "print(\"\\n=== Testing Hybrid Neural Network ===\")\n",
    "\n",
    "hybrid_model = HybridNeuralNetwork(num_classes=3, embed_dim=64, num_heads=4)\n",
    "\n",
    "# Compile with custom loss and metrics\n",
    "hybrid_model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.01),\n",
    "    loss=LabelSmoothingLoss(smoothing=0.1),\n",
    "    metrics=[\n",
    "        F1Score(num_classes=3, average='macro'),\n",
    "        TopKCategoricalAccuracy(k=2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Build model\n",
    "_ = hybrid_model(X_train[:1].astype(np.float32))\n",
    "print(f\"Hybrid Model Parameters: {hybrid_model.count_params():,}\")\n",
    "\n",
    "# Train with custom trainer and advanced techniques\n",
    "advanced_trainer = CustomTrainer(\n",
    "    model=hybrid_model,\n",
    "    loss_fn=LabelSmoothingLoss(smoothing=0.1),\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.01),\n",
    "    metrics=[\n",
    "        F1Score(num_classes=3, average='macro', name='f1_score'),\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Training hybrid model with custom components...\")\n",
    "hybrid_history = advanced_trainer.fit(\n",
    "    train_dataset, val_dataset,\n",
    "    epochs=8, verbose=1\n",
    ")\n",
    "\n",
    "print(f\"Final hybrid model performance:\")\n",
    "print(f\"  Train F1 Score: {hybrid_history['train_f1_score'][-1]:.4f}\")\n",
    "print(f\"  Val F1 Score: {hybrid_history['val_f1_score'][-1]:.4f}\")\n",
    "print(f\"  Train Accuracy: {hybrid_history['train_accuracy'][-1]:.4f}\")\n",
    "print(f\"  Val Accuracy: {hybrid_history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Debugging and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced debugging utilities for custom models\n",
    "class CustomModelDebugger:\n",
    "    \"\"\"Advanced debugging utilities for custom models\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def analyze_gradient_flow(self, x_sample, y_sample, loss_fn):\n",
    "        \"\"\"Analyze gradient flow through custom layers\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(x_sample, training=True)\n",
    "            loss = loss_fn(y_sample, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        \n",
    "        print(\"=== Gradient Flow Analysis ===\")\n",
    "        layer_gradients = {}\n",
    "        \n",
    "        for i, (var, grad) in enumerate(zip(self.model.trainable_variables, gradients)):\n",
    "            if grad is not None:\n",
    "                layer_name = var.name.split('/')[0] if '/' in var.name else f'layer_{i}'\n",
    "                grad_norm = tf.norm(grad).numpy()\n",
    "                grad_mean = tf.reduce_mean(tf.abs(grad)).numpy()\n",
    "                \n",
    "                if layer_name not in layer_gradients:\n",
    "                    layer_gradients[layer_name] = []\n",
    "                \n",
    "                layer_gradients[layer_name].append({\n",
    "                    'variable': var.name,\n",
    "                    'grad_norm': grad_norm,\n",
    "                    'grad_mean': grad_mean,\n",
    "                    'shape': var.shape\n",
    "                })\n",
    "        \n",
    "        for layer_name, grads in layer_gradients.items():\n",
    "            print(f\"\\nLayer: {layer_name}\")\n",
    "            for grad_info in grads:\n",
    "                print(f\"  {grad_info['variable']}: norm={grad_info['grad_norm']:.6f}, \"\n",
    "                      f\"mean_abs={grad_info['grad_mean']:.6f}, shape={grad_info['shape']}\")\n",
    "        \n",
    "        return layer_gradients\n",
    "    \n",
    "    def profile_layer_performance(self, x_sample, num_iterations=100):\n",
    "        \"\"\"Profile performance of each custom layer\"\"\"\n",
    "        import time\n",
    "        \n",
    "        print(\"=== Layer Performance Profiling ===\")\n",
    "        \n",
    "        # Get intermediate outputs\n",
    "        layer_outputs = []\n",
    "        x = x_sample\n",
    "        \n",
    "        times = []\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for _ in range(num_iterations):\n",
    "                if hasattr(layer, 'call'):\n",
    "                    x_temp = layer(x, training=False)\n",
    "                else:\n",
    "                    x_temp = layer(x)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            avg_time = (end_time - start_time) / num_iterations\n",
    "            \n",
    "            times.append(avg_time)\n",
    "            x = layer(x, training=False) if hasattr(layer, 'call') else layer(x)\n",
    "            \n",
    "            print(f\"Layer {i} ({layer.__class__.__name__}): {avg_time*1000:.3f} ms avg\")\n",
    "        \n",
    "        return times\n",
    "\n",
    "# Performance optimization utilities\n",
    "class ModelOptimizer:\n",
    "    \"\"\"Utilities for optimizing custom models\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def mixed_precision_setup():\n",
    "        \"\"\"Setup mixed precision training\"\"\"\n",
    "        policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "        print(\"Mixed precision enabled\")\n",
    "        \n",
    "        return policy\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantize_model(model, representative_dataset):\n",
    "        \"\"\"Quantize model for deployment\"\"\"\n",
    "        \n",
    "        def representative_data_gen():\n",
    "            for batch in representative_dataset.take(100):\n",
    "                yield [batch[0]]\n",
    "        \n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = representative_data_gen\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "        \n",
    "        quantized_model = converter.convert()\n",
    "        return quantized_model\n",
    "\n",
    "# Test debugging and optimization\n",
    "print(\"\\n=== Testing Model Debugging and Optimization ===\")\n",
    "\n",
    "# Debug hybrid model\n",
    "debugger = CustomModelDebugger(hybrid_model)\n",
    "\n",
    "# Analyze gradient flow\n",
    "sample_x = X_train[:8].astype(np.float32)\n",
    "sample_y = y_train[:8]\n",
    "\n",
    "gradient_analysis = debugger.analyze_gradient_flow(\n",
    "    sample_x, sample_y, LabelSmoothingLoss(smoothing=0.1)\n",
    ")\n",
    "\n",
    "# Profile layer performance\n",
    "performance_profile = debugger.profile_layer_performance(sample_x[:1])\n",
    "\n",
    "# Setup mixed precision\n",
    "optimizer = ModelOptimizer()\n",
    "mixed_precision_policy = optimizer.mixed_precision_setup()\n",
    "\n",
    "print(f\"\\nMixed precision policy: {mixed_precision_policy.name}\")\n",
    "print(\"Model debugging and optimization utilities ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This comprehensive notebook mastered advanced tf.keras customization techniques:\n",
    "\n",
    "**Custom Components Built:**\n",
    "- **Basic & Advanced Layers**: Templates, residual blocks, attention mechanisms\n",
    "- **Specialized Layers**: GLU, Swish activation, layer scaling, noise regularization\n",
    "- **Custom Loss Functions**: Focal loss, contrastive loss, label smoothing\n",
    "- **Custom Metrics**: F1-Score, Top-K accuracy with proper state management\n",
    "- **Custom Training Loops**: Multi-objective, gradient accumulation, advanced optimization\n",
    "\n",
    "**Advanced Architectures:**\n",
    "- **Hybrid Models**: Combining transformers, residual networks, and attention\n",
    "- **Multi-Component Integration**: Seamless combination of custom layers\n",
    "- **State Management**: Proper variable tracking and serialization\n",
    "\n",
    "**Training Innovations:**\n",
    "- **Multi-Objective Learning**: Multiple loss functions with weighted optimization\n",
    "- **Gradient Accumulation**: Large effective batch sizes with limited memory\n",
    "- **Advanced Optimizers**: Custom learning rate schedules and weight decay\n",
    "\n",
    "**Debugging & Optimization:**\n",
    "- **Gradient Flow Analysis**: Systematic gradient monitoring across layers\n",
    "- **Performance Profiling**: Layer-by-layer execution timing\n",
    "- **Mixed Precision**: Hardware acceleration support\n",
    "\n",
    "This foundation enables building cutting-edge neural network architectures with complete customization capability!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
