{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 keras sequential functional apis\n",
    "**Location: TensorVerseHub/notebooks/02_neural_networks_with_keras/04_keras_sequential_functional_apis.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras Sequential & Functional APIs + Model Subclassing\n",
    "\n",
    "**File Location:** `notebooks/02_neural_networks_with_keras/04_tf_keras_sequential_functional.ipynb`\n",
    "\n",
    "Master the three ways to build models in tf.keras: Sequential API for simple architectures, Functional API for complex topologies, and Model Subclassing for maximum flexibility. Learn when to use each approach and build sophisticated neural network architectures.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master tf.keras Sequential API for linear model architectures\n",
    "- Build complex models using tf.keras Functional API  \n",
    "- Implement custom models with Model Subclassing\n",
    "- Compare approaches and choose the right one for each scenario\n",
    "- Build multi-input, multi-output, and branched architectures\n",
    "- Handle shared layers and model composition\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Sequential API - Linear Stack Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create sample datasets for demonstrations\n",
    "def create_datasets():\n",
    "    \"\"\"Create various datasets for different model types\"\"\"\n",
    "    \n",
    "    # Classification dataset\n",
    "    X_class, y_class = make_classification(\n",
    "        n_samples=1000, n_features=20, n_classes=3, \n",
    "        n_redundant=0, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Regression dataset  \n",
    "    X_reg, y_reg = make_regression(\n",
    "        n_samples=1000, n_features=15, noise=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Multi-label classification\n",
    "    X_multi, y_multi = make_classification(\n",
    "        n_samples=1000, n_features=25, n_classes=5, \n",
    "        n_redundant=0, random_state=42\n",
    "    )\n",
    "    y_multi = tf.keras.utils.to_categorical(y_multi, 5)\n",
    "    \n",
    "    return (X_class.astype(np.float32), y_class), \\\n",
    "           (X_reg.astype(np.float32), y_reg.astype(np.float32)), \\\n",
    "           (X_multi.astype(np.float32), y_multi.astype(np.float32))\n",
    "\n",
    "# Create datasets\n",
    "(X_class, y_class), (X_reg, y_reg), (X_multi, y_multi) = create_datasets()\n",
    "\n",
    "# Split datasets\n",
    "X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Sequential models for different tasks\n",
    "class SequentialModels:\n",
    "    \"\"\"Collection of Sequential model architectures\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_simple_classifier(input_dim, num_classes):\n",
    "        \"\"\"Simple feed-forward classifier\"\"\"\n",
    "        \n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(input_dim,)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "        ], name='simple_classifier')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_deep_classifier(input_dim, num_classes, depth=5):\n",
    "        \"\"\"Deep classifier with batch normalization\"\"\"\n",
    "        \n",
    "        model = tf.keras.Sequential(name='deep_classifier')\n",
    "        model.add(tf.keras.layers.Input(shape=(input_dim,)))\n",
    "        \n",
    "        # First layer\n",
    "        model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dropout(0.4))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(depth - 2):\n",
    "            units = max(64, 256 // (2 ** i))\n",
    "            model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "            model.add(tf.keras.layers.BatchNormalization())\n",
    "            model.add(tf.keras.layers.Dropout(0.3))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_regressor(input_dim):\n",
    "        \"\"\"Regression model with regularization\"\"\"\n",
    "        \n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(input_dim,)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(64, activation='relu', \n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(32, activation='relu',\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            tf.keras.layers.Dense(1, activation='linear')\n",
    "        ], name='regressor')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_autoencoder(input_dim, encoding_dim=32):\n",
    "        \"\"\"Simple autoencoder using Sequential API\"\"\"\n",
    "        \n",
    "        # Encoder\n",
    "        encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(input_dim,)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(encoding_dim, activation='relu')\n",
    "        ], name='encoder')\n",
    "        \n",
    "        # Decoder\n",
    "        decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(encoding_dim,)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(input_dim, activation='sigmoid')\n",
    "        ], name='decoder')\n",
    "        \n",
    "        # Full autoencoder\n",
    "        autoencoder = tf.keras.Sequential([encoder, decoder], name='autoencoder')\n",
    "        \n",
    "        return autoencoder, encoder, decoder\n",
    "\n",
    "# Build and train Sequential models\n",
    "print(\"=== Sequential API Examples ===\")\n",
    "\n",
    "# Simple classifier\n",
    "simple_model = SequentialModels.create_simple_classifier(X_class.shape[1], 3)\n",
    "simple_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Simple Classifier Architecture:\")\n",
    "simple_model.summary()\n",
    "\n",
    "# Train simple model\n",
    "history_simple = simple_model.fit(\n",
    "    X_class_train, y_class_train,\n",
    "    validation_data=(X_class_test, y_class_test),\n",
    "    epochs=20, batch_size=32, verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Simple Model - Final Val Accuracy: {history_simple.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Deep classifier\n",
    "deep_model = SequentialModels.create_deep_classifier(X_class.shape[1], 3, depth=6)\n",
    "deep_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(f\"\\nDeep Model Parameters: {deep_model.count_params():,}\")\n",
    "\n",
    "# Regressor\n",
    "reg_model = SequentialModels.create_regressor(X_reg.shape[1])\n",
    "reg_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history_reg = reg_model.fit(\n",
    "    X_reg_train, y_reg_train,\n",
    "    validation_data=(X_reg_test, y_reg_test),\n",
    "    epochs=30, batch_size=32, verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Regression Model - Final Val MAE: {history_reg.history['val_mae'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Functional API - Complex Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Functional API models\n",
    "class FunctionalModels:\n",
    "    \"\"\"Collection of Functional API model architectures\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_multi_input_model():\n",
    "        \"\"\"Multi-input model for different data types\"\"\"\n",
    "        \n",
    "        # Define inputs\n",
    "        numerical_input = tf.keras.layers.Input(shape=(10,), name='numerical_features')\n",
    "        categorical_input = tf.keras.layers.Input(shape=(5,), name='categorical_features')\n",
    "        text_input = tf.keras.layers.Input(shape=(100,), name='text_features')\n",
    "        \n",
    "        # Process numerical features\n",
    "        numerical_branch = tf.keras.layers.Dense(64, activation='relu')(numerical_input)\n",
    "        numerical_branch = tf.keras.layers.BatchNormalization()(numerical_branch)\n",
    "        numerical_branch = tf.keras.layers.Dropout(0.3)(numerical_branch)\n",
    "        numerical_branch = tf.keras.layers.Dense(32, activation='relu')(numerical_branch)\n",
    "        \n",
    "        # Process categorical features\n",
    "        categorical_branch = tf.keras.layers.Dense(32, activation='relu')(categorical_input)\n",
    "        categorical_branch = tf.keras.layers.Dropout(0.2)(categorical_branch)\n",
    "        categorical_branch = tf.keras.layers.Dense(16, activation='relu')(categorical_branch)\n",
    "        \n",
    "        # Process text features\n",
    "        text_branch = tf.keras.layers.Dense(128, activation='relu')(text_input)\n",
    "        text_branch = tf.keras.layers.Dropout(0.4)(text_branch)\n",
    "        text_branch = tf.keras.layers.Dense(64, activation='relu')(text_branch)\n",
    "        text_branch = tf.keras.layers.Dense(32, activation='relu')(text_branch)\n",
    "        \n",
    "        # Combine all branches\n",
    "        combined = tf.keras.layers.Concatenate()([\n",
    "            numerical_branch, categorical_branch, text_branch\n",
    "        ])\n",
    "        \n",
    "        # Final layers\n",
    "        x = tf.keras.layers.Dense(128, activation='relu')(combined)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.4)(x)\n",
    "        x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # Multiple outputs\n",
    "        main_output = tf.keras.layers.Dense(3, activation='softmax', name='main_prediction')(x)\n",
    "        auxiliary_output = tf.keras.layers.Dense(1, activation='sigmoid', name='auxiliary_prediction')(x)\n",
    "        \n",
    "        model = tf.keras.Model(\n",
    "            inputs=[numerical_input, categorical_input, text_input],\n",
    "            outputs=[main_output, auxiliary_output],\n",
    "            name='multi_input_output_model'\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_residual_block_model(input_shape, num_classes):\n",
    "        \"\"\"Model with residual blocks using Functional API\"\"\"\n",
    "        \n",
    "        inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "        \n",
    "        # Initial processing\n",
    "        x = tf.keras.layers.Dense(128, activation='relu')(inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Residual blocks\n",
    "        for i in range(3):\n",
    "            # Main path\n",
    "            residual = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "            residual = tf.keras.layers.BatchNormalization()(residual)\n",
    "            residual = tf.keras.layers.Dropout(0.3)(residual)\n",
    "            residual = tf.keras.layers.Dense(128, activation='linear')(residual)\n",
    "            residual = tf.keras.layers.BatchNormalization()(residual)\n",
    "            \n",
    "            # Skip connection\n",
    "            x = tf.keras.layers.Add()([x, residual])\n",
    "            x = tf.keras.layers.Activation('relu')(x)\n",
    "            x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Final layers\n",
    "        x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.4)(x)\n",
    "        outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs, name='residual_model')\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_attention_model(input_shape, num_classes):\n",
    "        \"\"\"Model with self-attention mechanism\"\"\"\n",
    "        \n",
    "        inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "        \n",
    "        # Reshape for attention if needed\n",
    "        x = tf.keras.layers.Dense(64)(inputs)\n",
    "        x = tf.keras.layers.Reshape((input_shape[0] // 4, 64 * 4 // input_shape[0]))(x)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        attention_output = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=8, key_dim=32\n",
    "        )(x, x)\n",
    "        \n",
    "        # Add & Norm\n",
    "        x = tf.keras.layers.Add()([x, attention_output])\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "        \n",
    "        # Feed Forward Network\n",
    "        ffn = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "        ffn = tf.keras.layers.Dropout(0.3)(ffn)\n",
    "        ffn = tf.keras.layers.Dense(x.shape[-1])(ffn)\n",
    "        \n",
    "        # Add & Norm\n",
    "        x = tf.keras.layers.Add()([x, ffn])\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "        \n",
    "        # Global average pooling and classification\n",
    "        x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "        x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.4)(x)\n",
    "        outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs, name='attention_model')\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_branched_model(input_shape, num_classes):\n",
    "        \"\"\"Model with multiple processing branches\"\"\"\n",
    "        \n",
    "        inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "        \n",
    "        # Shared initial layers\n",
    "        shared = tf.keras.layers.Dense(128, activation='relu')(inputs)\n",
    "        shared = tf.keras.layers.BatchNormalization()(shared)\n",
    "        \n",
    "        # Branch 1: Deep narrow path\n",
    "        branch1 = tf.keras.layers.Dense(64, activation='relu')(shared)\n",
    "        for _ in range(4):\n",
    "            branch1 = tf.keras.layers.Dense(64, activation='relu')(branch1)\n",
    "            branch1 = tf.keras.layers.Dropout(0.2)(branch1)\n",
    "        branch1 = tf.keras.layers.Dense(32, activation='relu')(branch1)\n",
    "        \n",
    "        # Branch 2: Wide shallow path  \n",
    "        branch2 = tf.keras.layers.Dense(256, activation='relu')(shared)\n",
    "        branch2 = tf.keras.layers.Dropout(0.4)(branch2)\n",
    "        branch2 = tf.keras.layers.Dense(128, activation='relu')(branch2)\n",
    "        branch2 = tf.keras.layers.Dropout(0.3)(branch2)\n",
    "        \n",
    "        # Branch 3: Regularized path\n",
    "        branch3 = tf.keras.layers.Dense(128, activation='relu', \n",
    "                                       kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01))(shared)\n",
    "        branch3 = tf.keras.layers.BatchNormalization()(branch3)\n",
    "        branch3 = tf.keras.layers.Dense(64, activation='relu',\n",
    "                                       kernel_regularizer=tf.keras.regularizers.l2(0.01))(branch3)\n",
    "        \n",
    "        # Combine branches\n",
    "        combined = tf.keras.layers.Concatenate()([branch1, branch2, branch3])\n",
    "        \n",
    "        # Final processing\n",
    "        x = tf.keras.layers.Dense(128, activation='relu')(combined)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.4)(x)\n",
    "        outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs, name='branched_model')\n",
    "        return model\n",
    "\n",
    "# Build and test Functional API models\n",
    "print(\"\\n=== Functional API Examples ===\")\n",
    "\n",
    "# Multi-input model\n",
    "multi_input_model = FunctionalModels.create_multi_input_model()\n",
    "print(\"Multi-input Model Architecture:\")\n",
    "multi_input_model.summary()\n",
    "\n",
    "# Compile with multiple losses\n",
    "multi_input_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'main_prediction': 'sparse_categorical_crossentropy',\n",
    "          'auxiliary_prediction': 'binary_crossentropy'},\n",
    "    loss_weights={'main_prediction': 1.0, 'auxiliary_prediction': 0.3},\n",
    "    metrics={'main_prediction': 'accuracy', 'auxiliary_prediction': 'accuracy'}\n",
    ")\n",
    "\n",
    "# Residual model\n",
    "residual_model = FunctionalModels.create_residual_block_model(X_class.shape[1:], 3)\n",
    "residual_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(f\"Residual Model Parameters: {residual_model.count_params():,}\")\n",
    "\n",
    "# Train residual model\n",
    "history_residual = residual_model.fit(\n",
    "    X_class_train, y_class_train,\n",
    "    validation_data=(X_class_test, y_class_test),\n",
    "    epochs=25, batch_size=32, verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Residual Model - Final Val Accuracy: {history_residual.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Attention model (if input shape allows)\n",
    "if X_class.shape[1] >= 16:  # Ensure minimum size for attention\n",
    "    attention_model = FunctionalModels.create_attention_model(X_class.shape[1:], 3)\n",
    "    attention_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    print(f\"Attention Model Parameters: {attention_model.count_params():,}\")\n",
    "\n",
    "# Branched model\n",
    "branched_model = FunctionalModels.create_branched_model(X_class.shape[1:], 3)\n",
    "branched_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(f\"Branched Model Parameters: {branched_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Subclassing - Maximum Flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom models using Model Subclassing\n",
    "class CustomResNet(tf.keras.Model):\n",
    "    \"\"\"Custom ResNet implementation using Model Subclassing\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, num_blocks=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.num_blocks = num_blocks\n",
    "        \n",
    "        # Initial layers\n",
    "        self.initial_dense = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.initial_bn = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residual_blocks = []\n",
    "        for i in range(num_blocks):\n",
    "            self.residual_blocks.append({\n",
    "                'dense1': tf.keras.layers.Dense(128, activation='relu'),\n",
    "                'bn1': tf.keras.layers.BatchNormalization(),\n",
    "                'dropout1': tf.keras.layers.Dropout(0.3),\n",
    "                'dense2': tf.keras.layers.Dense(128),\n",
    "                'bn2': tf.keras.layers.BatchNormalization(),\n",
    "                'add': tf.keras.layers.Add(),\n",
    "                'activation': tf.keras.layers.Activation('relu'),\n",
    "                'dropout2': tf.keras.layers.Dropout(0.2)\n",
    "            })\n",
    "        \n",
    "        # Final layers\n",
    "        self.final_dense = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.final_dropout = tf.keras.layers.Dropout(0.4)\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Initial processing\n",
    "        x = self.initial_dense(inputs)\n",
    "        x = self.initial_bn(x, training=training)\n",
    "        \n",
    "        # Residual blocks\n",
    "        for block in self.residual_blocks:\n",
    "            # Main path\n",
    "            residual = block['dense1'](x)\n",
    "            residual = block['bn1'](residual, training=training)\n",
    "            residual = block['dropout1'](residual, training=training)\n",
    "            residual = block['dense2'](residual)\n",
    "            residual = block['bn2'](residual, training=training)\n",
    "            \n",
    "            # Skip connection\n",
    "            x = block['add']([x, residual])\n",
    "            x = block['activation'](x)\n",
    "            x = block['dropout2'](x, training=training)\n",
    "        \n",
    "        # Final processing\n",
    "        x = self.final_dense(x)\n",
    "        x = self.final_dropout(x, training=training)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_classes': self.num_classes,\n",
    "            'num_blocks': self.num_blocks\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class CustomVariationalAutoencoder(tf.keras.Model):\n",
    "    \"\"\"Variational Autoencoder with custom training logic\"\"\"\n",
    "    \n",
    "    def __init__(self, original_dim, latent_dim=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_layers = [\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "        ]\n",
    "        \n",
    "        # Latent space\n",
    "        self.z_mean_layer = tf.keras.layers.Dense(latent_dim)\n",
    "        self.z_log_var_layer = tf.keras.layers.Dense(latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_layers = [\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(original_dim, activation='sigmoid')\n",
    "        ]\n",
    "        \n",
    "        # Metrics\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker, \n",
    "            self.kl_loss_tracker\n",
    "        ]\n",
    "    \n",
    "    def encoder(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, training=training)\n",
    "        \n",
    "        z_mean = self.z_mean_layer(x)\n",
    "        z_log_var = self.z_log_var_layer(x)\n",
    "        return z_mean, z_log_var\n",
    "    \n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    def decoder(self, z, training=None):\n",
    "        x = z\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        z_mean, z_log_var = self.encoder(inputs, training=training)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        reconstruction = self.decoder(z, training=training)\n",
    "        return reconstruction, z_mean, z_log_var\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstruction, z_mean, z_log_var = self(data, training=True)\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.keras.losses.binary_crossentropy(data, reconstruction)\n",
    "            )\n",
    "            \n",
    "            # KL divergence loss\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "            )\n",
    "            \n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        \n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "class CustomGAN(tf.keras.Model):\n",
    "    \"\"\"Simple GAN implementation with custom training loop\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=100, data_dim=20, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.data_dim = data_dim\n",
    "        \n",
    "        # Generator\n",
    "        self.generator = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='relu', input_shape=(latent_dim,)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(data_dim, activation='tanh')\n",
    "        ], name='generator')\n",
    "        \n",
    "        # Discriminator\n",
    "        self.discriminator = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation='relu', input_shape=(data_dim,)),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ], name='discriminator')\n",
    "        \n",
    "        # Optimizers\n",
    "        self.gen_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5)\n",
    "        self.disc_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5)\n",
    "        \n",
    "        # Loss functions\n",
    "        self.loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "        \n",
    "        # Metrics\n",
    "        self.gen_loss_tracker = tf.keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = tf.keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "    \n",
    "    def compile(self, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "    \n",
    "    def train_step(self, real_data):\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "        \n",
    "        # Train discriminator\n",
    "        noise = tf.random.normal([batch_size, self.latent_dim])\n",
    "        \n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            generated_data = self.generator(noise, training=True)\n",
    "            \n",
    "            real_predictions = self.discriminator(real_data, training=True)\n",
    "            fake_predictions = self.discriminator(generated_data, training=True)\n",
    "            \n",
    "            real_loss = self.loss_fn(tf.ones_like(real_predictions), real_predictions)\n",
    "            fake_loss = self.loss_fn(tf.zeros_like(fake_predictions), fake_predictions)\n",
    "            disc_loss = real_loss + fake_loss\n",
    "        \n",
    "        disc_gradients = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "        self.disc_optimizer.apply_gradients(zip(disc_gradients, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # Train generator\n",
    "        noise = tf.random.normal([batch_size, self.latent_dim])\n",
    "        \n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            generated_data = self.generator(noise, training=True)\n",
    "            fake_predictions = self.discriminator(generated_data, training=True)\n",
    "            gen_loss = self.loss_fn(tf.ones_like(fake_predictions), fake_predictions)\n",
    "        \n",
    "        gen_gradients = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        self.gen_optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\n",
    "        \n",
    "        # Update metrics\n",
    "        self.gen_loss_tracker.update_state(gen_loss)\n",
    "        self.disc_loss_tracker.update_state(disc_loss)\n",
    "        \n",
    "        return {\n",
    "            \"generator_loss\": self.gen_loss_tracker.result(),\n",
    "            \"discriminator_loss\": self.disc_loss_tracker.result()\n",
    "        }\n",
    "\n",
    "# Test custom models\n",
    "print(\"\\n=== Model Subclassing Examples ===\")\n",
    "\n",
    "# Custom ResNet\n",
    "custom_resnet = CustomResNet(num_classes=3, num_blocks=4, name='custom_resnet')\n",
    "custom_resnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Build the model by calling it\n",
    "_ = custom_resnet(X_class_train[:1])  # Build model\n",
    "print(f\"Custom ResNet Parameters: {custom_resnet.count_params():,}\")\n",
    "\n",
    "# Train custom ResNet\n",
    "history_custom = custom_resnet.fit(\n",
    "    X_class_train, y_class_train,\n",
    "    validation_data=(X_class_test, y_class_test),\n",
    "    epochs=20, batch_size=32, verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Custom ResNet - Final Val Accuracy: {history_custom.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Variational Autoencoder\n",
    "vae = CustomVariationalAutoencoder(original_dim=X_class.shape[1], latent_dim=16)\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "# Train VAE\n",
    "print(\"\\nTraining Variational Autoencoder...\")\n",
    "vae_history = vae.fit(X_class_train, epochs=15, batch_size=32, verbose=0)\n",
    "\n",
    "print(f\"VAE - Final Loss: {vae_history.history['loss'][-1]:.4f}\")\n",
    "print(f\"VAE - Reconstruction Loss: {vae_history.history['reconstruction_loss'][-1]:.4f}\")\n",
    "print(f\"VAE - KL Loss: {vae_history.history['kl_loss'][-1]:.4f}\")\n",
    "\n",
    "# Simple GAN\n",
    "gan = CustomGAN(latent_dim=50, data_dim=X_class.shape[1])\n",
    "gan.compile()\n",
    "\n",
    "# Normalize data for GAN\n",
    "X_class_normalized = (X_class_train - X_class_train.mean()) / X_class_train.std()\n",
    "X_class_normalized = np.clip(X_class_normalized, -1, 1)\n",
    "\n",
    "print(\"\\nTraining GAN...\")\n",
    "gan_history = gan.fit(X_class_normalized, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "print(f\"GAN - Final Generator Loss: {gan_history.history['generator_loss'][-1]:.4f}\")\n",
    "print(f\"GAN - Final Discriminator Loss: {gan_history.history['discriminator_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Composition and Advanced Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced model composition patterns\n",
    "class ModelComposition:\n",
    "    \"\"\"Advanced patterns for combining and composing models\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_ensemble_model(input_shape, num_classes, num_models=3):\n",
    "        \"\"\"Ensemble of different model architectures\"\"\"\n",
    "        \n",
    "        inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "        \n",
    "        # Model 1: Deep narrow network\n",
    "        model1 = tf.keras.layers.Dense(64, activation='relu')(inputs)\n",
    "        for _ in range(5):\n",
    "            model1 = tf.keras.layers.Dense(64, activation='relu')(model1)\n",
    "            model1 = tf.keras.layers.Dropout(0.2)(model1)\n",
    "        model1_out = tf.keras.layers.Dense(num_classes, activation='softmax')(model1)\n",
    "        \n",
    "        # Model 2: Wide shallow network\n",
    "        model2 = tf.keras.layers.Dense(512, activation='relu')(inputs)\n",
    "        model2 = tf.keras.layers.Dropout(0.4)(model2)\n",
    "        model2 = tf.keras.layers.Dense(256, activation='relu')(model2)\n",
    "        model2 = tf.keras.layers.Dropout(0.3)(model2)\n",
    "        model2_out = tf.keras.layers.Dense(num_classes, activation='softmax')(model2)\n",
    "        \n",
    "        # Model 3: Residual network\n",
    "        model3 = tf.keras.layers.Dense(128, activation='relu')(inputs)\n",
    "        residual = tf.keras.layers.Dense(128, activation='relu')(model3)\n",
    "        residual = tf.keras.layers.Dense(128)(residual)\n",
    "        model3 = tf.keras.layers.Add()([model3, residual])\n",
    "        model3 = tf.keras.layers.Activation('relu')(model3)\n",
    "        model3_out = tf.keras.layers.Dense(num_classes, activation='softmax')(model3)\n",
    "        \n",
    "        # Average ensemble predictions\n",
    "        ensemble_output = tf.keras.layers.Average()([model1_out, model2_out, model3_out])\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=ensemble_output, name='ensemble_model')\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_hierarchical_model(input_shape, num_classes):\n",
    "        \"\"\"Hierarchical model with multiple prediction levels\"\"\"\n",
    "        \n",
    "        inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "        \n",
    "        # Shared feature extraction\n",
    "        x = tf.keras.layers.Dense(256, activation='relu')(inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        shared_features = tf.keras.layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # Level 1: Coarse classification (binary)\n",
    "        level1 = tf.keras.layers.Dense(128, activation='relu')(shared_features)\n",
    "        level1 = tf.keras.layers.Dropout(0.3)(level1)\n",
    "        level1_output = tf.keras.layers.Dense(2, activation='softmax', name='level1_prediction')(level1)\n",
    "        \n",
    "        # Level 2: Fine classification (conditional on level 1)\n",
    "        level2_input = tf.keras.layers.Concatenate()([shared_features, level1])\n",
    "        level2 = tf.keras.layers.Dense(128, activation='relu')(level2_input)\n",
    "        level2 = tf.keras.layers.Dropout(0.3)(level2)\n",
    "        level2_output = tf.keras.layers.Dense(num_classes, activation='softmax', name='level2_prediction')(level2)\n",
    "        \n",
    "        # Final combined prediction\n",
    "        combined_input = tf.keras.layers.Concatenate()([shared_features, level1, level2])\n",
    "        final = tf.keras.layers.Dense(64, activation='relu')(combined_input)\n",
    "        final = tf.keras.layers.Dropout(0.3)(final)\n",
    "        final_output = tf.keras.layers.Dense(num_classes, activation='softmax', name='final_prediction')(final)\n",
    "        \n",
    "        model = tf.keras.Model(\n",
    "            inputs=inputs, \n",
    "            outputs=[level1_output, level2_output, final_output],\n",
    "            name='hierarchical_model'\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_progressive_model(input_shape, num_classes):\n",
    "        \"\"\"Progressive model that can be grown during training\"\"\"\n",
    "        \n",
    "        # Base model\n",
    "        base_inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "        base_x = tf.keras.layers.Dense(64, activation='relu')(base_inputs)\n",
    "        base_x = tf.keras.layers.BatchNormalization()(base_x)\n",
    "        base_outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(base_x)\n",
    "        \n",
    "        base_model = tf.keras.Model(inputs=base_inputs, outputs=base_outputs, name='base_model')\n",
    "        \n",
    "        # Extended model (adds complexity)\n",
    "        extended_inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "        \n",
    "        # Use base model as feature extractor (frozen)\n",
    "        base_features = base_model.layers[-2](base_model.layers[-3](base_model.layers[-4](extended_inputs)))\n",
    "        \n",
    "        # Add more complexity\n",
    "        extended_x = tf.keras.layers.Dense(128, activation='relu')(extended_inputs)\n",
    "        extended_x = tf.keras.layers.BatchNormalization()(extended_x)\n",
    "        extended_x = tf.keras.layers.Dropout(0.3)(extended_x)\n",
    "        \n",
    "        # Combine with base features\n",
    "        combined = tf.keras.layers.Concatenate()([base_features, extended_x])\n",
    "        combined = tf.keras.layers.Dense(64, activation='relu')(combined)\n",
    "        extended_outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(combined)\n",
    "        \n",
    "        extended_model = tf.keras.Model(inputs=extended_inputs, outputs=extended_outputs, name='extended_model')\n",
    "        \n",
    "        return base_model, extended_model\n",
    "\n",
    "# Test advanced composition patterns\n",
    "print(\"\\n=== Advanced Model Composition ===\")\n",
    "\n",
    "# Ensemble model\n",
    "ensemble_model = ModelComposition.create_ensemble_model(X_class.shape[1:], 3)\n",
    "ensemble_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(f\"Ensemble Model Parameters: {ensemble_model.count_params():,}\")\n",
    "\n",
    "# Train ensemble model\n",
    "history_ensemble = ensemble_model.fit(\n",
    "    X_class_train, y_class_train,\n",
    "    validation_data=(X_class_test, y_class_test),\n",
    "    epochs=15, batch_size=32, verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Ensemble Model - Final Val Accuracy: {history_ensemble.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Hierarchical model\n",
    "hierarchical_model = ModelComposition.create_hierarchical_model(X_class.shape[1:], 3)\n",
    "hierarchical_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'level1_prediction': 'sparse_categorical_crossentropy',\n",
    "        'level2_prediction': 'sparse_categorical_crossentropy', \n",
    "        'final_prediction': 'sparse_categorical_crossentropy'\n",
    "    },\n",
    "    loss_weights={'level1_prediction': 0.3, 'level2_prediction': 0.3, 'final_prediction': 0.4},\n",
    "    metrics='accuracy'\n",
    ")\n",
    "\n",
    "print(f\"Hierarchical Model Parameters: {hierarchical_model.count_params():,}\")\n",
    "\n",
    "# Progressive models\n",
    "base_model, extended_model = ModelComposition.create_progressive_model(X_class.shape[1:], 3)\n",
    "\n",
    "# Train base model first\n",
    "base_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "base_history = base_model.fit(\n",
    "    X_class_train, y_class_train,\n",
    "    validation_data=(X_class_test, y_class_test),\n",
    "    epochs=10, batch_size=32, verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Base Model - Final Val Accuracy: {base_history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Then train extended model\n",
    "extended_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "extended_history = extended_model.fit(\n",
    "    X_class_train, y_class_train,\n",
    "    validation_data=(X_class_test, y_class_test),\n",
    "    epochs=10, batch_size=32, verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Extended Model - Final Val Accuracy: {extended_history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model comparison\n",
    "def compare_models():\n",
    "    \"\"\"Compare different model architectures and approaches\"\"\"\n",
    "    \n",
    "    models_to_compare = {\n",
    "        'Sequential_Simple': SequentialModels.create_simple_classifier(X_class.shape[1], 3),\n",
    "        'Sequential_Deep': SequentialModels.create_deep_classifier(X_class.shape[1], 3),\n",
    "        'Functional_Residual': FunctionalModels.create_residual_block_model(X_class.shape[1:], 3),\n",
    "        'Functional_Branched': FunctionalModels.create_branched_model(X_class.shape[1:], 3),\n",
    "        'Custom_ResNet': CustomResNet(num_classes=3, num_blocks=3),\n",
    "        'Ensemble': ModelComposition.create_ensemble_model(X_class.shape[1:], 3)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== Model Comparison Results ===\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Model':<20} {'Parameters':<12} {'Train Acc':<12} {'Val Acc':<12} {'Train Time':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, model in models_to_compare.items():\n",
    "        # Compile model\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Build model to count parameters\n",
    "        if hasattr(model, 'build'):\n",
    "            model.build(input_shape=(None, X_class.shape[1]))\n",
    "        else:\n",
    "            _ = model(X_class_train[:1])\n",
    "        \n",
    "        # Train model and measure time\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_class_train, y_class_train,\n",
    "            validation_data=(X_class_test, y_class_test),\n",
    "            epochs=15, batch_size=32, verbose=0\n",
    "        )\n",
    "        \n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'parameters': model.count_params(),\n",
    "            'train_accuracy': history.history['accuracy'][-1],\n",
    "            'val_accuracy': history.history['val_accuracy'][-1],\n",
    "            'train_time': train_time,\n",
    "            'history': history.history\n",
    "        }\n",
    "        \n",
    "        print(f\"{name:<20} {results[name]['parameters']:<12,} \"\n",
    "              f\"{results[name]['train_accuracy']:<12.4f} \"\n",
    "              f\"{results[name]['val_accuracy']:<12.4f} \"\n",
    "              f\"{train_time:<12.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Visualization of model performance\n",
    "def visualize_model_comparison(results):\n",
    "    \"\"\"Visualize model comparison results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Performance vs Parameters\n",
    "    params = [results[name]['parameters'] for name in results.keys()]\n",
    "    val_accs = [results[name]['val_accuracy'] for name in results.keys()]\n",
    "    names = list(results.keys())\n",
    "    \n",
    "    axes[0, 0].scatter(params, val_accs, s=100, alpha=0.7)\n",
    "    for i, name in enumerate(names):\n",
    "        axes[0, 0].annotate(name, (params[i], val_accs[i]), xytext=(5, 5), textcoords='offset points')\n",
    "    axes[0, 0].set_xlabel('Number of Parameters')\n",
    "    axes[0, 0].set_ylabel('Validation Accuracy')\n",
    "    axes[0, 0].set_title('Performance vs Model Complexity')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training Time vs Performance\n",
    "    train_times = [results[name]['train_time'] for name in results.keys()]\n",
    "    \n",
    "    axes[0, 1].scatter(train_times, val_accs, s=100, alpha=0.7, color='orange')\n",
    "    for i, name in enumerate(names):\n",
    "        axes[0, 1].annotate(name, (train_times[i], val_accs[i]), xytext=(5, 5), textcoords='offset points')\n",
    "    axes[0, 1].set_xlabel('Training Time (seconds)')\n",
    "    axes[0, 1].set_ylabel('Validation Accuracy')\n",
    "    axes[0, 1].set_title('Performance vs Training Time')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Model Performance Bar Chart\n",
    "    axes[1, 0].bar(names, val_accs, alpha=0.7, color='green')\n",
    "    axes[1, 0].set_ylabel('Validation Accuracy')\n",
    "    axes[1, 0].set_title('Model Performance Comparison')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Parameter Count Bar Chart\n",
    "    axes[1, 1].bar(names, params, alpha=0.7, color='red')\n",
    "    axes[1, 1].set_ylabel('Number of Parameters')\n",
    "    axes[1, 1].set_title('Model Complexity Comparison')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance summary\n",
    "    print(\"\\n=== Performance Summary ===\")\n",
    "    best_accuracy = max(results.values(), key=lambda x: x['val_accuracy'])\n",
    "    fastest_training = min(results.values(), key=lambda x: x['train_time'])\n",
    "    most_efficient = min(results.values(), key=lambda x: x['parameters'])\n",
    "    \n",
    "    best_acc_name = [k for k, v in results.items() if v == best_accuracy][0]\n",
    "    fastest_name = [k for k, v in results.items() if v == fastest_training][0] \n",
    "    most_eff_name = [k for k, v in results.items() if v == most_efficient][0]\n",
    "    \n",
    "    print(f\"Best Accuracy: {best_acc_name} ({best_accuracy['val_accuracy']:.4f})\")\n",
    "    print(f\"Fastest Training: {fastest_name} ({fastest_training['train_time']:.2f}s)\")\n",
    "    print(f\"Most Efficient: {most_eff_name} ({most_efficient['parameters']:,} params)\")\n",
    "\n",
    "# Run model comparison\n",
    "comparison_results = compare_models()\n",
    "visualize_model_comparison(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices and Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices and design guidelines\n",
    "class ModelDesignGuidelines:\n",
    "    \"\"\"Guidelines for choosing the right model architecture approach\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_guidelines():\n",
    "        \"\"\"Print comprehensive guidelines for model selection\"\"\"\n",
    "        \n",
    "        print(\"=== tf.keras Model Architecture Guidelines ===\")\n",
    "        print()\n",
    "        \n",
    "        print(\" SEQUENTIAL API - Use when:\")\n",
    "        print(\"   Linear stack of layers (no branching/merging)\")\n",
    "        print(\"   Simple feedforward networks\") \n",
    "        print(\"   Quick prototyping and experimentation\")\n",
    "        print(\"   Standard architectures (MLP, simple CNN/RNN)\")\n",
    "        print(\"   Avoid for: Multi-input/output, complex topologies\")\n",
    "        print()\n",
    "        \n",
    "        print(\" FUNCTIONAL API - Use when:\")\n",
    "        print(\"   Multi-input or multi-output models\")\n",
    "        print(\"   Models with shared layers\")\n",
    "        print(\"   Non-linear topology (skip connections, branches)\")\n",
    "        print(\"   Complex architectures (ResNet, U-Net, etc.)\")\n",
    "        print(\"   Need to access intermediate layer outputs\")\n",
    "        print(\"   Avoid for: Very simple linear models\")\n",
    "        print()\n",
    "        \n",
    "        print(\" MODEL SUBCLASSING - Use when:\")\n",
    "        print(\"   Need custom training loops or loss functions\")\n",
    "        print(\"   Dynamic architectures that change during forward pass\")\n",
    "        print(\"   Complex control flow in the model\")\n",
    "        print(\"   Research-level customizations (GANs, VAEs, etc.)\")\n",
    "        print(\"   Need to track custom metrics or states\")\n",
    "        print(\"   Avoid for: Standard architectures, production models\")\n",
    "        print()\n",
    "        \n",
    "        print(\" PERFORMANCE CONSIDERATIONS:\")\n",
    "        print(\"   Sequential: Fastest to build and train\")\n",
    "        print(\"   Functional: Good performance, flexible\")\n",
    "        print(\"   Subclassing: Most flexible, potentially slower\")\n",
    "        print()\n",
    "        \n",
    "        print(\" MAINTAINABILITY:\")\n",
    "        print(\"   Sequential: Easiest to understand and modify\")\n",
    "        print(\"   Functional: Good balance of flexibility and clarity\")  \n",
    "        print(\"   Subclassing: Requires careful documentation\")\n",
    "        print()\n",
    "        \n",
    "        print(\" DEPLOYMENT CONSIDERATIONS:\")\n",
    "        print(\"   Sequential/Functional: Easy to save/load, convert to other formats\")\n",
    "        print(\"   Subclassing: May require custom code for deployment\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def architecture_recommendations():\n",
    "        \"\"\"Specific architecture recommendations for common use cases\"\"\"\n",
    "        \n",
    "        print(\"\\n=== Architecture Recommendations ===\")\n",
    "        print()\n",
    "        \n",
    "        recommendations = {\n",
    "            \"Binary Classification\": {\n",
    "                \"approach\": \"Sequential\",\n",
    "                \"architecture\": \"Dense(128)  ReLU  Dropout  Dense(64)  ReLU  Dense(1)  Sigmoid\",\n",
    "                \"considerations\": \"Use batch normalization for deeper networks\"\n",
    "            },\n",
    "            \"Multi-class Classification\": {\n",
    "                \"approach\": \"Sequential or Functional\",\n",
    "                \"architecture\": \"Dense(256)  ReLU  BatchNorm  Dropout  Dense(128)  ReLU  Dense(num_classes)  Softmax\",\n",
    "                \"considerations\": \"Consider residual connections for very deep networks\"\n",
    "            },\n",
    "            \"Regression\": {\n",
    "                \"approach\": \"Sequential\",\n",
    "                \"architecture\": \"Dense(128)  ReLU  Dropout  Dense(64)  ReLU  Dense(1)  Linear\",\n",
    "                \"considerations\": \"Add L2 regularization, careful with output activation\"\n",
    "            },\n",
    "            \"Multi-input Model\": {\n",
    "                \"approach\": \"Functional\",\n",
    "                \"architecture\": \"Separate branches for each input type, concatenate, then dense layers\",\n",
    "                \"considerations\": \"Normalize inputs differently, consider input-specific preprocessing\"\n",
    "            },\n",
    "            \"Time Series\": {\n",
    "                \"approach\": \"Sequential or Functional\",\n",
    "                \"architecture\": \"LSTM/GRU layers followed by dense layers\",\n",
    "                \"considerations\": \"Consider attention mechanisms for long sequences\"\n",
    "            },\n",
    "            \"Autoencoder\": {\n",
    "                \"approach\": \"Functional or Subclassing\",\n",
    "                \"architecture\": \"Symmetric encoder-decoder with bottleneck\",\n",
    "                \"considerations\": \"Use skip connections for better reconstruction\"\n",
    "            },\n",
    "            \"GAN\": {\n",
    "                \"approach\": \"Model Subclassing\",\n",
    "                \"architecture\": \"Separate generator and discriminator with custom training loop\",\n",
    "                \"considerations\": \"Careful learning rate scheduling and loss balancing\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for use_case, rec in recommendations.items():\n",
    "            print(f\" {use_case}:\")\n",
    "            print(f\"   Approach: {rec['approach']}\")\n",
    "            print(f\"   Architecture: {rec['architecture']}\")\n",
    "            print(f\"   Considerations: {rec['considerations']}\")\n",
    "            print()\n",
    "\n",
    "# Print guidelines and recommendations\n",
    "ModelDesignGuidelines.print_guidelines()\n",
    "ModelDesignGuidelines.architecture_recommendations()\n",
    "\n",
    "# Final comparison summary\n",
    "print(\"\\n=== Final Summary ===\")\n",
    "print(\"This notebook demonstrated three approaches to building tf.keras models:\")\n",
    "print(\"1. Sequential API:  Fast prototyping, linear architectures\")\n",
    "print(\"2. Functional API:  Flexible, complex topologies\") \n",
    "print(\"3. Model Subclassing:  Maximum control, research applications\")\n",
    "print(\"\\nChoose based on your specific requirements for flexibility vs. simplicity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**File Location:** `notebooks/02_neural_networks_with_keras/04_tf_keras_sequential_functional.ipynb`\n",
    "\n",
    "This comprehensive notebook mastered all three tf.keras model-building approaches:\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. **Sequential API**: Linear layer stacking for simple architectures\n",
    "2. **Functional API**: Complex topologies with branching and merging  \n",
    "3. **Model Subclassing**: Custom training loops and dynamic architectures\n",
    "4. **Advanced Patterns**: Ensembles, hierarchical models, progressive training\n",
    "5. **Model Composition**: Combining different architectural approaches\n",
    "6. **Performance Comparison**: Systematic evaluation of different approaches\n",
    "\n",
    "### Architecture Patterns Mastered:\n",
    "- **Simple Classifiers**: Basic feedforward networks\n",
    "- **Deep Networks**: Batch normalization and residual connections\n",
    "- **Multi-input/Output**: Complex data fusion architectures\n",
    "- **Attention Mechanisms**: Self-attention for improved performance\n",
    "- **Custom Models**: VAEs, GANs with custom training logic\n",
    "- **Ensemble Methods**: Averaging multiple model predictions\n",
    "\n",
    "### Decision Framework:\n",
    "- **Sequential**: Simple, linear architectures; fastest development\n",
    "- **Functional**: Complex topologies; good balance of flexibility/performance  \n",
    "- **Subclassing**: Research applications; maximum customization capability\n",
    "\n",
    "### Best Practices Learned:\n",
    "- Choose architecture complexity based on data complexity\n",
    "- Use appropriate regularization (dropout, batch norm, L2)\n",
    "- Consider residual connections for very deep networks\n",
    "- Implement proper train/validation/test splitting\n",
    "- Profile different approaches for your specific use case\n",
    "\n",
    "### Next Steps:\n",
    "- Apply custom layers and advanced training techniques (Notebook 05)\n",
    "- Implement domain-specific architectures (CNN, RNN, Transformers)\n",
    "- Scale to production deployments with optimized architectures\n",
    "\n",
    "This foundation enables building sophisticated neural networks for any machine learning task!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
