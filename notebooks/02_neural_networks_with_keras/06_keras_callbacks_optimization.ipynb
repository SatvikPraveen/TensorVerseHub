{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 keras callbacks optimization\n",
    "**Location: TensorVerseHub/notebooks/02_neural_networks_with_keras/06_keras_callbacks_optimization.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras Training Optimization & Advanced Techniques\n",
    "\n",
    "**File Location:** `notebooks/02_neural_networks_with_keras/06_training_optimization_keras.ipynb`\n",
    "\n",
    "Master advanced training optimization techniques in tf.keras including callbacks, optimizers, learning rate schedules, regularization methods, and performance monitoring. Build production-ready training pipelines with automated hyperparameter tuning and advanced optimization strategies.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master tf.keras callbacks for training automation and monitoring\n",
    "- Implement advanced optimizers and learning rate scheduling\n",
    "- Apply comprehensive regularization techniques\n",
    "- Build automated hyperparameter tuning systems\n",
    "- Optimize training performance and memory usage\n",
    "- Monitor and visualize training dynamics\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Advanced tf.keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup logging directories\n",
    "log_base_dir = os.path.join(\"logs\", \"training_optimization\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "os.makedirs(log_base_dir, exist_ok=True)\n",
    "\n",
    "# Create comprehensive dataset\n",
    "def create_datasets():\n",
    "    \"\"\"Create various datasets for optimization experiments\"\"\"\n",
    "    \n",
    "    # Large classification dataset\n",
    "    X_large, y_large = make_classification(\n",
    "        n_samples=10000, n_features=50, n_classes=5, \n",
    "        n_redundant=5, n_informative=30, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Digits dataset for computer vision\n",
    "    digits = load_digits()\n",
    "    X_digits, y_digits = digits.data, digits.target\n",
    "    \n",
    "    # Split datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_large.astype(np.float32), y_large, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    X_digits_train, X_digits_test, y_digits_train, y_digits_test = train_test_split(\n",
    "        X_digits.astype(np.float32), y_digits, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test), (X_digits_train, X_digits_test, y_digits_train, y_digits_test)\n",
    "\n",
    "(X_train, X_test, y_train, y_test), (X_digits_train, X_digits_test, y_digits_train, y_digits_test) = create_datasets()\n",
    "\n",
    "print(f\"Main dataset: {X_train.shape}, {X_test.shape}\")\n",
    "print(f\"Digits dataset: {X_digits_train.shape}, {X_digits_test.shape}\")\n",
    "\n",
    "# Advanced callback implementations\n",
    "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Log learning rate changes with visualization\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        super().__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.writer = tf.summary.create_file_writer(os.path.join(log_dir, 'lr_logs'))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_lr = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n",
    "        \n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar('learning_rate', current_lr, step=epoch)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Learning Rate = {current_lr:.6f}\")\n",
    "\n",
    "class GradientNormCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Monitor gradient norms during training\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir, log_freq=10):\n",
    "        super().__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.log_freq = log_freq\n",
    "        self.writer = tf.summary.create_file_writer(os.path.join(log_dir, 'gradient_logs'))\n",
    "        self.batch_count = 0\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.batch_count += 1\n",
    "        \n",
    "        if self.batch_count % self.log_freq == 0:\n",
    "            # Get gradients (simplified approach)\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Use a small batch for gradient calculation\n",
    "                sample_x = self.validation_data[0][:32] if self.validation_data else None\n",
    "                sample_y = self.validation_data[1][:32] if self.validation_data else None\n",
    "                \n",
    "                if sample_x is not None:\n",
    "                    predictions = self.model(sample_x, training=True)\n",
    "                    loss = self.model.compiled_loss(sample_y, predictions)\n",
    "                    \n",
    "                    gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "                    \n",
    "                    # Calculate gradient norms\n",
    "                    grad_norms = []\n",
    "                    for grad in gradients:\n",
    "                        if grad is not None:\n",
    "                            grad_norms.append(tf.norm(grad))\n",
    "                    \n",
    "                    if grad_norms:\n",
    "                        total_grad_norm = tf.norm(grad_norms)\n",
    "                        max_grad_norm = tf.reduce_max(grad_norms)\n",
    "                        \n",
    "                        with self.writer.as_default():\n",
    "                            tf.summary.scalar('total_gradient_norm', total_grad_norm, step=self.batch_count)\n",
    "                            tf.summary.scalar('max_gradient_norm', max_grad_norm, step=self.batch_count)\n",
    "\n",
    "class AdaptiveEarlyStopping(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Adaptive early stopping with patience adjustment\"\"\"\n",
    "    \n",
    "    def __init__(self, monitor='val_loss', patience=10, min_delta=0.001, \n",
    "                 restore_best_weights=True, adaptive_patience=True):\n",
    "        super().__init__()\n",
    "        self.monitor = monitor\n",
    "        self.patience = patience\n",
    "        self.original_patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.adaptive_patience = adaptive_patience\n",
    "        \n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.best = None\n",
    "        self.best_weights = None\n",
    "        self.improvement_history = []\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.best = np.Inf if 'loss' in self.monitor else -np.Inf\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "            \n",
    "        # Check for improvement\n",
    "        if 'loss' in self.monitor:\n",
    "            improved = current < self.best - self.min_delta\n",
    "        else:\n",
    "            improved = current > self.best + self.min_delta\n",
    "            \n",
    "        if improved:\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = self.model.get_weights()\n",
    "            \n",
    "            # Track improvement\n",
    "            self.improvement_history.append(epoch)\n",
    "            \n",
    "            # Adaptive patience adjustment\n",
    "            if self.adaptive_patience and len(self.improvement_history) > 5:\n",
    "                recent_improvements = np.diff(self.improvement_history[-5:])\n",
    "                avg_improvement_gap = np.mean(recent_improvements)\n",
    "                \n",
    "                # Adjust patience based on improvement frequency\n",
    "                if avg_improvement_gap < 5:  # Frequent improvements\n",
    "                    self.patience = min(self.original_patience * 2, 50)\n",
    "                elif avg_improvement_gap > 15:  # Infrequent improvements\n",
    "                    self.patience = max(self.original_patience // 2, 5)\n",
    "                    \n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                if self.restore_best_weights and self.best_weights:\n",
    "                    self.model.set_weights(self.best_weights)\n",
    "                    \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(f\"\\nEarly stopping at epoch {self.stopped_epoch + 1}\")\n",
    "            print(f\"Best {self.monitor}: {self.best:.4f}\")\n",
    "\n",
    "class PerformanceProfiler(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Profile training performance and resource usage\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        super().__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.epoch_times = []\n",
    "        self.batch_times = []\n",
    "        self.memory_usage = []\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        import psutil\n",
    "        self.process = psutil.Process()\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        import time\n",
    "        self.epoch_start_time = time.time()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        import time\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.epoch_times.append(epoch_time)\n",
    "        \n",
    "        # Memory usage\n",
    "        memory_mb = self.process.memory_info().rss / 1024 / 1024\n",
    "        self.memory_usage.append(memory_mb)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}: {epoch_time:.2f}s, Memory: {memory_mb:.1f}MB\")\n",
    "            \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        import time\n",
    "        self.batch_start_time = time.time()\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        import time\n",
    "        batch_time = time.time() - self.batch_start_time\n",
    "        self.batch_times.append(batch_time)\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        # Save performance report\n",
    "        report = {\n",
    "            'avg_epoch_time': np.mean(self.epoch_times),\n",
    "            'total_training_time': np.sum(self.epoch_times),\n",
    "            'avg_batch_time': np.mean(self.batch_times),\n",
    "            'peak_memory_mb': np.max(self.memory_usage),\n",
    "            'avg_memory_mb': np.mean(self.memory_usage)\n",
    "        }\n",
    "        \n",
    "        report_path = os.path.join(self.log_dir, 'performance_report.json')\n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "            \n",
    "        print(f\"\\nPerformance Report:\")\n",
    "        print(f\"  Average epoch time: {report['avg_epoch_time']:.2f}s\")\n",
    "        print(f\"  Total training time: {report['total_training_time']:.2f}s\")\n",
    "        print(f\"  Peak memory usage: {report['peak_memory_mb']:.1f}MB\")\n",
    "\n",
    "# Test advanced callbacks\n",
    "def test_advanced_callbacks():\n",
    "    \"\"\"Test advanced callback implementations\"\"\"\n",
    "    \n",
    "    print(\"=== Testing Advanced Callbacks ===\")\n",
    "    \n",
    "    # Create model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Setup callbacks\n",
    "    callbacks = [\n",
    "        LearningRateLogger(log_base_dir),\n",
    "        AdaptiveEarlyStopping(monitor='val_loss', patience=8, adaptive_patience=True),\n",
    "        PerformanceProfiler(log_base_dir),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            os.path.join(log_base_dir, 'best_model.h5'),\n",
    "            save_best_only=True, monitor='val_accuracy', mode='max'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Run callback tests\n",
    "callback_model, callback_history = test_advanced_callbacks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Optimizers and Learning Rate Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom learning rate schedules\n",
    "class WarmupCosineDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"Warmup followed by cosine decay schedule\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_learning_rate, decay_steps, warmup_steps=1000, alpha=0.0):\n",
    "        super().__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        # Warmup phase\n",
    "        warmup_lr = self.initial_learning_rate * (step / self.warmup_steps)\n",
    "        \n",
    "        # Cosine decay phase\n",
    "        cosine_decay_lr = self.alpha + (self.initial_learning_rate - self.alpha) * (\n",
    "            1 + tf.cos(tf.constant(np.pi) * step / self.decay_steps)\n",
    "        ) / 2\n",
    "        \n",
    "        return tf.cond(\n",
    "            step < self.warmup_steps,\n",
    "            lambda: warmup_lr,\n",
    "            lambda: cosine_decay_lr\n",
    "        )\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'initial_learning_rate': self.initial_learning_rate,\n",
    "            'decay_steps': self.decay_steps,\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "            'alpha': self.alpha\n",
    "        }\n",
    "\n",
    "class CyclicalLearningRate(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"Cyclical learning rate with triangular policy\"\"\"\n",
    "    \n",
    "    def __init__(self, base_lr, max_lr, step_size, mode='triangular'):\n",
    "        super().__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        cycle = tf.floor(1 + step / (2 * self.step_size))\n",
    "        x = tf.abs(step / self.step_size - 2 * cycle + 1)\n",
    "        \n",
    "        if self.mode == 'triangular':\n",
    "            scale_factor = 1.0\n",
    "        elif self.mode == 'triangular2':\n",
    "            scale_factor = 1.0 / (2.0 ** (cycle - 1))\n",
    "        else:  # exp_range\n",
    "            scale_factor = 0.99999 ** step\n",
    "            \n",
    "        lr = self.base_lr + (self.max_lr - self.base_lr) * tf.maximum(0.0, 1 - x) * scale_factor\n",
    "        return lr\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'base_lr': self.base_lr,\n",
    "            'max_lr': self.max_lr,\n",
    "            'step_size': self.step_size,\n",
    "            'mode': self.mode\n",
    "        }\n",
    "\n",
    "class OneCycleLR(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"One cycle learning rate policy\"\"\"\n",
    "    \n",
    "    def __init__(self, max_lr, total_steps, div_factor=25, final_div_factor=10000):\n",
    "        super().__init__()\n",
    "        self.max_lr = max_lr\n",
    "        self.total_steps = total_steps\n",
    "        self.div_factor = div_factor\n",
    "        self.final_div_factor = final_div_factor\n",
    "        \n",
    "        self.initial_lr = max_lr / div_factor\n",
    "        self.final_lr = self.initial_lr / final_div_factor\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        pct = step / self.total_steps\n",
    "        \n",
    "        # First half: increase to max_lr\n",
    "        if pct <= 0.5:\n",
    "            lr = self.initial_lr + (self.max_lr - self.initial_lr) * pct * 2\n",
    "        # Second half: decrease to final_lr\n",
    "        else:\n",
    "            lr = self.max_lr - (self.max_lr - self.final_lr) * (pct - 0.5) * 2\n",
    "            \n",
    "        return tf.maximum(lr, self.final_lr)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'max_lr': self.max_lr,\n",
    "            'total_steps': self.total_steps,\n",
    "            'div_factor': self.div_factor,\n",
    "            'final_div_factor': self.final_div_factor\n",
    "        }\n",
    "\n",
    "# Custom optimizer with gradient clipping\n",
    "class AdamWWithClipping(tf.keras.optimizers.Optimizer):\n",
    "    \"\"\"Adam with weight decay and gradient clipping\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, \n",
    "                 weight_decay=0.01, clipnorm=1.0, epsilon=1e-7, name=\"AdamWClip\", **kwargs):\n",
    "        super().__init__(name, **kwargs)\n",
    "        \n",
    "        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n",
    "        self._set_hyper('beta_1', beta_1)\n",
    "        self._set_hyper('beta_2', beta_2)\n",
    "        self._set_hyper('weight_decay', weight_decay)\n",
    "        self._set_hyper('clipnorm', clipnorm)\n",
    "        self.epsilon = epsilon or tf.keras.backend.epsilon()\n",
    "        \n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'm')  # First moment\n",
    "            self.add_slot(var, 'v')  # Second moment\n",
    "            \n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        \n",
    "        # Get hyperparameters\n",
    "        beta_1_t = self._get_hyper('beta_1', var_dtype)\n",
    "        beta_2_t = self._get_hyper('beta_2', var_dtype)\n",
    "        weight_decay_t = self._get_hyper('weight_decay', var_dtype)\n",
    "        clipnorm_t = self._get_hyper('clipnorm', var_dtype)\n",
    "        \n",
    "        # Clip gradients\n",
    "        if clipnorm_t > 0:\n",
    "            grad = tf.clip_by_norm(grad, clipnorm_t)\n",
    "        \n",
    "        # Get slots\n",
    "        m = self.get_slot(var, 'm')\n",
    "        v = self.get_slot(var, 'v')\n",
    "        \n",
    "        # Update biased first moment estimate\n",
    "        m_t = beta_1_t * m + (1.0 - beta_1_t) * grad\n",
    "        \n",
    "        # Update biased second raw moment estimate\n",
    "        v_t = beta_2_t * v + (1.0 - beta_2_t) * tf.square(grad)\n",
    "        \n",
    "        # Bias correction\n",
    "        m_corr = m_t / (1.0 - tf.pow(beta_1_t, tf.cast(self.iterations + 1, var_dtype)))\n",
    "        v_corr = v_t / (1.0 - tf.pow(beta_2_t, tf.cast(self.iterations + 1, var_dtype)))\n",
    "        \n",
    "        # Update parameters\n",
    "        update = lr_t * m_corr / (tf.sqrt(v_corr) + self.epsilon)\n",
    "        \n",
    "        # Add weight decay\n",
    "        if weight_decay_t > 0:\n",
    "            update += weight_decay_t * lr_t * var\n",
    "            \n",
    "        var_update = var.assign_sub(update)\n",
    "        \n",
    "        # Update slots\n",
    "        m_update = m.assign(m_t)\n",
    "        v_update = v.assign(v_t)\n",
    "        \n",
    "        return tf.group(var_update, m_update, v_update)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'learning_rate': self._serialize_hyperparameter('learning_rate'),\n",
    "            'beta_1': self._serialize_hyperparameter('beta_1'),\n",
    "            'beta_2': self._serialize_hyperparameter('beta_2'),\n",
    "            'weight_decay': self._serialize_hyperparameter('weight_decay'),\n",
    "            'clipnorm': self._serialize_hyperparameter('clipnorm'),\n",
    "            'epsilon': self.epsilon,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Test different optimizers and schedules\n",
    "def compare_optimizers_and_schedules():\n",
    "    \"\"\"Compare different optimizer and learning rate schedule combinations\"\"\"\n",
    "    \n",
    "    print(\"\\n=== Optimizer and Schedule Comparison ===\")\n",
    "    \n",
    "    # Define configurations to test\n",
    "    configs = {\n",
    "        'Adam_Standard': {\n",
    "            'optimizer': tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            'schedule': None\n",
    "        },\n",
    "        'Adam_CosineDecay': {\n",
    "            'optimizer': tf.keras.optimizers.Adam,\n",
    "            'schedule': tf.keras.optimizers.schedules.CosineDecay(0.001, 1000)\n",
    "        },\n",
    "        'Adam_WarmupCosine': {\n",
    "            'optimizer': tf.keras.optimizers.Adam,\n",
    "            'schedule': WarmupCosineDecaySchedule(0.001, 1000, 100)\n",
    "        },\n",
    "        'AdamW_OneCycle': {\n",
    "            'optimizer': tf.keras.optimizers.AdamW,\n",
    "            'schedule': OneCycleLR(0.003, 1000)\n",
    "        },\n",
    "        'Custom_AdamW_Clip': {\n",
    "            'optimizer': AdamWWithClipping,\n",
    "            'schedule': CyclicalLearningRate(0.0001, 0.001, 200)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, config in configs.items():\n",
    "        print(f\"\\nTesting {name}...\")\n",
    "        \n",
    "        # Create model\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(5, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        # Setup optimizer\n",
    "        if config['schedule']:\n",
    "            optimizer = config['optimizer'](learning_rate=config['schedule'])\n",
    "        else:\n",
    "            optimizer = config['optimizer']\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train with early stopping\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=30,\n",
    "            batch_size=64,\n",
    "            verbose=0,\n",
    "            callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'final_train_acc': history.history['accuracy'][-1],\n",
    "            'final_val_acc': history.history['val_accuracy'][-1],\n",
    "            'best_val_acc': max(history.history['val_accuracy']),\n",
    "            'epochs_trained': len(history.history['accuracy']),\n",
    "            'final_loss': history.history['val_loss'][-1]\n",
    "        }\n",
    "        \n",
    "        print(f\"  Best Val Acc: {results[name]['best_val_acc']:.4f}\")\n",
    "        print(f\"  Epochs: {results[name]['epochs_trained']}\")\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\n=== Optimization Results Summary ===\")\n",
    "    print(f\"{'Configuration':<20} {'Best Val Acc':<12} {'Final Val Acc':<12} {'Epochs':<8} {'Final Loss':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        print(f\"{name:<20} {result['best_val_acc']:<12.4f} \"\n",
    "              f\"{result['final_val_acc']:<12.4f} {result['epochs_trained']:<8} \"\n",
    "              f\"{result['final_loss']:<12.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run optimizer comparison\n",
    "optimizer_results = compare_optimizers_and_schedules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comprehensive Regularization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced regularization methods\n",
    "class MixupCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Mixup data augmentation callback\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # Get batch data (this is simplified - in practice you'd need to modify the data pipeline)\n",
    "        pass  # Implementation would require custom training loop\n",
    "        \n",
    "class CutMixCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"CutMix data augmentation callback\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "class DropBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"DropBlock regularization layer\"\"\"\n",
    "    \n",
    "    def __init__(self, drop_rate=0.1, block_size=7, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        if not training:\n",
    "            return inputs\n",
    "            \n",
    "        # For 2D inputs, treat as flattened feature maps\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        feature_size = tf.shape(inputs)[1]\n",
    "        \n",
    "        # Calculate gamma (DropBlock paper)\n",
    "        gamma = self.drop_rate / (self.block_size ** 2)\n",
    "        \n",
    "        # Sample mask\n",
    "        mask_shape = (batch_size, feature_size)\n",
    "        mask = tf.random.uniform(mask_shape) < gamma\n",
    "        \n",
    "        # Apply block structure (simplified for 1D case)\n",
    "        block_mask = tf.cast(mask, tf.float32)\n",
    "        \n",
    "        # Scale to maintain expected value\n",
    "        scale = tf.cast(tf.size(block_mask), tf.float32) / tf.reduce_sum(block_mask)\n",
    "        \n",
    "        return inputs * block_mask * scale\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'drop_rate': self.drop_rate,\n",
    "            'block_size': self.block_size\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class SpectralNormalization(tf.keras.layers.Wrapper):\n",
    "    \"\"\"Spectral normalization wrapper for any layer\"\"\"\n",
    "    \n",
    "    def __init__(self, layer, power_iterations=1, **kwargs):\n",
    "        super().__init__(layer, **kwargs)\n",
    "        self.power_iterations = power_iterations\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        \n",
    "        # Get the layer's kernel\n",
    "        if hasattr(self.layer, 'kernel'):\n",
    "            kernel = self.layer.kernel\n",
    "            kernel_shape = kernel.shape\n",
    "            \n",
    "            # Create u and v vectors for power iteration\n",
    "            self.u = self.add_weight(\n",
    "                shape=(1, kernel_shape[-1]),\n",
    "                initializer='random_normal',\n",
    "                trainable=False,\n",
    "                name='u_vector'\n",
    "            )\n",
    "            \n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        if hasattr(self.layer, 'kernel') and training:\n",
    "            # Power iteration to estimate spectral norm\n",
    "            kernel = self.layer.kernel\n",
    "            w_reshaped = tf.reshape(kernel, [-1, kernel.shape[-1]])\n",
    "            \n",
    "            u = self.u\n",
    "            for _ in range(self.power_iterations):\n",
    "                v = tf.nn.l2_normalize(tf.matmul(u, w_reshaped, transpose_b=True))\n",
    "                u = tf.nn.l2_normalize(tf.matmul(v, w_reshaped))\n",
    "            \n",
    "            # Calculate spectral norm\n",
    "            sigma = tf.matmul(tf.matmul(v, w_reshaped), u, transpose_b=True)\n",
    "            \n",
    "            # Normalize kernel\n",
    "            kernel_normalized = kernel / sigma\n",
    "            \n",
    "            # Temporarily replace kernel\n",
    "            original_kernel = self.layer.kernel\n",
    "            self.layer.kernel = kernel_normalized\n",
    "            \n",
    "            # Update u vector\n",
    "            self.u.assign(u)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = self.layer(inputs, **kwargs)\n",
    "            \n",
    "            # Restore original kernel\n",
    "            self.layer.kernel = original_kernel\n",
    "            \n",
    "            return output\n",
    "        else:\n",
    "            return self.layer(inputs, **kwargs)\n",
    "\n",
    "class StochasticDepth(tf.keras.layers.Layer):\n",
    "    \"\"\"Stochastic depth regularization\"\"\"\n",
    "    \n",
    "    def __init__(self, drop_prob=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        if not training:\n",
    "            return inputs\n",
    "            \n",
    "        # Randomly drop the entire layer\n",
    "        random_tensor = tf.random.uniform([tf.shape(inputs)[0], 1, 1])\n",
    "        keep_prob = 1.0 - self.drop_prob\n",
    "        \n",
    "        # Create binary mask\n",
    "        binary_tensor = tf.cast(random_tensor < keep_prob, tf.float32)\n",
    "        \n",
    "        # Scale output to maintain expected value\n",
    "        output = inputs * binary_tensor / keep_prob\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'drop_prob': self.drop_prob})\n",
    "        return config\n",
    "\n",
    "# Advanced regularized model\n",
    "def create_heavily_regularized_model(input_shape, num_classes):\n",
    "    \"\"\"Create model with comprehensive regularization\"\"\"\n",
    "    \n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Input normalization and noise\n",
    "    x = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    x = tf.keras.layers.GaussianNoise(0.1)(x)\n",
    "    \n",
    "    # First block with multiple regularization techniques\n",
    "    x = SpectralNormalization(tf.keras.layers.Dense(128, activation='relu'))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = DropBlock(drop_rate=0.1)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Residual connection with stochastic depth\n",
    "    residual = x\n",
    "    x = SpectralNormalization(tf.keras.layers.Dense(128, activation='relu', \n",
    "                                                   kernel_regularizer=tf.keras.regularizers.l1_l2(0.01, 0.01)))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = StochasticDepth(drop_prob=0.1)(x)\n",
    "    x = tf.keras.layers.Add()([x, residual])\n",
    "    \n",
    "    # Second block\n",
    "    x = SpectralNormalization(tf.keras.layers.Dense(64, activation='relu'))(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = DropBlock(drop_rate=0.15)(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs, name='heavily_regularized_model')\n",
    "    return model\n",
    "\n",
    "# Test regularization techniques\n",
    "print(\"\\n=== Testing Advanced Regularization ===\")\n",
    "\n",
    "# Create regularized model\n",
    "reg_model = create_heavily_regularized_model(X_train.shape[1:], 5)\n",
    "reg_model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.01),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Regularized Model Parameters: {reg_model.count_params():,}\")\n",
    "\n",
    "# Train with regularization\n",
    "reg_history = reg_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(patience=4, factor=0.5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Regularized Model - Best Val Acc: {max(reg_history.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# Compare with standard model\n",
    "standard_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "standard_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "standard_history = standard_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    verbose=0,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=8)]\n",
    ")\n",
    "\n",
    "print(f\"Standard Model - Best Val Acc: {max(standard_history.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# Compare overfitting\n",
    "reg_overfit = max(reg_history.history['accuracy']) - max(reg_history.history['val_accuracy'])\n",
    "std_overfit = max(standard_history.history['accuracy']) - max(standard_history.history['val_accuracy'])\n",
    "\n",
    "print(f\"Regularized Model Overfitting Gap: {reg_overfit:.4f}\")\n",
    "print(f\"Standard Model Overfitting Gap: {std_overfit:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Automated Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Tuner integration\n",
    "try:\n",
    "    import keras_tuner as kt\n",
    "    KERAS_TUNER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    KERAS_TUNER_AVAILABLE = False\n",
    "    print(\"Keras Tuner not available. Implementing basic grid search.\")\n",
    "\n",
    "class BasicHyperparameterTuner:\n",
    "    \"\"\"Basic hyperparameter tuning without external dependencies\"\"\"\n",
    "    \n",
    "    def __init__(self, build_model_fn, objective='val_accuracy'):\n",
    "        self.build_model_fn = build_model_fn\n",
    "        self.objective = objective\n",
    "        self.results = []\n",
    "        \n",
    "    def search(self, x_train, y_train, x_val, y_val, param_grid, max_trials=10):\n",
    "        \"\"\"Search hyperparameter space\"\"\"\n",
    "        \n",
    "        print(\"=== Starting Hyperparameter Search ===\")\n",
    "        \n",
    "        # Generate parameter combinations\n",
    "        import itertools\n",
    "        param_names = list(param_grid.keys())\n",
    "        param_values = list(param_grid.values())\n",
    "        combinations = list(itertools.product(*param_values))\n",
    "        \n",
    "        # Limit trials\n",
    "        if len(combinations) > max_trials:\n",
    "            combinations = np.random.choice(len(combinations), max_trials, replace=False)\n",
    "            combinations = [combinations[i] for i in range(max_trials)]\n",
    "        \n",
    "        for i, params in enumerate(combinations[:max_trials]):\n",
    "            print(f\"\\nTrial {i+1}/{min(max_trials, len(combinations))}\")\n",
    "            \n",
    "            # Create parameter dict\n",
    "            param_dict = dict(zip(param_names, params))\n",
    "            print(f\"Parameters: {param_dict}\")\n",
    "            \n",
    "            # Build and train model\n",
    "            model = self.build_model_fn(**param_dict)\n",
    "            \n",
    "            history = model.fit(\n",
    "                x_train, y_train,\n",
    "                validation_data=(x_val, y_val),\n",
    "                epochs=20,\n",
    "                batch_size=param_dict.get('batch_size', 32),\n",
    "                verbose=0,\n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
    "            )\n",
    "            \n",
    "            # Record results\n",
    "            best_score = max(history.history[self.objective])\n",
    "            self.results.append({\n",
    "                'params': param_dict,\n",
    "                'score': best_score,\n",
    "                'history': history.history\n",
    "            })\n",
    "            \n",
    "            print(f\"Score: {best_score:.4f}\")\n",
    "        \n",
    "        # Sort by score\n",
    "        self.results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        print(f\"\\n=== Search Complete ===\")\n",
    "        print(f\"Best score: {self.results[0]['score']:.4f}\")\n",
    "        print(f\"Best parameters: {self.results[0]['params']}\")\n",
    "        \n",
    "        return self.results[0]\n",
    "\n",
    "# Hyperparameter search space\n",
    "def build_tunable_model(learning_rate=0.001, units_1=64, units_2=32, \n",
    "                       dropout_rate=0.3, batch_norm=True, optimizer_type='adam'):\n",
    "    \"\"\"Build model with tunable hyperparameters\"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(units_1, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    \n",
    "    if batch_norm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(units_2, activation='relu'))\n",
    "    \n",
    "    if batch_norm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        \n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "    \n",
    "    # Choose optimizer\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_type == 'adamw':\n",
    "        optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=0.01)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "def run_hyperparameter_search():\n",
    "    \"\"\"Run comprehensive hyperparameter search\"\"\"\n",
    "    \n",
    "    print(\"\\n=== Hyperparameter Tuning ===\")\n",
    "    \n",
    "    # Define search space\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.001, 0.003, 0.01],\n",
    "        'units_1': [64, 128, 256],\n",
    "        'units_2': [32, 64, 128],\n",
    "        'dropout_rate': [0.2, 0.3, 0.5],\n",
    "        'batch_norm': [True, False],\n",
    "        'optimizer_type': ['adam', 'adamw'],\n",
    "        'batch_size': [32, 64, 128]\n",
    "    }\n",
    "    \n",
    "    # Create tuner\n",
    "    tuner = BasicHyperparameterTuner(build_tunable_model)\n",
    "    \n",
    "    # Run search\n",
    "    best_trial = tuner.search(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        param_grid, max_trials=15\n",
    "    )\n",
    "    \n",
    "    # Train best model with more epochs\n",
    "    print(\"\\n=== Training Best Model ===\")\n",
    "    best_model = build_tunable_model(**best_trial['params'])\n",
    "    \n",
    "    final_history = best_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=50,\n",
    "        batch_size=best_trial['params'].get('batch_size', 64),\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    final_score = max(final_history.history['val_accuracy'])\n",
    "    print(f\"Final optimized model score: {final_score:.4f}\")\n",
    "    \n",
    "    return best_model, best_trial, final_history\n",
    "\n",
    "# Run hyperparameter search\n",
    "if len(X_train) > 1000:  # Only run if we have enough data\n",
    "    best_model, best_params, final_history = run_hyperparameter_search()\n",
    "else:\n",
    "    print(\"Skipping hyperparameter search for small dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Monitoring and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive training visualization\n",
    "class TrainingVisualizer:\n",
    "    \"\"\"Comprehensive training visualization and analysis\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.histories = {}\n",
    "        \n",
    "    def add_history(self, name, history):\n",
    "        \"\"\"Add training history for comparison\"\"\"\n",
    "        self.histories[name] = history.history if hasattr(history, 'history') else history\n",
    "        \n",
    "    def plot_training_curves(self, figsize=(15, 10)):\n",
    "        \"\"\"Plot comprehensive training curves\"\"\"\n",
    "        \n",
    "        n_models = len(self.histories)\n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        # Colors for different models\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, n_models))\n",
    "        \n",
    "        # Loss curves\n",
    "        for i, (name, history) in enumerate(self.histories.items()):\n",
    "            axes[0, 0].plot(history['loss'], color=colors[i], label=f'{name} (train)', alpha=0.8)\n",
    "            if 'val_loss' in history:\n",
    "                axes[0, 0].plot(history['val_loss'], color=colors[i], label=f'{name} (val)', \n",
    "                               linestyle='--', alpha=0.8)\n",
    "        \n",
    "        axes[0, 0].set_title('Training Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy curves\n",
    "        for i, (name, history) in enumerate(self.histories.items()):\n",
    "            if 'accuracy' in history:\n",
    "                axes[0, 1].plot(history['accuracy'], color=colors[i], label=f'{name} (train)', alpha=0.8)\n",
    "            if 'val_accuracy' in history:\n",
    "                axes[0, 1].plot(history['val_accuracy'], color=colors[i], label=f'{name} (val)', \n",
    "                               linestyle='--', alpha=0.8)\n",
    "        \n",
    "        axes[0, 1].set_title('Training Accuracy')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning rate curves (if available)\n",
    "        lr_data_available = False\n",
    "        for i, (name, history) in enumerate(self.histories.items()):\n",
    "            if 'lr' in history:\n",
    "                axes[1, 0].plot(history['lr'], color=colors[i], label=f'{name}', alpha=0.8)\n",
    "                lr_data_available = True\n",
    "        \n",
    "        if lr_data_available:\n",
    "            axes[1, 0].set_title('Learning Rate')\n",
    "            axes[1, 0].set_xlabel('Epoch')\n",
    "            axes[1, 0].set_ylabel('Learning Rate')\n",
    "            axes[1, 0].set_yscale('log')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'No Learning Rate Data', ha='center', va='center', \n",
    "                           transform=axes[1, 0].transAxes, fontsize=14)\n",
    "            axes[1, 0].set_title('Learning Rate (No Data)')\n",
    "        \n",
    "        # Overfitting analysis\n",
    "        for i, (name, history) in enumerate(self.histories.items()):\n",
    "            if 'accuracy' in history and 'val_accuracy' in history:\n",
    "                train_acc = history['accuracy']\n",
    "                val_acc = history['val_accuracy']\n",
    "                gap = np.array(train_acc) - np.array(val_acc)\n",
    "                axes[1, 1].plot(gap, color=colors[i], label=f'{name}', alpha=0.8)\n",
    "        \n",
    "        axes[1, 1].set_title('Overfitting Gap (Train - Val Accuracy)')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Accuracy Gap')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def analyze_training_stability(self):\n",
    "        \"\"\"Analyze training stability metrics\"\"\"\n",
    "        \n",
    "        print(\"=== Training Stability Analysis ===\")\n",
    "        \n",
    "        for name, history in self.histories.items():\n",
    "            print(f\"\\n{name}:\")\n",
    "            \n",
    "            if 'val_loss' in history:\n",
    "                val_loss = np.array(history['val_loss'])\n",
    "                \n",
    "                # Loss volatility\n",
    "                loss_volatility = np.std(np.diff(val_loss))\n",
    "                print(f\"  Validation Loss Volatility: {loss_volatility:.4f}\")\n",
    "                \n",
    "                # Convergence analysis\n",
    "                if len(val_loss) >= 10:\n",
    "                    early_loss = np.mean(val_loss[:5])\n",
    "                    late_loss = np.mean(val_loss[-5:])\n",
    "                    improvement = early_loss - late_loss\n",
    "                    print(f\"  Loss Improvement (Early vs Late): {improvement:.4f}\")\n",
    "                \n",
    "                # Best epoch\n",
    "                best_epoch = np.argmin(val_loss)\n",
    "                best_loss = val_loss[best_epoch]\n",
    "                print(f\"  Best Epoch: {best_epoch} (Loss: {best_loss:.4f})\")\n",
    "                \n",
    "            if 'val_accuracy' in history:\n",
    "                val_acc = np.array(history['val_accuracy'])\n",
    "                best_acc = np.max(val_acc)\n",
    "                best_acc_epoch = np.argmax(val_acc)\n",
    "                print(f\"  Best Accuracy: {best_acc:.4f} at Epoch {best_acc_epoch}\")\n",
    "                \n",
    "                # Accuracy plateau detection\n",
    "                if len(val_acc) >= 10:\n",
    "                    recent_std = np.std(val_acc[-10:])\n",
    "                    if recent_std < 0.001:\n",
    "                        print(f\"   Accuracy plateau detected (std: {recent_std:.5f})\")\n",
    "\n",
    "# Performance benchmarking\n",
    "class ModelBenchmark:\n",
    "    \"\"\"Comprehensive model benchmarking\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def benchmark_model(self, name, model, test_data, num_runs=5):\n",
    "        \"\"\"Benchmark model inference performance\"\"\"\n",
    "        \n",
    "        import time\n",
    "        \n",
    "        print(f\"\\nBenchmarking {name}...\")\n",
    "        \n",
    "        x_test, y_test = test_data\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(3):\n",
    "            _ = model(x_test[:10], training=False)\n",
    "        \n",
    "        # Timing runs\n",
    "        times = []\n",
    "        accuracies = []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            predictions = model(x_test, training=False)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            inference_time = end_time - start_time\n",
    "            times.append(inference_time)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            if len(predictions.shape) > 1 and predictions.shape[1] > 1:\n",
    "                pred_classes = tf.argmax(predictions, axis=1)\n",
    "                accuracy = tf.reduce_mean(tf.cast(tf.equal(pred_classes, y_test), tf.float32))\n",
    "                accuracies.append(accuracy.numpy())\n",
    "        \n",
    "        # Store results\n",
    "        self.results[name] = {\n",
    "            'avg_inference_time': np.mean(times),\n",
    "            'std_inference_time': np.std(times),\n",
    "            'throughput': len(x_test) / np.mean(times),\n",
    "            'accuracy': np.mean(accuracies) if accuracies else None,\n",
    "            'model_size': model.count_params(),\n",
    "            'memory_usage': self.estimate_memory_usage(model)\n",
    "        }\n",
    "        \n",
    "        print(f\"  Avg Inference Time: {self.results[name]['avg_inference_time']:.4f}s\")\n",
    "        print(f\"  Throughput: {self.results[name]['throughput']:.1f} samples/sec\")\n",
    "        print(f\"  Accuracy: {self.results[name]['accuracy']:.4f}\" if accuracies else \"  No accuracy computed\")\n",
    "        \n",
    "    def estimate_memory_usage(self, model):\n",
    "        \"\"\"Estimate model memory usage\"\"\"\n",
    "        \n",
    "        total_params = model.count_params()\n",
    "        # Assume float32 (4 bytes per parameter)\n",
    "        memory_mb = (total_params * 4) / (1024 ** 2)\n",
    "        return memory_mb\n",
    "        \n",
    "    def print_benchmark_summary(self):\n",
    "        \"\"\"Print comprehensive benchmark summary\"\"\"\n",
    "        \n",
    "        print(\"\\n=== Model Benchmark Summary ===\")\n",
    "        print(f\"{'Model':<20} {'Accuracy':<10} {'Params':<10} {'Memory(MB)':<12} {'Throughput':<15} {'Latency(ms)':<12}\")\n",
    "        print(\"-\" * 95)\n",
    "        \n",
    "        for name, result in self.results.items():\n",
    "            acc_str = f\"{result['accuracy']:.4f}\" if result['accuracy'] else \"N/A\"\n",
    "            latency_ms = result['avg_inference_time'] * 1000\n",
    "            \n",
    "            print(f\"{name:<20} {acc_str:<10} {result['model_size']:<10,} \"\n",
    "                  f\"{result['memory_usage']:<12.1f} {result['throughput']:<15.1f} \"\n",
    "                  f\"{latency_ms:<12.2f}\")\n",
    "\n",
    "# Visualize training results\n",
    "visualizer = TrainingVisualizer()\n",
    "\n",
    "# Add training histories from previous experiments\n",
    "if 'callback_history' in locals():\n",
    "    visualizer.add_history('With_Callbacks', callback_history)\n",
    "\n",
    "if 'reg_history' in locals():\n",
    "    visualizer.add_history('Regularized', reg_history)\n",
    "    \n",
    "if 'standard_history' in locals():\n",
    "    visualizer.add_history('Standard', standard_history)\n",
    "\n",
    "# Plot comparisons\n",
    "if len(visualizer.histories) > 0:\n",
    "    visualizer.plot_training_curves()\n",
    "    visualizer.analyze_training_stability()\n",
    "\n",
    "# Benchmark models\n",
    "benchmark = ModelBenchmark()\n",
    "\n",
    "if 'callback_model' in locals():\n",
    "    benchmark.benchmark_model('Callback_Model', callback_model, (X_test, y_test))\n",
    "    \n",
    "if 'reg_model' in locals():\n",
    "    benchmark.benchmark_model('Regularized_Model', reg_model, (X_test, y_test))\n",
    "    \n",
    "if 'standard_model' in locals():\n",
    "    benchmark.benchmark_model('Standard_Model', standard_model, (X_test, y_test))\n",
    "\n",
    "if len(benchmark.results) > 0:\n",
    "    benchmark.print_benchmark_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete production training pipeline\n",
    "class ProductionTrainingPipeline:\n",
    "    \"\"\"Complete production-ready training pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.callbacks = []\n",
    "        \n",
    "    def setup_callbacks(self):\n",
    "        \"\"\"Setup comprehensive callback system\"\"\"\n",
    "        \n",
    "        callbacks = []\n",
    "        \n",
    "        # Model checkpointing\n",
    "        callbacks.append(tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(self.config['output_dir'], 'best_model.h5'),\n",
    "            monitor=self.config['monitor_metric'],\n",
    "            save_best_only=True,\n",
    "            mode='max' if 'acc' in self.config['monitor_metric'] else 'min'\n",
    "        ))\n",
    "        \n",
    "        # Early stopping with adaptive patience\n",
    "        callbacks.append(AdaptiveEarlyStopping(\n",
    "            monitor=self.config['monitor_metric'],\n",
    "            patience=self.config['patience'],\n",
    "            restore_best_weights=True\n",
    "        ))\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        if self.config['lr_schedule'] == 'reduce_on_plateau':\n",
    "            callbacks.append(tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor=self.config['monitor_metric'],\n",
    "                factor=0.5,\n",
    "                patience=self.config['lr_patience'],\n",
    "                min_lr=1e-7\n",
    "            ))\n",
    "        \n",
    "        # Performance monitoring\n",
    "        callbacks.append(PerformanceProfiler(self.config['output_dir']))\n",
    "        \n",
    "        # TensorBoard logging\n",
    "        callbacks.append(tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=os.path.join(self.config['output_dir'], 'tensorboard'),\n",
    "            histogram_freq=5,\n",
    "            write_graph=True,\n",
    "            update_freq='epoch'\n",
    "        ))\n",
    "        \n",
    "        # Custom callbacks\n",
    "        if self.config.get('gradient_monitoring', False):\n",
    "            callbacks.append(GradientNormCallback(self.config['output_dir']))\n",
    "            \n",
    "        callbacks.append(LearningRateLogger(self.config['output_dir']))\n",
    "        \n",
    "        self.callbacks = callbacks\n",
    "        return callbacks\n",
    "    \n",
    "    def create_model(self):\n",
    "        \"\"\"Create model based on configuration\"\"\"\n",
    "        \n",
    "        # Model architecture\n",
    "        if self.config['model_type'] == 'simple':\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(\n",
    "                    self.config['units'][0], \n",
    "                    activation='relu', \n",
    "                    input_shape=(self.config['input_dim'],)\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(self.config['dropout_rate']),\n",
    "                tf.keras.layers.Dense(self.config['units'][1], activation='relu'),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Dropout(self.config['dropout_rate']),\n",
    "                tf.keras.layers.Dense(self.config['num_classes'], activation='softmax')\n",
    "            ])\n",
    "            \n",
    "        elif self.config['model_type'] == 'regularized':\n",
    "            model = create_heavily_regularized_model(\n",
    "                (self.config['input_dim'],), \n",
    "                self.config['num_classes']\n",
    "            )\n",
    "            \n",
    "        # Optimizer setup\n",
    "        if self.config['optimizer'] == 'adamw':\n",
    "            optimizer = tf.keras.optimizers.AdamW(\n",
    "                learning_rate=self.config['learning_rate'],\n",
    "                weight_decay=self.config.get('weight_decay', 0.01)\n",
    "            )\n",
    "        else:\n",
    "            optimizer = tf.keras.optimizers.Adam(\n",
    "                learning_rate=self.config['learning_rate']\n",
    "            )\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=self.config['loss_function'],\n",
    "            metrics=self.config['metrics']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def train(self, train_data, val_data):\n",
    "        \"\"\"Execute training pipeline\"\"\"\n",
    "        \n",
    "        print(\"=== Starting Production Training Pipeline ===\")\n",
    "        print(f\"Configuration: {self.config}\")\n",
    "        \n",
    "        # Setup\n",
    "        self.setup_callbacks()\n",
    "        model = self.create_model()\n",
    "        \n",
    "        print(f\"Model created with {model.count_params():,} parameters\")\n",
    "        \n",
    "        # Training\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.history = model.fit(\n",
    "            train_data[0], train_data[1],\n",
    "            validation_data=val_data,\n",
    "            epochs=self.config['epochs'],\n",
    "            batch_size=self.config['batch_size'],\n",
    "            callbacks=self.callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Results summary\n",
    "        best_metric = max(self.history.history[self.config['monitor_metric']])\n",
    "        \n",
    "        print(f\"\\n=== Training Complete ===\")\n",
    "        print(f\"Training time: {training_time:.2f} seconds\")\n",
    "        print(f\"Best {self.config['monitor_metric']}: {best_metric:.4f}\")\n",
    "        print(f\"Total epochs: {len(self.history.history['loss'])}\")\n",
    "        \n",
    "        # Save final model\n",
    "        model.save(os.path.join(self.config['output_dir'], 'final_model.h5'))\n",
    "        \n",
    "        return model, self.history\n",
    "    \n",
    "    def evaluate_and_report(self, test_data):\n",
    "        \"\"\"Comprehensive model evaluation and reporting\"\"\"\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
    "            \n",
    "        print(\"\\n=== Model Evaluation ===\")\n",
    "        \n",
    "        # Test set evaluation\n",
    "        test_loss, test_acc = self.model.evaluate(test_data[0], test_data[1], verbose=0)\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions = self.model.predict(test_data[0])\n",
    "        pred_classes = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        # Classification report\n",
    "        from sklearn.metrics import classification_report, confusion_matrix\n",
    "        \n",
    "        report = classification_report(test_data[1], pred_classes, output_dict=True)\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(test_data[1], pred_classes))\n",
    "        \n",
    "        # Save comprehensive report\n",
    "        final_report = {\n",
    "            'config': self.config,\n",
    "            'training_history': {k: [float(x) for x in v] for k, v in self.history.history.items()},\n",
    "            'test_metrics': {\n",
    "                'loss': float(test_loss),\n",
    "                'accuracy': float(test_acc)\n",
    "            },\n",
    "            'classification_report': report,\n",
    "            'model_summary': {\n",
    "                'total_params': int(self.model.count_params()),\n",
    "                'trainable_params': int(sum([tf.keras.backend.count_params(w) for w in self.model.trainable_weights]))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        report_path = os.path.join(self.config['output_dir'], 'training_report.json')\n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(final_report, f, indent=2)\n",
    "            \n",
    "        print(f\"Detailed report saved to: {report_path}\")\n",
    "        \n",
    "        return final_report\n",
    "\n",
    "# Production training configuration\n",
    "production_config = {\n",
    "    'model_type': 'regularized',\n",
    "    'input_dim': X_train.shape[1],\n",
    "    'num_classes': 5,\n",
    "    'units': [128, 64],\n",
    "    'dropout_rate': 0.3,\n",
    "    'optimizer': 'adamw',\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 0.01,\n",
    "    'loss_function': 'sparse_categorical_crossentropy',\n",
    "    'metrics': ['accuracy'],\n",
    "    'epochs': 100,\n",
    "    'batch_size': 64,\n",
    "    'monitor_metric': 'val_accuracy',\n",
    "    'patience': 15,\n",
    "    'lr_patience': 5,\n",
    "    'lr_schedule': 'reduce_on_plateau',\n",
    "    'gradient_monitoring': True,\n",
    "    'output_dir': log_base_dir\n",
    "}\n",
    "\n",
    "# Run production pipeline\n",
    "pipeline = ProductionTrainingPipeline(production_config)\n",
    "production_model, production_history = pipeline.train(\n",
    "    (X_train, y_train), \n",
    "    (X_test, y_test)\n",
    ")\n",
    "\n",
    "# Generate comprehensive report\n",
    "final_report = pipeline.evaluate_and_report((X_test, y_test))\n",
    "\n",
    "print(f\"\\n Production training pipeline completed successfully!\")\n",
    "print(f\" All artifacts saved to: {log_base_dir}\")\n",
    "print(f\" Final test accuracy: {final_report['test_metrics']['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**File Location:** `notebooks/02_neural_networks_with_keras/06_training_optimization_keras.ipynb`\n",
    "\n",
    "This comprehensive notebook mastered advanced tf.keras training optimization:\n",
    "\n",
    "### Advanced Callbacks Implemented:\n",
    "1. **Adaptive Early Stopping**: Dynamic patience adjustment based on improvement patterns\n",
    "2. **Learning Rate Logging**: Comprehensive LR tracking with visualization\n",
    "3. **Gradient Monitoring**: Real-time gradient norm analysis\n",
    "4. **Performance Profiling**: Memory and timing analysis during training\n",
    "\n",
    "### Optimization Techniques Covered:\n",
    "- **Custom Learning Rate Schedules**: Warmup, cosine decay, cyclical, one-cycle policies\n",
    "- **Advanced Optimizers**: Custom AdamW with gradient clipping and weight decay\n",
    "- **Hyperparameter Tuning**: Automated search with comprehensive parameter spaces\n",
    "- **Regularization Methods**: DropBlock, spectral normalization, stochastic depth\n",
    "\n",
    "### Training Infrastructure:\n",
    "- **Production Pipeline**: Complete automated training system\n",
    "- **Comprehensive Monitoring**: Real-time performance tracking\n",
    "- **Automated Reporting**: Detailed training reports and analysis\n",
    "- **Benchmark Suite**: Model performance comparison framework\n",
    "\n",
    "### Key Insights Gained:\n",
    "- Adaptive callbacks improve training robustness\n",
    "- Custom learning rate schedules can significantly accelerate convergence\n",
    "- Comprehensive regularization reduces overfitting effectively\n",
    "- Automated hyperparameter tuning finds better configurations than manual search\n",
    "- Production pipelines ensure reproducible, monitored training\n",
    "\n",
    "### Production Features:\n",
    "- **Automated Checkpointing**: Best model preservation\n",
    "- **Performance Monitoring**: Resource usage tracking\n",
    "- **Comprehensive Logging**: TensorBoard integration with custom metrics\n",
    "- **Report Generation**: Automated training summary reports\n",
    "\n",
    "### Next Steps:\n",
    "- Apply these techniques to computer vision models (CNN architectures)\n",
    "- Implement distributed training with optimization strategies\n",
    "- Deploy optimized training pipelines to cloud infrastructure\n",
    "- Scale hyperparameter tuning to larger search spaces\n",
    "\n",
    "This optimization toolkit enables building production-ready training systems with state-of-the-art techniques!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
