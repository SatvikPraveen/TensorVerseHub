{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 transformers attention keras\n",
    "**Location: TensorVerseHub/notebooks/04_natural_language_processing/11_transformers_attention_keras.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers & Attention with tf.keras\n",
    "\n",
    "**File Location:** `notebooks/04_natural_language_processing/11_transformers_attention_keras.ipynb`\n",
    "\n",
    "Master Transformer architectures using tf.keras.layers.MultiHeadAttention, implement self-attention mechanisms, positional encoding, and build BERT-style models. Create state-of-the-art NLP models for text classification, language modeling, and sequence tasks.\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement Transformer architecture with tf.keras.layers.MultiHeadAttention\n",
    "- Build positional encoding and attention mechanisms from scratch\n",
    "- Create encoder-only, decoder-only, and encoder-decoder Transformers\n",
    "- Apply attention to various NLP tasks (classification, generation)\n",
    "- Optimize Transformer models for production deployment\n",
    "- Handle long sequences and memory-efficient attention\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Foundation: Understanding Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Understanding the core attention mechanism\n",
    "def scaled_dot_product_attention(queries, keys, values, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention mechanism - the heart of transformers\n",
    "    \n",
    "    Args:\n",
    "        queries: Query matrix (batch_size, seq_len_q, d_k)\n",
    "        keys: Key matrix (batch_size, seq_len_k, d_k)\n",
    "        values: Value matrix (batch_size, seq_len_v, d_v)\n",
    "        mask: Optional mask to prevent attention to certain positions\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (batch_size, seq_len_q, d_v)\n",
    "        attention_weights: Attention weights for visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Calculate attention scores (Q * K^T)\n",
    "    scores = tf.matmul(queries, keys, transpose_b=True)\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k) to prevent softmax saturation\n",
    "    dk = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    scaled_scores = scores / tf.math.sqrt(dk)\n",
    "    \n",
    "    # Step 3: Apply mask if provided (set masked positions to large negative value)\n",
    "    if mask is not None:\n",
    "        scaled_scores += (mask * -1e9)\n",
    "    \n",
    "    # Step 4: Apply softmax to get attention weights\n",
    "    attention_weights = tf.nn.softmax(scaled_scores, axis=-1)\n",
    "    \n",
    "    # Step 5: Apply attention weights to values\n",
    "    output = tf.matmul(attention_weights, values)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Demonstrate attention with a simple example\n",
    "print(\"=== Attention Mechanism Demonstration ===\")\n",
    "\n",
    "# Create sample sequences\n",
    "batch_size, seq_len, d_model = 2, 5, 8\n",
    "queries = tf.random.normal((batch_size, seq_len, d_model))\n",
    "keys = tf.random.normal((batch_size, seq_len, d_model))\n",
    "values = tf.random.normal((batch_size, seq_len, d_model))\n",
    "\n",
    "# Apply attention\n",
    "attention_output, attention_weights = scaled_dot_product_attention(queries, keys, values)\n",
    "\n",
    "print(f\"Input shapes - Q: {queries.shape}, K: {keys.shape}, V: {values.shape}\")\n",
    "print(f\"Attention output shape: {attention_output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "\n",
    "# Visualize attention pattern\n",
    "def plot_attention_weights(attention_weights, title=\"Attention Weights\"):\n",
    "    \"\"\"Visualize attention weights as a heatmap\"\"\"\n",
    "    \n",
    "    # Take first sample, average across heads if multi-head\n",
    "    if len(attention_weights.shape) == 4:\n",
    "        weights = attention_weights[0, 0].numpy()  # First sample, first head\n",
    "    else:\n",
    "        weights = attention_weights[0].numpy()  # First sample\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(weights, annot=True, fmt='.2f', cmap='Blues', \n",
    "                xticklabels=[f'Key_{i}' for i in range(weights.shape[1])],\n",
    "                yticklabels=[f'Query_{i}' for i in range(weights.shape[0])])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Key Positions')\n",
    "    plt.ylabel('Query Positions')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_attention_weights(attention_weights, \"Basic Attention Pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Head Attention Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Multi-Head Attention Layer\n",
    "class CustomMultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom implementation of Multi-Head Attention to understand the internals\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.depth = embed_dim // num_heads\n",
    "        \n",
    "        # Linear transformations for Q, K, V\n",
    "        self.query_dense = tf.keras.layers.Dense(embed_dim, name='query_projection')\n",
    "        self.key_dense = tf.keras.layers.Dense(embed_dim, name='key_projection')\n",
    "        self.value_dense = tf.keras.layers.Dense(embed_dim, name='value_projection')\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_dense = tf.keras.layers.Dense(embed_dim, name='output_projection')\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth)\"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, inputs, attention_mask=None, training=None):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        # Linear transformations and head splitting\n",
    "        queries = self.query_dense(inputs)\n",
    "        keys = self.key_dense(inputs)\n",
    "        values = self.value_dense(inputs)\n",
    "        \n",
    "        queries = self.split_heads(queries, batch_size)  # (batch, heads, seq_len, depth)\n",
    "        keys = self.split_heads(keys, batch_size)\n",
    "        values = self.split_heads(values, batch_size)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attention_output, attention_weights = scaled_dot_product_attention(\n",
    "            queries, keys, values, attention_mask\n",
    "        )\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention_output, \n",
    "                                    (batch_size, -1, self.embed_dim))\n",
    "        \n",
    "        # Final linear transformation\n",
    "        output = self.output_dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Compare custom implementation with tf.keras version\n",
    "print(\"=== Multi-Head Attention Comparison ===\")\n",
    "\n",
    "embed_dim, num_heads = 64, 8\n",
    "sample_input = tf.random.normal((2, 10, embed_dim))\n",
    "\n",
    "# Custom implementation\n",
    "custom_mha = CustomMultiHeadAttention(embed_dim, num_heads)\n",
    "custom_output, custom_weights = custom_mha(sample_input)\n",
    "\n",
    "# tf.keras implementation\n",
    "keras_mha = tf.keras.layers.MultiHeadAttention(\n",
    "    num_heads=num_heads, \n",
    "    key_dim=embed_dim//num_heads\n",
    ")\n",
    "keras_output = keras_mha(sample_input, sample_input)\n",
    "\n",
    "print(f\"Custom MHA output shape: {custom_output.shape}\")\n",
    "print(f\"Keras MHA output shape: {keras_output.shape}\")\n",
    "print(f\"Custom MHA parameters: {custom_mha.count_params():,}\")\n",
    "print(f\"Keras MHA parameters: {keras_mha.count_params():,}\")\n",
    "\n",
    "# Visualize multi-head attention patterns\n",
    "if custom_weights.shape[1] >= 4:  # If we have at least 4 heads\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for head in range(4):\n",
    "        weights = custom_weights[0, head].numpy()  # First sample, specific head\n",
    "        ax = axes[head]\n",
    "        sns.heatmap(weights, annot=False, cmap='Blues', ax=ax, cbar=True)\n",
    "        ax.set_title(f'Head {head + 1}')\n",
    "        ax.set_xlabel('Key Position')\n",
    "        ax.set_ylabel('Query Position')\n",
    "    \n",
    "    plt.suptitle('Multi-Head Attention Patterns', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding Implementation\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding for transformers\n",
    "    Since transformers have no inherent notion of position, we add position information\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_position=10000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_position = max_position\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.seq_len = input_shape[1]\n",
    "        self.embed_dim = input_shape[2]\n",
    "        \n",
    "        # Create position encodings\n",
    "        position = tf.range(self.seq_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, self.embed_dim, 2, dtype=tf.float32) * \n",
    "                         -(tf.math.log(10000.0) / self.embed_dim))\n",
    "        \n",
    "        # Apply sin to even indices, cos to odd indices\n",
    "        pos_encoding = tf.zeros((self.seq_len, self.embed_dim))\n",
    "        \n",
    "        # Create the encoding matrix\n",
    "        even_indices = tf.range(0, self.embed_dim, 2)\n",
    "        odd_indices = tf.range(1, self.embed_dim, 2)\n",
    "        \n",
    "        sin_encoding = tf.sin(position * div_term)\n",
    "        cos_encoding = tf.cos(position * div_term)\n",
    "        \n",
    "        # Interleave sin and cos\n",
    "        encoding_matrix = tf.zeros((self.seq_len, self.embed_dim))\n",
    "        \n",
    "        # Use scatter_nd to place sin values at even indices\n",
    "        sin_indices = tf.stack([\n",
    "            tf.repeat(tf.range(self.seq_len), len(even_indices)),\n",
    "            tf.tile(even_indices, [self.seq_len])\n",
    "        ], axis=1)\n",
    "        sin_values = tf.reshape(sin_encoding, [-1])\n",
    "        \n",
    "        encoding_matrix = tf.tensor_scatter_nd_add(encoding_matrix, sin_indices, sin_values)\n",
    "        \n",
    "        # Use scatter_nd to place cos values at odd indices\n",
    "        if len(odd_indices) > 0:\n",
    "            cos_indices = tf.stack([\n",
    "                tf.repeat(tf.range(self.seq_len), len(odd_indices)),\n",
    "                tf.tile(odd_indices, [self.seq_len])\n",
    "            ], axis=1)\n",
    "            cos_values = tf.reshape(cos_encoding[:, :len(odd_indices)], [-1])\n",
    "            \n",
    "            encoding_matrix = tf.tensor_scatter_nd_add(encoding_matrix, cos_indices, cos_values)\n",
    "        \n",
    "        self.pos_encoding = tf.Variable(\n",
    "            encoding_matrix[tf.newaxis, :, :], \n",
    "            trainable=False, \n",
    "            name=\"positional_encoding\"\n",
    "        )\n",
    "        \n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'max_position': self.max_position})\n",
    "        return config\n",
    "\n",
    "# Visualize positional encodings\n",
    "print(\"=== Positional Encoding Visualization ===\")\n",
    "\n",
    "# Create sample embeddings\n",
    "sample_seq_len, sample_embed_dim = 50, 128\n",
    "sample_embeddings = tf.zeros((1, sample_seq_len, sample_embed_dim))\n",
    "\n",
    "# Apply positional encoding\n",
    "pos_encoder = PositionalEncoding()\n",
    "pos_encoded = pos_encoder(sample_embeddings)\n",
    "\n",
    "# Extract the positional encoding for visualization\n",
    "pos_encoding_matrix = pos_encoder.pos_encoding[0].numpy()\n",
    "\n",
    "# Plot positional encoding patterns\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot 1: Heatmap of positional encodings\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(pos_encoding_matrix[:30, :64].T, cmap='RdBu', center=0, \n",
    "            xticklabels=False, yticklabels=False)\n",
    "plt.title('Positional Encoding Heatmap\\n(First 30 positions, 64 dimensions)')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Encoding Dimension')\n",
    "\n",
    "# Plot 2: Sinusoidal patterns for specific dimensions\n",
    "plt.subplot(2, 2, 2)\n",
    "dims_to_plot = [0, 1, 4, 8, 16, 32]\n",
    "for dim in dims_to_plot:\n",
    "    plt.plot(pos_encoding_matrix[:, dim], label=f'Dim {dim}')\n",
    "plt.title('Positional Encoding Patterns')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Encoding Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Frequency analysis\n",
    "plt.subplot(2, 2, 3)\n",
    "frequencies = []\n",
    "for i in range(0, sample_embed_dim, 2):\n",
    "    freq = 1 / (10000 ** (i / sample_embed_dim))\n",
    "    frequencies.append(freq)\n",
    "\n",
    "plt.semilogy(range(0, sample_embed_dim, 2), frequencies)\n",
    "plt.title('Frequency Spectrum of Positional Encoding')\n",
    "plt.xlabel('Dimension Index (Even)')\n",
    "plt.ylabel('Frequency (log scale)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Position similarity (dot product)\n",
    "plt.subplot(2, 2, 4)\n",
    "pos_similarities = np.dot(pos_encoding_matrix, pos_encoding_matrix.T)\n",
    "sns.heatmap(pos_similarities[:20, :20], annot=False, cmap='coolwarm', center=0)\n",
    "plt.title('Position Similarity Matrix\\n(First 20 positions)')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Transformer Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder Block\n",
    "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Complete transformer encoder block with:\n",
    "    - Multi-head self-attention\n",
    "    - Feed-forward network\n",
    "    - Residual connections\n",
    "    - Layer normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Multi-head attention layer\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=embed_dim // num_heads,\n",
    "            dropout=dropout_rate,\n",
    "            name='multi_head_attention'\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation='relu', name='ffn_dense1'),\n",
    "            tf.keras.layers.Dropout(dropout_rate, name='ffn_dropout'),\n",
    "            tf.keras.layers.Dense(embed_dim, name='ffn_dense2')\n",
    "        ], name='feed_forward')\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name='layernorm1')\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name='layernorm2')\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate, name='dropout1')\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate, name='dropout2')\n",
    "    \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # Multi-head attention with residual connection\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask, training=training\n",
    "        )\n",
    "        attention_output = self.dropout1(attention_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attention_output)  # Residual connection\n",
    "        \n",
    "        # Feed-forward network with residual connection\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection\n",
    "        \n",
    "        return out2\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "            'dropout_rate': self.dropout_rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Transformer Decoder Block\n",
    "class TransformerDecoderBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Complete transformer decoder block with:\n",
    "    - Masked multi-head self-attention\n",
    "    - Cross-attention (encoder-decoder attention)\n",
    "    - Feed-forward network\n",
    "    - Residual connections and layer normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # Self-attention (with look-ahead mask)\n",
    "        self.self_attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            key_dim=embed_dim // num_heads,\n",
    "            dropout=dropout_rate,\n",
    "            name='self_attention'\n",
    "        )\n",
    "        \n",
    "        # Cross-attention (encoder-decoder attention)\n",
    "        self.cross_attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim // num_heads,\n",
    "            dropout=dropout_rate,\n",
    "            name='cross_attention'\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(embed_dim)\n",
    "        ], name='feed_forward')\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, inputs, encoder_outputs, training=None, \n",
    "             look_ahead_mask=None, padding_mask=None):\n",
    "        \n",
    "        # Masked self-attention\n",
    "        self_attn_output = self.self_attention(\n",
    "            inputs, inputs, attention_mask=look_ahead_mask, training=training\n",
    "        )\n",
    "        self_attn_output = self.dropout1(self_attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + self_attn_output)\n",
    "        \n",
    "        # Cross-attention\n",
    "        cross_attn_output = self.cross_attention(\n",
    "            out1, encoder_outputs, attention_mask=padding_mask, training=training\n",
    "        )\n",
    "        cross_attn_output = self.dropout2(cross_attn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + cross_attn_output)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(out2, training=training)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(out2 + ffn_output)\n",
    "        \n",
    "        return out3\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "            'dropout_rate': self.dropout_rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Attention mask utilities\n",
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"Create look-ahead mask for decoder self-attention\"\"\"\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def create_padding_mask(seq, pad_token=0):\n",
    "    \"\"\"Create padding mask for attention\"\"\"\n",
    "    seq = tf.cast(tf.math.equal(seq, pad_token), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "# Test transformer building blocks\n",
    "print(\"=== Testing Transformer Building Blocks ===\")\n",
    "\n",
    "# Create sample data\n",
    "batch_size, seq_len, embed_dim = 2, 10, 128\n",
    "num_heads, ff_dim = 8, 256\n",
    "\n",
    "sample_input = tf.random.normal((batch_size, seq_len, embed_dim))\n",
    "\n",
    "# Test encoder block\n",
    "encoder_block = TransformerEncoderBlock(embed_dim, num_heads, ff_dim, dropout_rate=0.1)\n",
    "encoder_output = encoder_block(sample_input, training=True)\n",
    "\n",
    "print(f\"Encoder Block:\")\n",
    "print(f\"  Input shape: {sample_input.shape}\")\n",
    "print(f\"  Output shape: {encoder_output.shape}\")\n",
    "print(f\"  Parameters: {encoder_block.count_params():,}\")\n",
    "\n",
    "# Test decoder block\n",
    "decoder_block = TransformerDecoderBlock(embed_dim, num_heads, ff_dim, dropout_rate=0.1)\n",
    "\n",
    "# Create masks for decoder\n",
    "look_ahead_mask = create_look_ahead_mask(seq_len)\n",
    "sample_encoder_output = tf.random.normal((batch_size, seq_len, embed_dim))\n",
    "\n",
    "decoder_output = decoder_block(\n",
    "    sample_input, sample_encoder_output,\n",
    "    look_ahead_mask=look_ahead_mask,\n",
    "    training=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDecoder Block:\")\n",
    "print(f\"  Input shape: {sample_input.shape}\")\n",
    "print(f\"  Encoder output shape: {sample_encoder_output.shape}\")\n",
    "print(f\"  Decoder output shape: {decoder_output.shape}\")\n",
    "print(f\"  Parameters: {decoder_block.count_params():,}\")\n",
    "\n",
    "# Visualize look-ahead mask\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(look_ahead_mask.numpy(), annot=True, fmt='.0f', cmap='Reds',\n",
    "            xticklabels=[f'Pos {i}' for i in range(seq_len)],\n",
    "            yticklabels=[f'Pos {i}' for i in range(seq_len)])\n",
    "plt.title('Look-Ahead Mask for Decoder\\n(1 = masked, 0 = allowed)')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Transformer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT-style Encoder-only Transformer\n",
    "class TransformerClassifier(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    BERT-style transformer for text classification\n",
    "    Uses only encoder blocks with [CLS] token classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=8, ff_dim=256,\n",
    "                 num_layers=4, num_classes=3, max_length=512, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, embed_dim, name='token_embedding'\n",
    "        )\n",
    "        self.pos_encoding = PositionalEncoding(max_position=max_length)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        self.encoder_layers = [\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, ff_dim, dropout_rate, name=f'encoder_{i}')\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        # Classification head\n",
    "        self.global_pool = tf.keras.layers.GlobalAveragePooling1D(name='global_pool')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate, name='final_dropout')\n",
    "        self.classifier = tf.keras.layers.Dense(\n",
    "            num_classes, activation='softmax', name='classifier'\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # Token embeddings\n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        # Scale embeddings (common in transformer literature)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Create attention mask if not provided\n",
    "        if mask is None:\n",
    "            mask = create_padding_mask(inputs)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, training=training, mask=mask)\n",
    "        \n",
    "        # Classification\n",
    "        # Use mask for global pooling to ignore padded tokens\n",
    "        if mask is not None:\n",
    "            mask_for_pool = tf.squeeze(mask, axis=[1, 2])  # Remove extra dimensions\n",
    "            pooled = self.global_pool(x, mask=mask_for_pool)\n",
    "        else:\n",
    "            pooled = self.global_pool(x)\n",
    "            \n",
    "        pooled = self.dropout(pooled, training=training)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "# GPT-style Decoder-only Transformer\n",
    "class GPTStyleTransformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    GPT-style decoder-only transformer for text generation\n",
    "    Uses causal (look-ahead) masking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=8, ff_dim=256,\n",
    "                 num_layers=6, max_length=512, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.token_embedding = tf.keras.layers.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = tf.keras.layers.Embedding(max_length, embed_dim)\n",
    "        \n",
    "        # Transformer decoder layers (self-attention only, no cross-attention)\n",
    "        self.decoder_layers = []\n",
    "        for i in range(num_layers):\n",
    "            # Use encoder blocks with causal masking instead of decoder blocks\n",
    "            self.decoder_layers.append(\n",
    "                TransformerEncoderBlock(embed_dim, num_heads, ff_dim, dropout_rate, \n",
    "                                      name=f'decoder_{i}')\n",
    "            )\n",
    "        \n",
    "        # Output layers\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.output_projection = tf.keras.layers.Dense(vocab_size, name='lm_head')\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        \n",
    "        # Token embeddings\n",
    "        token_emb = self.token_embedding(inputs)\n",
    "        \n",
    "        # Position embeddings\n",
    "        positions = tf.range(start=0, limit=seq_len, delta=1)\n",
    "        position_emb = self.position_embedding(positions)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        x = token_emb + position_emb\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Create causal mask\n",
    "        causal_mask = create_look_ahead_mask(seq_len)\n",
    "        causal_mask = causal_mask[tf.newaxis, tf.newaxis, :, :]\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            x = decoder_layer(x, training=training, mask=causal_mask)\n",
    "        \n",
    "        # Final layer norm and projection\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Encoder-Decoder Transformer (like original Transformer paper)\n",
    "class TransformerSeq2Seq(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Complete encoder-decoder transformer for sequence-to-sequence tasks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=8, ff_dim=256,\n",
    "                 num_encoder_layers=4, num_decoder_layers=4, max_length=512,\n",
    "                 dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Shared embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(max_position=max_length)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_layers = [\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ]\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder_layers = [\n",
    "            TransformerDecoderBlock(embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ]\n",
    "        \n",
    "        # Output projection\n",
    "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        encoder_input, decoder_input = inputs\n",
    "        \n",
    "        # Create masks\n",
    "        enc_padding_mask = create_padding_mask(encoder_input)\n",
    "        dec_padding_mask = create_padding_mask(encoder_input)  # For cross-attention\n",
    "        \n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(decoder_input)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(decoder_input)\n",
    "        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "        \n",
    "        # Encoder\n",
    "        enc_output = self.embedding(encoder_input)\n",
    "        enc_output *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        enc_output = self.pos_encoding(enc_output)\n",
    "        \n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_output = encoder_layer(enc_output, training=training, mask=enc_padding_mask)\n",
    "        \n",
    "        # Decoder\n",
    "        dec_output = self.embedding(decoder_input)\n",
    "        dec_output *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        dec_output = self.pos_encoding(dec_output)\n",
    "        \n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            dec_output = decoder_layer(\n",
    "                dec_output, enc_output, training=training,\n",
    "                look_ahead_mask=combined_mask, padding_mask=dec_padding_mask\n",
    "            )\n",
    "        \n",
    "        # Output projection\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        return final_output\n",
    "\n",
    "# Test complete transformer models\n",
    "print(\"=== Testing Complete Transformer Models ===\")\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = 5000\n",
    "max_length = 64  # Smaller for testing\n",
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "ff_dim = 256\n",
    "\n",
    "# Test BERT-style classifier\n",
    "print(\"1. BERT-style Classifier:\")\n",
    "bert_classifier = TransformerClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    num_layers=3,\n",
    "    num_classes=3,\n",
    "    max_length=max_length,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "sample_input = tf.random.uniform((2, max_length), maxval=vocab_size, dtype=tf.int32)\n",
    "bert_output = bert_classifier(sample_input)\n",
    "print(f\"  Input shape: {sample_input.shape}\")\n",
    "print(f\"  Output shape: {bert_output.shape}\")\n",
    "print(f\"  Parameters: {bert_classifier.count_params():,}\")\n",
    "\n",
    "# Test GPT-style model\n",
    "print(\"\\n2. GPT-style Transformer:\")\n",
    "gpt_model = GPTStyleTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    num_layers=3,\n",
    "    max_length=max_length,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "gpt_output = gpt_model(sample_input)\n",
    "print(f\"  Input shape: {sample_input.shape}\")\n",
    "print(f\"  Output shape: {gpt_output.shape}\")\n",
    "print(f\"  Parameters: {gpt_model.count_params():,}\")\n",
    "\n",
    "# Test Seq2Seq model\n",
    "print(\"\\n3. Encoder-Decoder Transformer:\")\n",
    "seq2seq_model = TransformerSeq2Seq(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    max_length=max_length,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "encoder_input = tf.random.uniform((2, max_length), maxval=vocab_size, dtype=tf.int32)\n",
    "decoder_input = tf.random.uniform((2, max_length), maxval=vocab_size, dtype=tf.int32)\n",
    "seq2seq_output = seq2seq_model([encoder_input, decoder_input])\n",
    "\n",
    "print(f\"  Encoder input: {encoder_input.shape}\")\n",
    "print(f\"  Decoder input: {decoder_input.shape}\")\n",
    "print(f\"  Output shape: {seq2seq_output.shape}\")\n",
    "print(f\"  Parameters: {seq2seq_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training and Fine-tuning Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample text data for transformer training\n",
    "def create_sample_datasets():\n",
    "    \"\"\"Create sample datasets for transformer training\"\"\"\n",
    "    \n",
    "    # Sample texts for classification\n",
    "    tech_texts = [\n",
    "        \"Machine learning algorithms can process vast amounts of data to identify patterns.\",\n",
    "        \"Artificial intelligence has revolutionized many industries through automation.\",\n",
    "        \"Deep learning models use neural networks with multiple layers.\",\n",
    "        \"Computer vision applications can recognize objects in images.\",\n",
    "        \"Natural language processing enables machines to understand human language.\"\n",
    "    ] * 20\n",
    "    \n",
    "    science_texts = [\n",
    "        \"Scientists have discovered new properties of quantum particles.\",\n",
    "        \"Climate change research shows increasing global temperatures.\",\n",
    "        \"Genetic engineering offers promising medical treatments.\",\n",
    "        \"Space exploration missions provide insights into the universe.\",\n",
    "        \"Renewable energy sources are becoming more efficient.\"\n",
    "    ] * 20\n",
    "    \n",
    "    business_texts = [\n",
    "        \"Market analysis indicates growth in the technology sector.\",\n",
    "        \"Companies are investing heavily in digital transformation.\",\n",
    "        \"Supply chain optimization reduces operational costs.\",\n",
    "        \"Customer satisfaction surveys show improved service quality.\",\n",
    "        \"Financial reports demonstrate strong quarterly performance.\"\n",
    "    ] * 20\n",
    "    \n",
    "    all_texts = tech_texts + science_texts + business_texts\n",
    "    all_labels = [0] * len(tech_texts) + [1] * len(science_texts) + [2] * len(business_texts)\n",
    "    \n",
    "    return all_texts, all_labels\n",
    "\n",
    "# Text preprocessing for transformers\n",
    "def create_tokenizer_and_preprocess(texts, vocab_size=5000, max_length=128):\n",
    "    \"\"\"Create tokenizer and preprocess texts\"\"\"\n",
    "    \n",
    "    # Simple tokenization using TextVectorization\n",
    "    tokenizer = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_sequence_length=max_length,\n",
    "        pad_to_max_tokens=False\n",
    "    )\n",
    "    \n",
    "    # Add special tokens\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\"]\n",
    "    tokenizer.adapt(special_tokens + texts)\n",
    "    \n",
    "    # Get vocabulary\n",
    "    vocab = tokenizer.get_vocabulary()\n",
    "    vocab_size_actual = len(vocab)\n",
    "    \n",
    "    print(f\"Vocabulary size: {vocab_size_actual}\")\n",
    "    print(f\"Sample vocabulary: {vocab[:10]}\")\n",
    "    \n",
    "    return tokenizer, vocab_size_actual\n",
    "\n",
    "# Advanced training configuration\n",
    "class TransformerTrainingConfig:\n",
    "    \"\"\"Configuration for transformer training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Model architecture\n",
    "        self.embed_dim = 128\n",
    "        self.num_heads = 8\n",
    "        self.ff_dim = 256\n",
    "        self.num_layers = 4\n",
    "        self.dropout_rate = 0.1\n",
    "        self.max_length = 128\n",
    "        \n",
    "        # Training hyperparameters\n",
    "        self.learning_rate = 1e-4\n",
    "        self.batch_size = 16\n",
    "        self.epochs = 10\n",
    "        self.warmup_steps = 1000\n",
    "        \n",
    "        # Optimization\n",
    "        self.weight_decay = 0.01\n",
    "        self.gradient_clip_norm = 1.0\n",
    "        self.label_smoothing = 0.1\n",
    "\n",
    "def create_learning_rate_schedule(embed_dim, warmup_steps=1000):\n",
    "    \"\"\"Create warmup + decay learning rate schedule\"\"\"\n",
    "    \n",
    "    class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, embed_dim, warmup_steps=1000):\n",
    "            super().__init__()\n",
    "            self.embed_dim = tf.cast(embed_dim, tf.float32)\n",
    "            self.warmup_steps = warmup_steps\n",
    "            \n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps ** -1.5)\n",
    "            \n",
    "            return tf.math.rsqrt(self.embed_dim) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "    return CustomSchedule(embed_dim, warmup_steps)\n",
    "\n",
    "# Custom callbacks for transformer training\n",
    "class TransformerTrainingCallbacks:\n",
    "    \"\"\"Custom callbacks for transformer training\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_callbacks(model_path='best_transformer.h5'):\n",
    "        return [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                model_path,\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_accuracy',\n",
    "                patience=5,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=3,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            tf.keras.callbacks.CSVLogger(\n",
    "                'transformer_training.csv',\n",
    "                append=True\n",
    "            )\n",
    "        ]\n",
    "\n",
    "# Train transformer classifier\n",
    "print(\"=== Training Transformer Classifier ===\")\n",
    "\n",
    "# Create sample data\n",
    "texts, labels = create_sample_datasets()\n",
    "print(f\"Dataset size: {len(texts)} texts, {len(set(labels))} classes\")\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer, actual_vocab_size = create_tokenizer_and_preprocess(texts)\n",
    "\n",
    "# Encode texts\n",
    "encoded_texts = tokenizer(texts)\n",
    "labels_array = tf.constant(labels)\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.8 * len(texts))\n",
    "val_size = int(0.1 * len(texts))\n",
    "\n",
    "X_train = encoded_texts[:train_size]\n",
    "y_train = labels_array[:train_size]\n",
    "X_val = encoded_texts[train_size:train_size + val_size]\n",
    "y_val = labels_array[train_size:train_size + val_size]\n",
    "X_test = encoded_texts[train_size + val_size:]\n",
    "y_test = labels_array[train_size + val_size:]\n",
    "\n",
    "print(f\"Training data: {X_train.shape}\")\n",
    "print(f\"Validation data: {X_val.shape}\")\n",
    "print(f\"Test data: {X_test.shape}\")\n",
    "\n",
    "# Create and configure model\n",
    "config = TransformerTrainingConfig()\n",
    "model = TransformerClassifier(\n",
    "    vocab_size=actual_vocab_size,\n",
    "    embed_dim=config.embed_dim,\n",
    "    num_heads=config.num_heads,\n",
    "    ff_dim=config.ff_dim,\n",
    "    num_layers=config.num_layers,\n",
    "    num_classes=3,\n",
    "    max_length=config.max_length,\n",
    "    dropout_rate=config.dropout_rate\n",
    ")\n",
    "\n",
    "# Create learning rate schedule\n",
    "lr_schedule = create_learning_rate_schedule(config.embed_dim, config.warmup_steps)\n",
    "\n",
    "# Compile model with AdamW optimizer\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=lr_schedule,\n",
    "    weight_decay=config.weight_decay,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-7,\n",
    "    clipnorm=config.gradient_clip_norm\n",
    ")\n",
    "\n",
    "# Label smoothing loss\n",
    "def label_smoothing_loss(y_true, y_pred, smoothing=0.1):\n",
    "    \"\"\"Apply label smoothing to categorical crossentropy\"\"\"\n",
    "    num_classes = tf.shape(y_pred)[-1]\n",
    "    y_true = tf.one_hot(tf.cast(y_true, tf.int32), num_classes)\n",
    "    \n",
    "    # Apply smoothing\n",
    "    y_true = y_true * (1 - smoothing) + smoothing / tf.cast(num_classes, tf.float32)\n",
    "    \n",
    "    return tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Model compiled. Total parameters: {model.count_params():,}\")\n",
    "\n",
    "# Train model\n",
    "callbacks = TransformerTrainingCallbacks.get_callbacks()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=config.batch_size,\n",
    "    epochs=config.epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training metrics\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Training', marker='o')\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation', marker='s')\n",
    "    axes[0].set_title('Model Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss\n",
    "    axes[1].plot(history.history['loss'], label='Training', marker='o')\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation', marker='s')\n",
    "    axes[1].set_title('Model Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Transformer Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient attention mechanisms\n",
    "class EfficientMultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Memory-efficient multi-head attention with optional techniques\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, dropout_rate=0.1, \n",
    "                 use_relative_position=False, max_relative_position=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_relative_position = use_relative_position\n",
    "        self.max_relative_position = max_relative_position\n",
    "        \n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.depth = embed_dim // num_heads\n",
    "        \n",
    "        # Use single dense layer for efficiency\n",
    "        self.qkv_dense = tf.keras.layers.Dense(embed_dim * 3, use_bias=False)\n",
    "        self.output_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # Relative position embeddings if enabled\n",
    "        if use_relative_position:\n",
    "            self.relative_position_k = tf.keras.layers.Embedding(\n",
    "                2 * max_relative_position + 1, self.depth\n",
    "            )\n",
    "            self.relative_position_v = tf.keras.layers.Embedding(\n",
    "                2 * max_relative_position + 1, self.depth\n",
    "            )\n",
    "    \n",
    "    def get_relative_positions(self, seq_len):\n",
    "        \"\"\"Get relative position indices\"\"\"\n",
    "        positions = tf.range(seq_len)[:, tf.newaxis] - tf.range(seq_len)[tf.newaxis, :]\n",
    "        positions = tf.clip_by_value(positions, -self.max_relative_position, self.max_relative_position)\n",
    "        return positions + self.max_relative_position\n",
    "    \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        \n",
    "        # Compute Q, K, V in one shot\n",
    "        qkv = self.qkv_dense(inputs)\n",
    "        qkv = tf.reshape(qkv, (batch_size, seq_len, 3, self.num_heads, self.depth))\n",
    "        qkv = tf.transpose(qkv, perm=[2, 0, 3, 1, 4])\n",
    "        \n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = tf.matmul(queries, keys, transpose_b=True)\n",
    "        \n",
    "        # Add relative position bias if enabled\n",
    "        if self.use_relative_position:\n",
    "            relative_positions = self.get_relative_positions(seq_len)\n",
    "            relative_position_scores_k = self.relative_position_k(relative_positions)\n",
    "            relative_position_scores_k = tf.transpose(relative_position_scores_k, [2, 0, 1])\n",
    "            scores += tf.einsum('bhqd,dhv->bhqv', queries, relative_position_scores_k)\n",
    "        \n",
    "        # Scale and apply mask\n",
    "        scores /= tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores += (mask * -1e9)\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        attention_weights = self.dropout(attention_weights, training=training)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attention_output = tf.matmul(attention_weights, values)\n",
    "        \n",
    "        # Add relative position bias to values if enabled\n",
    "        if self.use_relative_position:\n",
    "            relative_position_scores_v = self.relative_position_v(relative_positions)\n",
    "            relative_position_scores_v = tf.transpose(relative_position_scores_v, [2, 0, 1])\n",
    "            attention_output += tf.einsum('bhqv,dhv->bhqd', attention_weights, relative_position_scores_v)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "        attention_output = tf.reshape(attention_output, (batch_size, seq_len, self.embed_dim))\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output_dense(attention_output)\n",
    "        return output\n",
    "\n",
    "# Transformer with advanced techniques\n",
    "class AdvancedTransformerBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Advanced transformer block with modern improvements\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1,\n",
    "                 activation='gelu', pre_norm=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.pre_norm = pre_norm\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = EfficientMultiHeadAttention(\n",
    "            embed_dim, num_heads, dropout_rate, use_relative_position=True\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network with GELU activation\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation=activation),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(embed_dim)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        if self.pre_norm:\n",
    "            # Pre-normalization (more stable training)\n",
    "            normed_inputs = self.layernorm1(inputs)\n",
    "            attention_output = self.attention(normed_inputs, training=training, mask=mask)\n",
    "            attention_output = self.dropout1(attention_output, training=training)\n",
    "            out1 = inputs + attention_output\n",
    "            \n",
    "            normed_out1 = self.layernorm2(out1)\n",
    "            ffn_output = self.ffn(normed_out1, training=training)\n",
    "            ffn_output = self.dropout2(ffn_output, training=training)\n",
    "            return out1 + ffn_output\n",
    "        else:\n",
    "            # Post-normalization (original transformer)\n",
    "            attention_output = self.attention(inputs, training=training, mask=mask)\n",
    "            attention_output = self.dropout1(attention_output, training=training)\n",
    "            out1 = self.layernorm1(inputs + attention_output)\n",
    "            \n",
    "            ffn_output = self.ffn(out1, training=training)\n",
    "            ffn_output = self.dropout2(ffn_output, training=training)\n",
    "            return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# Sparse attention patterns for long sequences\n",
    "class SparseAttentionTransformer(tf.keras.Model):\n",
    "    \"\"\"Transformer with sparse attention for handling long sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=8, ff_dim=256,\n",
    "                 num_layers=4, max_length=1024, attention_pattern='local',\n",
    "                 window_size=256, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.attention_pattern = attention_pattern\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(max_position=max_length)\n",
    "        \n",
    "        # Transformer layers with sparse attention\n",
    "        self.transformer_layers = [\n",
    "            AdvancedTransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.output_projection = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def create_sparse_attention_mask(self, seq_len):\n",
    "        \"\"\"Create sparse attention mask based on pattern\"\"\"\n",
    "        \n",
    "        if self.attention_pattern == 'local':\n",
    "            # Local attention: attend to nearby tokens\n",
    "            mask = tf.ones((seq_len, seq_len)) * -1e9\n",
    "            \n",
    "            for i in range(seq_len):\n",
    "                start = max(0, i - self.window_size // 2)\n",
    "                end = min(seq_len, i + self.window_size // 2 + 1)\n",
    "                mask = tf.tensor_scatter_nd_update(\n",
    "                    mask, \n",
    "                    [[i, j] for j in range(start, end)], \n",
    "                    tf.zeros(end - start)\n",
    "                )\n",
    "        \n",
    "        elif self.attention_pattern == 'strided':\n",
    "            # Strided attention: attend to every k-th token\n",
    "            mask = tf.ones((seq_len, seq_len)) * -1e9\n",
    "            stride = self.window_size\n",
    "            \n",
    "            for i in range(seq_len):\n",
    "                # Local attention\n",
    "                local_start = max(0, i - 64)\n",
    "                local_end = min(seq_len, i + 64)\n",
    "                \n",
    "                # Global attention (strided)\n",
    "                global_indices = list(range(0, seq_len, stride))\n",
    "                \n",
    "                all_indices = list(range(local_start, local_end)) + global_indices\n",
    "                all_indices = list(set(all_indices))  # Remove duplicates\n",
    "                \n",
    "                for j in all_indices:\n",
    "                    if j < seq_len:\n",
    "                        mask = tf.tensor_scatter_nd_update(mask, [[i, j]], [0.0])\n",
    "        \n",
    "        else:  # 'full' attention\n",
    "            mask = tf.zeros((seq_len, seq_len))\n",
    "        \n",
    "        return mask[tf.newaxis, tf.newaxis, :, :]  # Add batch and head dimensions\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        \n",
    "        # Embeddings\n",
    "        x = self.embedding(inputs)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Create sparse attention mask\n",
    "        sparse_mask = self.create_sparse_attention_mask(seq_len)\n",
    "        \n",
    "        # Apply transformer layers\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x, training=training, mask=sparse_mask)\n",
    "        \n",
    "        # Output\n",
    "        x = self.layer_norm(x)\n",
    "        return self.output_projection(x)\n",
    "\n",
    "# Test advanced techniques\n",
    "print(\"=== Testing Advanced Transformer Techniques ===\")\n",
    "\n",
    "# Test efficient attention\n",
    "print(\"1. Efficient Multi-Head Attention:\")\n",
    "efficient_attention = EfficientMultiHeadAttention(\n",
    "    embed_dim=128, num_heads=8, use_relative_position=True\n",
    ")\n",
    "\n",
    "sample_input = tf.random.normal((2, 32, 128))\n",
    "efficient_output = efficient_attention(sample_input)\n",
    "\n",
    "print(f\"  Input shape: {sample_input.shape}\")\n",
    "print(f\"  Output shape: {efficient_output.shape}\")\n",
    "print(f\"  Parameters: {efficient_attention.count_params():,}\")\n",
    "\n",
    "# Test advanced transformer block\n",
    "print(\"\\n2. Advanced Transformer Block:\")\n",
    "advanced_block = AdvancedTransformerBlock(\n",
    "    embed_dim=128, num_heads=8, ff_dim=256, \n",
    "    activation='gelu', pre_norm=True\n",
    ")\n",
    "\n",
    "advanced_output = advanced_block(sample_input)\n",
    "print(f\"  Output shape: {advanced_output.shape}\")\n",
    "print(f\"  Parameters: {advanced_block.count_params():,}\")\n",
    "\n",
    "# Test sparse attention transformer\n",
    "print(\"\\n3. Sparse Attention Transformer:\")\n",
    "sparse_transformer = SparseAttentionTransformer(\n",
    "    vocab_size=5000,\n",
    "    embed_dim=128,\n",
    "    num_heads=8,\n",
    "    ff_dim=256,\n",
    "    num_layers=3,\n",
    "    max_length=512,\n",
    "    attention_pattern='local',\n",
    "    window_size=64\n",
    ")\n",
    "\n",
    "sparse_input = tf.random.uniform((2, 128), maxval=5000, dtype=tf.int32)\n",
    "sparse_output = sparse_transformer(sparse_input)\n",
    "\n",
    "print(f\"  Input shape: {sparse_input.shape}\")\n",
    "print(f\"  Output shape: {sparse_output.shape}\")\n",
    "print(f\"  Parameters: {sparse_transformer.count_params():,}\")\n",
    "\n",
    "# Visualize sparse attention patterns\n",
    "seq_len = 20\n",
    "local_mask = sparse_transformer.create_sparse_attention_mask(seq_len)\n",
    "local_mask_viz = tf.where(local_mask[0, 0] == 0, 1.0, 0.0)  # Invert for visualization\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(local_mask_viz.numpy(), cmap='Blues', cbar=True, square=True)\n",
    "plt.title('Local Sparse Attention Pattern')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "\n",
    "# Create strided pattern for comparison\n",
    "sparse_transformer.attention_pattern = 'strided'\n",
    "sparse_transformer.window_size = 8\n",
    "strided_mask = sparse_transformer.create_sparse_attention_mask(seq_len)\n",
    "strided_mask_viz = tf.where(strided_mask[0, 0] == 0, 1.0, 0.0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(strided_mask_viz.numpy(), cmap='Greens', cbar=True, square=True)\n",
    "plt.title('Strided Sparse Attention Pattern')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This comprehensive notebook has covered the complete implementation of Transformer architectures using tf.keras:\n",
    "\n",
    "### Key Concepts Mastered\n",
    "\n",
    "**1. Attention Mechanisms:**\n",
    "- Scaled dot-product attention from scratch\n",
    "- Multi-head attention implementation and comparison\n",
    "- Attention visualization and interpretation\n",
    "\n",
    "**2. Transformer Components:**\n",
    "- Positional encoding with sinusoidal patterns\n",
    "- Encoder and decoder blocks with residual connections\n",
    "- Layer normalization and feed-forward networks\n",
    "\n",
    "**3. Complete Architectures:**\n",
    "- BERT-style encoder-only models for classification\n",
    "- GPT-style decoder-only models for generation\n",
    "- Full encoder-decoder models for sequence-to-sequence tasks\n",
    "\n",
    "**4. Training and Optimization:**\n",
    "- Custom learning rate schedules with warmup\n",
    "- Label smoothing and advanced optimization techniques\n",
    "- Comprehensive evaluation and visualization\n",
    "\n",
    "**5. Advanced Techniques:**\n",
    "- Memory-efficient attention mechanisms\n",
    "- Relative position embeddings\n",
    "- Sparse attention patterns for long sequences\n",
    "- Pre-normalization and modern architectural improvements\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "The implementations in this notebook enable you to:\n",
    "- Build state-of-the-art NLP models for classification, generation, and translation\n",
    "- Handle sequences of varying lengths efficiently\n",
    "- Scale to longer sequences using sparse attention\n",
    "- Apply transfer learning principles to transformer models\n",
    "- Optimize models for production deployment\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue to notebook 12 to explore TensorFlow Hub integration and pre-trained transformer models, where you'll learn to leverage existing models and fine-tune them for specific tasks.\n",
    "\n",
    "The transformer architecture has revolutionized NLP and continues to be the foundation for the most advanced language models. The implementations here provide a solid foundation for understanding and building upon these powerful architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
