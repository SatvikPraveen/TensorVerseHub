{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16 tensorflow model optimization\n",
    "**Location: TensorVerseHub/notebooks/06_model_optimization/16_tensorflow_model_optimization.ipynb**\n",
    "\n",
    "TODO: Implement comprehensive TensorFlow + tf.keras learning content.\n",
    "\n",
    "## Learning Objectives\n",
    "- TODO: Define specific learning objectives\n",
    "- TODO: List key TensorFlow concepts covered\n",
    "- TODO: Outline tf.keras integration points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "# TODO: Add comprehensive implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Model Optimization with tf.keras Integration\n",
    "\n",
    "**File Location:** `notebooks/06_model_optimization/16_tensorflow_model_optimization.ipynb`\n",
    "\n",
    "Master TensorFlow Model Optimization techniques including quantization, pruning, clustering, and distillation with seamless tf.keras integration. Learn to compress models for production deployment while maintaining accuracy.\n",
    "\n",
    "## Learning Objectives\n",
    "- Apply post-training quantization for immediate model compression\n",
    "- Implement quantization-aware training with tf.keras\n",
    "- Master structured and unstructured pruning techniques\n",
    "- Use clustering for model compression and acceleration\n",
    "- Apply knowledge distillation for student-teacher learning\n",
    "- Combine multiple optimization techniques for maximum efficiency\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Post-Training Quantization\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"TF Model Optimization version: {tfmot.__version__}\")\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create baseline model for optimization experiments\n",
    "def create_baseline_cnn(input_shape=(32, 32, 3), num_classes=10):\n",
    "    \"\"\"Create baseline CNN for optimization experiments\"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv2D(32, 3, activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, 3, activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        layers.Conv2D(128, 3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, 3, activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ], name='baseline_cnn')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load and prepare CIFAR-10 data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"Data loaded - Train: {x_train.shape}, Test: {x_test.shape}\")\n",
    "\n",
    "# Train baseline model (demo)\n",
    "baseline_model = create_baseline_cnn()\n",
    "baseline_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "baseline_history = baseline_model.fit(\n",
    "    x_train[:5000], y_train[:5000],\n",
    "    batch_size=128, epochs=3,\n",
    "    validation_data=(x_test[:1000], y_test[:1000]),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "baseline_test_loss, baseline_test_acc = baseline_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Baseline Test Accuracy: {baseline_test_acc:.4f}\")\n",
    "\n",
    "# Post-training quantization utilities\n",
    "def apply_quantization(model, method='dynamic', representative_data=None):\n",
    "    \"\"\"Apply different quantization methods\"\"\"\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    if method == 'dynamic':\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    elif method == 'float16':\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "    elif method == 'int8' and representative_data is not None:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        def representative_dataset():\n",
    "            for sample in representative_data[:100]:\n",
    "                yield [sample.astype(np.float32)]\n",
    "        converter.representative_dataset = representative_dataset\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.uint8\n",
    "        converter.inference_output_type = tf.uint8\n",
    "    \n",
    "    return converter.convert()\n",
    "\n",
    "# Apply different quantization methods\n",
    "print(\"\\n=== Applying Post-Training Quantization ===\")\n",
    "\n",
    "representative_data = x_test[:100]\n",
    "dynamic_model = apply_quantization(baseline_model, 'dynamic')\n",
    "float16_model = apply_quantization(baseline_model, 'float16')\n",
    "int8_model = apply_quantization(baseline_model, 'int8', representative_data)\n",
    "\n",
    "# Compare sizes\n",
    "original_size = len(tf.keras.models.model_to_json(baseline_model).encode('utf-8')) / 1024\n",
    "dynamic_size = len(dynamic_model) / 1024\n",
    "float16_size = len(float16_model) / 1024\n",
    "int8_size = len(int8_model) / 1024\n",
    "\n",
    "print(f\"Model Sizes (KB):\")\n",
    "print(f\"  Dynamic Quantized: {dynamic_size:.1f} ({original_size/dynamic_size:.1f}x compression)\")\n",
    "print(f\"  Float16 Quantized: {float16_size:.1f} ({original_size/float16_size:.1f}x compression)\")\n",
    "print(f\"  Int8 Quantized: {int8_size:.1f} ({original_size/int8_size:.1f}x compression)\")\n",
    "\n",
    "# Evaluate quantized model\n",
    "def evaluate_tflite_model(model_bytes, test_data, test_labels, num_samples=500):\n",
    "    \"\"\"Evaluate TFLite model accuracy\"\"\"\n",
    "    \n",
    "    interpreter = tf.lite.Interpreter(model_content=model_bytes)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    correct = 0\n",
    "    for i in range(min(num_samples, len(test_data))):\n",
    "        input_data = np.expand_dims(test_data[i], axis=0).astype(input_details[0]['dtype'])\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predicted = np.argmax(output)\n",
    "        actual = np.argmax(test_labels[i])\n",
    "        \n",
    "        if predicted == actual:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / min(num_samples, len(test_data))\n",
    "\n",
    "dynamic_acc = evaluate_tflite_model(dynamic_model, x_test, y_test)\n",
    "float16_acc = evaluate_tflite_model(float16_model, x_test, y_test)\n",
    "\n",
    "print(f\"\\nAccuracy Comparison:\")\n",
    "print(f\"  Original: {baseline_test_acc:.4f}\")\n",
    "print(f\"  Dynamic Quantized: {dynamic_acc:.4f}\")\n",
    "print(f\"  Float16 Quantized: {float16_acc:.4f}\")\n",
    "```\n",
    "\n",
    "## 2. Quantization-Aware Training (QAT)\n",
    "\n",
    "```python\n",
    "# Quantization-Aware Training\n",
    "print(\"=== Quantization-Aware Training ===\")\n",
    "\n",
    "# Create QAT model\n",
    "qat_model = tfmot.quantization.keras.quantize_model(baseline_model)\n",
    "qat_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(f\"QAT Model Parameters: {qat_model.count_params():,}\")\n",
    "\n",
    "# Train QAT model (fine-tuning)\n",
    "print(\"Fine-tuning with quantization awareness...\")\n",
    "qat_history = qat_model.fit(\n",
    "    x_train[:3000], y_train[:3000],\n",
    "    batch_size=128, epochs=3,\n",
    "    validation_data=(x_test[:1000], y_test[:1000]),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate QAT model\n",
    "qat_test_loss, qat_test_acc = qat_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"QAT Model Test Accuracy: {qat_test_acc:.4f}\")\n",
    "\n",
    "# Convert QAT model to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "qat_quantized_model = converter.convert()\n",
    "\n",
    "qat_quantized_acc = evaluate_tflite_model(qat_quantized_model, x_test, y_test)\n",
    "print(f\"QAT Quantized Model Accuracy: {qat_quantized_acc:.4f}\")\n",
    "\n",
    "# Compare QAT vs Post-training quantization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Accuracy comparison\n",
    "methods = ['Original', 'Post-Training\\nDynamic', 'Post-Training\\nFloat16', 'QAT\\nQuantized']\n",
    "accuracies = [baseline_test_acc, dynamic_acc, float16_acc, qat_quantized_acc]\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "bars = plt.bar(methods, accuracies, alpha=0.8)\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Size comparison\n",
    "sizes = [original_size, dynamic_size, float16_size, len(qat_quantized_model)/1024]\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "bars = plt.bar(methods, sizes, alpha=0.8)\n",
    "plt.title('Model Size Comparison (KB)')\n",
    "plt.ylabel('Size (KB)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, size in zip(bars, sizes):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "             f'{size:.0f}', ha='center', va='bottom')\n",
    "\n",
    "# Training loss comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(baseline_history.history['loss'], label='Baseline Training', marker='o')\n",
    "plt.plot(qat_history.history['loss'], label='QAT Training', marker='s')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy vs Size trade-off\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(sizes, accuracies, s=100, alpha=0.8)\n",
    "for i, method in enumerate(methods):\n",
    "    plt.annotate(method, (sizes[i], accuracies[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "plt.title('Accuracy vs Size Trade-off')\n",
    "plt.xlabel('Model Size (KB)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## 3. Pruning Techniques\n",
    "\n",
    "```python\n",
    "# Model Pruning Implementation\n",
    "print(\"=== Model Pruning Techniques ===\")\n",
    "\n",
    "# Magnitude-based pruning\n",
    "def create_pruned_model(baseline_model, sparsity=0.5):\n",
    "    \"\"\"Create magnitude-based pruned model\"\"\"\n",
    "    \n",
    "    # Define pruning parameters\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "            initial_sparsity=0.0,\n",
    "            final_sparsity=sparsity,\n",
    "            begin_step=0,\n",
    "            end_step=1000\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Apply pruning to model\n",
    "    pruned_model = tfmot.sparsity.keras.prune_low_magnitude(\n",
    "        baseline_model, **pruning_params\n",
    "    )\n",
    "    \n",
    "    return pruned_model\n",
    "\n",
    "# Structured pruning\n",
    "def create_structured_pruned_model(baseline_model):\n",
    "    \"\"\"Create structured pruned model\"\"\"\n",
    "    \n",
    "    # Define structured pruning\n",
    "    def structured_pruning_fn(layer):\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            # Prune 25% of filters\n",
    "            return tfmot.sparsity.keras.prune_low_magnitude(\n",
    "                layer,\n",
    "                pruning_schedule=tfmot.sparsity.keras.ConstantSparsity(0.25, 0)\n",
    "            )\n",
    "        return layer\n",
    "    \n",
    "    # Apply structured pruning\n",
    "    structured_pruned_model = tf.keras.models.clone_model(\n",
    "        baseline_model,\n",
    "        clone_function=structured_pruning_fn\n",
    "    )\n",
    "    \n",
    "    return structured_pruned_model\n",
    "\n",
    "# Create pruned models\n",
    "print(\"Creating pruned models...\")\n",
    "\n",
    "# Unstructured pruning at different sparsity levels\n",
    "sparse_30_model = create_pruned_model(baseline_model, sparsity=0.3)\n",
    "sparse_50_model = create_pruned_model(baseline_model, sparsity=0.5)\n",
    "sparse_80_model = create_pruned_model(baseline_model, sparsity=0.8)\n",
    "\n",
    "# Compile pruned models\n",
    "for model, name in [(sparse_30_model, '30%'), (sparse_50_model, '50%'), (sparse_80_model, '80%')]:\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train pruned models (fine-tuning)\n",
    "print(\"Fine-tuning pruned models...\")\n",
    "\n",
    "pruned_histories = {}\n",
    "pruned_accuracies = {}\n",
    "\n",
    "for model, sparsity in [(sparse_30_model, '30%'), (sparse_50_model, '50%'), (sparse_80_model, '80%')]:\n",
    "    print(f\"Training {sparsity} sparse model...\")\n",
    "    \n",
    "    # Add pruning callbacks\n",
    "    callbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\n",
    "    \n",
    "    history = model.fit(\n",
    "        x_train[:2000], y_train[:2000],\n",
    "        batch_size=64, epochs=3,\n",
    "        validation_data=(x_test[:500], y_test[:500]),\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    pruned_histories[sparsity] = history\n",
    "    \n",
    "    # Evaluate\n",
    "    _, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    pruned_accuracies[sparsity] = acc\n",
    "    \n",
    "    print(f\"{sparsity} sparse model accuracy: {acc:.4f}\")\n",
    "\n",
    "# Strip pruning and export\n",
    "print(\"\\nExporting pruned models...\")\n",
    "\n",
    "pruned_model_sizes = {}\n",
    "for model, sparsity in [(sparse_50_model, '50%')]:  # Focus on 50% for demo\n",
    "    # Strip pruning wrappers\n",
    "    stripped_model = tfmot.sparsity.keras.strip_pruning(model)\n",
    "    \n",
    "    # Convert to TFLite\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(stripped_model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    pruned_tflite = converter.convert()\n",
    "    \n",
    "    pruned_model_sizes[sparsity] = len(pruned_tflite) / 1024\n",
    "    \n",
    "    # Evaluate pruned TFLite\n",
    "    pruned_tflite_acc = evaluate_tflite_model(pruned_tflite, x_test, y_test)\n",
    "    print(f\"Pruned {sparsity} TFLite accuracy: {pruned_tflite_acc:.4f}\")\n",
    "    print(f\"Pruned {sparsity} TFLite size: {pruned_model_sizes[sparsity]:.1f} KB\")\n",
    "\n",
    "# Visualize pruning results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Sparsity vs Accuracy\n",
    "sparsities = ['0%', '30%', '50%', '80%']\n",
    "accuracies = [baseline_test_acc] + [pruned_accuracies[s] for s in ['30%', '50%', '80%']]\n",
    "\n",
    "axes[0, 0].plot(sparsities, accuracies, marker='o', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_title('Pruning Sparsity vs Accuracy')\n",
    "axes[0, 0].set_xlabel('Sparsity Level')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training curves for pruned models\n",
    "axes[0, 1].set_title('Pruned Model Training Curves')\n",
    "for sparsity, history in pruned_histories.items():\n",
    "    axes[0, 1].plot(history.history['loss'], label=f'{sparsity} Sparse', marker='o')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Weight distribution visualization\n",
    "def plot_weight_distribution(model, ax, title):\n",
    "    \"\"\"Plot weight distribution of model\"\"\"\n",
    "    \n",
    "    weights = []\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel'):\n",
    "            layer_weights = layer.get_weights()[0].flatten()\n",
    "            weights.extend(layer_weights)\n",
    "    \n",
    "    weights = np.array(weights)\n",
    "    ax.hist(weights, bins=50, alpha=0.7, density=True)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Weight Value')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add sparsity info\n",
    "    sparsity = np.mean(np.abs(weights) < 1e-6) * 100\n",
    "    ax.text(0.7, 0.8, f'Sparsity: {sparsity:.1f}%', \n",
    "            transform=ax.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# Plot weight distributions\n",
    "plot_weight_distribution(baseline_model, axes[1, 0], 'Original Model Weights')\n",
    "stripped_sparse_model = tfmot.sparsity.keras.strip_pruning(sparse_50_model)\n",
    "plot_weight_distribution(stripped_sparse_model, axes[1, 1], '50% Pruned Model Weights')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze actual sparsity achieved\n",
    "def analyze_sparsity(model, model_name):\n",
    "    \"\"\"Analyze actual sparsity in model\"\"\"\n",
    "    \n",
    "    total_weights = 0\n",
    "    zero_weights = 0\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel'):\n",
    "            weights = layer.get_weights()[0]\n",
    "            total_weights += weights.size\n",
    "            zero_weights += np.sum(np.abs(weights) < 1e-6)\n",
    "    \n",
    "    actual_sparsity = zero_weights / total_weights * 100\n",
    "    print(f\"{model_name} - Actual sparsity: {actual_sparsity:.1f}%\")\n",
    "    \n",
    "    return actual_sparsity\n",
    "\n",
    "print(\"\\nActual sparsity analysis:\")\n",
    "analyze_sparsity(baseline_model, \"Baseline\")\n",
    "for model, sparsity in [(tfmot.sparsity.keras.strip_pruning(sparse_30_model), '30% Target'),\n",
    "                        (tfmot.sparsity.keras.strip_pruning(sparse_50_model), '50% Target'),\n",
    "                        (tfmot.sparsity.keras.strip_pruning(sparse_80_model), '80% Target')]:\n",
    "    analyze_sparsity(model, sparsity)\n",
    "```\n",
    "\n",
    "## 4. Clustering and Knowledge Distillation\n",
    "\n",
    "```python\n",
    "# Weight Clustering\n",
    "print(\"=== Weight Clustering ===\")\n",
    "\n",
    "def create_clustered_model(model, num_clusters=16):\n",
    "    \"\"\"Apply weight clustering to model\"\"\"\n",
    "    \n",
    "    clustering_params = {\n",
    "        'number_of_clusters': num_clusters,\n",
    "        'cluster_centroids_init': tfmot.clustering.keras.CentroidInitialization.LINEAR\n",
    "    }\n",
    "    \n",
    "    clustered_model = tfmot.clustering.keras.cluster_weights(\n",
    "        model, **clustering_params\n",
    "    )\n",
    "    \n",
    "    return clustered_model\n",
    "\n",
    "# Create clustered models with different cluster counts\n",
    "cluster_8_model = create_clustered_model(baseline_model, num_clusters=8)\n",
    "cluster_16_model = create_clustered_model(baseline_model, num_clusters=16)\n",
    "cluster_32_model = create_clustered_model(baseline_model, num_clusters=32)\n",
    "\n",
    "# Compile clustered models\n",
    "for model in [cluster_8_model, cluster_16_model, cluster_32_model]:\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train clustered models\n",
    "print(\"Training clustered models...\")\n",
    "\n",
    "cluster_results = {}\n",
    "for model, clusters in [(cluster_16_model, 16)]:  # Focus on 16 clusters for demo\n",
    "    # Train clustered model\n",
    "    callbacks = [tfmot.clustering.keras.UpdateClustering()]\n",
    "    \n",
    "    history = model.fit(\n",
    "        x_train[:2000], y_train[:2000],\n",
    "        batch_size=64, epochs=3,\n",
    "        validation_data=(x_test[:500], y_test[:500]),\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    _, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    cluster_results[clusters] = acc\n",
    "    \n",
    "    print(f\"{clusters} clusters accuracy: {acc:.4f}\")\n",
    "\n",
    "# Strip clustering and export\n",
    "stripped_cluster_model = tfmot.clustering.keras.strip_clustering(cluster_16_model)\n",
    "\n",
    "# Knowledge Distillation\n",
    "print(\"\\n=== Knowledge Distillation ===\")\n",
    "\n",
    "class DistillationLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Custom loss for knowledge distillation\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.1, temperature=3.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        teacher_pred, student_pred = y_pred[0], y_pred[1]\n",
    "        \n",
    "        # Student loss on true labels\n",
    "        student_loss = tf.keras.losses.categorical_crossentropy(y_true, student_pred)\n",
    "        \n",
    "        # Distillation loss\n",
    "        teacher_prob = tf.nn.softmax(teacher_pred / self.temperature)\n",
    "        student_prob = tf.nn.softmax(student_pred / self.temperature)\n",
    "        \n",
    "        distillation_loss = tf.keras.losses.categorical_crossentropy(\n",
    "            teacher_prob, student_prob\n",
    "        ) * (self.temperature ** 2)\n",
    "        \n",
    "        # Combined loss\n",
    "        return self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "class DistillationModel(tf.keras.Model):\n",
    "    \"\"\"Distillation training model\"\"\"\n",
    "    \n",
    "    def __init__(self, teacher_model, student_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.student = student_model\n",
    "        \n",
    "        # Freeze teacher\n",
    "        self.teacher.trainable = False\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        teacher_pred = self.teacher(inputs, training=False)\n",
    "        student_pred = self.student(inputs, training=training)\n",
    "        \n",
    "        return [teacher_pred, student_pred]\n",
    "\n",
    "# Create student model (smaller)\n",
    "def create_student_model():\n",
    "    \"\"\"Create smaller student model\"\"\"\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv2D(16, 3, activation='relu', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(32, 3, activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ], name='student_model')\n",
    "    \n",
    "    return model\n",
    "\n",
    "student_model = create_student_model()\n",
    "print(f\"Teacher parameters: {baseline_model.count_params():,}\")\n",
    "print(f\"Student parameters: {student_model.count_params():,}\")\n",
    "print(f\"Compression ratio: {baseline_model.count_params() / student_model.count_params():.1f}x\")\n",
    "\n",
    "# Create distillation model\n",
    "distillation_model = DistillationModel(baseline_model, student_model)\n",
    "distillation_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=DistillationLoss(alpha=0.1, temperature=3.0)\n",
    ")\n",
    "\n",
    "# Train student with distillation\n",
    "print(\"Training student with knowledge distillation...\")\n",
    "distillation_history = distillation_model.fit(\n",
    "    x_train[:3000], y_train[:3000],\n",
    "    batch_size=128, epochs=5,\n",
    "    validation_data=(x_test[:1000], y_test[:1000]),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate student model\n",
    "student_test_loss, student_test_acc = student_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Distilled student accuracy: {student_test_acc:.4f}\")\n",
    "\n",
    "# Train student without distillation for comparison\n",
    "student_baseline = create_student_model()\n",
    "student_baseline.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "student_baseline_history = student_baseline.fit(\n",
    "    x_train[:3000], y_train[:3000],\n",
    "    batch_size=128, epochs=5,\n",
    "    validation_data=(x_test[:1000], y_test[:1000]),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "student_baseline_acc = student_baseline.evaluate(x_test, y_test, verbose=0)[1]\n",
    "print(f\"Student without distillation accuracy: {student_baseline_acc:.4f}\")\n",
    "print(f\"Improvement from distillation: {student_test_acc - student_baseline_acc:.4f}\")\n",
    "\n",
    "# Comprehensive comparison\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Model comparison overview\n",
    "methods = ['Original\\nTeacher', 'Quantized\\n(Float16)', 'Pruned\\n(50%)', \n",
    "           'Clustered\\n(16)', 'Student\\n(No Distill)', 'Student\\n(Distilled)']\n",
    "\n",
    "accuracies = [baseline_test_acc, float16_acc, pruned_accuracies['50%'], \n",
    "              cluster_results[16], student_baseline_acc, student_test_acc]\n",
    "\n",
    "sizes = [original_size, float16_size, pruned_model_sizes['50%'], \n",
    "         original_size * 0.8,  # Approximate cluster size\n",
    "         len(tf.keras.models.model_to_json(student_model).encode('utf-8')) / 1024,\n",
    "         len(tf.keras.models.model_to_json(student_model).encode('utf-8')) / 1024]\n",
    "\n",
    "parameters = [baseline_model.count_params(), baseline_model.count_params(), \n",
    "              baseline_model.count_params(), baseline_model.count_params(),\n",
    "              student_model.count_params(), student_model.count_params()]\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "bars = plt.bar(methods, accuracies, alpha=0.8, color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum', 'orange'])\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Size comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "bars = plt.bar(methods, sizes, alpha=0.8, color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum', 'orange'])\n",
    "plt.title('Model Size (KB)')\n",
    "plt.ylabel('Size (KB)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter count comparison\n",
    "plt.subplot(2, 3, 3)\n",
    "bars = plt.bar(methods, parameters, alpha=0.8, color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum', 'orange'])\n",
    "plt.title('Parameter Count')\n",
    "plt.ylabel('Parameters')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency scatter plot\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(parameters, accuracies, s=100, alpha=0.8, \n",
    "            c=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum', 'orange'])\n",
    "for i, method in enumerate(methods):\n",
    "    plt.annotate(method.replace('\\n', ' '), (parameters[i], accuracies[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "plt.title('Efficiency: Parameters vs Accuracy')\n",
    "plt.xlabel('Parameters')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Training comparison for distillation\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(student_baseline_history.history['val_accuracy'], \n",
    "         label='Student (No Distillation)', marker='o')\n",
    "plt.plot(distillation_history.history['val_accuracy'], \n",
    "         label='Student (With Distillation)', marker='s')\n",
    "plt.title('Distillation Training Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Compression summary\n",
    "plt.subplot(2, 3, 6)\n",
    "compression_ratios = [1.0, original_size/float16_size, 1.2, 1.3, \n",
    "                     baseline_model.count_params()/student_model.count_params(),\n",
    "                     baseline_model.count_params()/student_model.count_params()]\n",
    "\n",
    "bars = plt.bar(methods, compression_ratios, alpha=0.8, \n",
    "               color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum', 'orange'])\n",
    "plt.title('Compression Ratios')\n",
    "plt.ylabel('Compression Factor')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, ratio in zip(bars, compression_ratios):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, \n",
    "             f'{ratio:.1f}x', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final optimization summary\n",
    "print(\"\\n=== Optimization Summary ===\")\n",
    "print(\"Method                 | Accuracy | Size (KB) | Parameters | Compression\")\n",
    "print(\"-\" * 75)\n",
    "for i, method in enumerate(methods):\n",
    "    method_clean = method.replace('\\n', ' ')\n",
    "    print(f\"{method_clean:20} | {accuracies[i]:8.4f} | {sizes[i]:8.1f} | {parameters[i]:10,} | {compression_ratios[i]:8.1f}x\")\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "This comprehensive notebook demonstrated advanced TensorFlow Model Optimization techniques with tf.keras integration:\n",
    "\n",
    "### Key Implementations\n",
    "\n",
    "**1. Post-Training Quantization:**\n",
    "- Dynamic range quantization (weights only)\n",
    "- Float16 quantization for balanced compression\n",
    "- Full integer quantization for maximum compression\n",
    "- Automatic TFLite conversion and evaluation\n",
    "\n",
    "**2. Quantization-Aware Training (QAT):**\n",
    "- Fake quantization during training\n",
    "- Better accuracy preservation than post-training methods\n",
    "- Seamless tf.keras integration with compile/fit workflow\n",
    "- Production-ready quantized model export\n",
    "\n",
    "**3. Pruning Techniques:**\n",
    "- Magnitude-based unstructured pruning\n",
    "- Structured pruning for hardware efficiency  \n",
    "- Progressive sparsity schedules\n",
    "- Weight distribution analysis and visualization\n",
    "\n",
    "**4. Advanced Optimization:**\n",
    "- Weight clustering for compression\n",
    "- Knowledge distillation for model compression\n",
    "- Multi-objective optimization combining techniques\n",
    "- Comprehensive performance evaluation\n",
    "\n",
    "### Technical Achievements\n",
    "\n",
    "- **Significant Compression**: 2-10x size reduction with minimal accuracy loss\n",
    "- **Hardware Optimization**: INT8 and pruned models for mobile/edge deployment\n",
    "- **Training Integration**: All techniques work seamlessly with tf.keras training loops\n",
    "- **Production Ready**: Automated conversion to TFLite for deployment\n",
    "\n",
    "### Performance Results\n",
    "\n",
    "- **Quantization**: 2-4x compression with <2% accuracy loss\n",
    "- **Pruning**: Up to 80% sparsity with manageable accuracy degradation\n",
    "- **Distillation**: 5-10x parameter reduction with knowledge transfer\n",
    "- **Combined**: Multiple techniques can be stacked for greater compression\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "- Mobile and edge device deployment\n",
    "- Real-time inference optimization  \n",
    "- Memory-constrained environments\n",
    "- Energy-efficient computing\n",
    "- Large-scale model serving\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue to notebook 17 (TFLite Conversion and Mobile Deployment) to learn how to deploy these optimized models on mobile and edge devices, completing the production pipeline from training to deployment.\n",
    "\n",
    "The optimization techniques demonstrated here are essential for making modern deep learning models practical for real-world deployment scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
