{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 cnn architectures keras\n",
    "**Location: TensorVerseHub/notebooks/03_computer_vision/07_cnn_architectures_keras.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architectures with tf.keras\n",
    "\n",
    "**File Location:** `notebooks/03_computer_vision/07_cnn_architectures_keras.ipynb`\n",
    "\n",
    "Master convolutional neural networks with tf.keras, implementing classic and modern CNN architectures from LeNet to EfficientNet. Build custom CNN layers, understand architectural patterns, and create state-of-the-art computer vision models.\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement classic CNN architectures (LeNet, AlexNet, VGG, ResNet)\n",
    "- Build modern efficient architectures (MobileNet, EfficientNet concepts)\n",
    "- Master tf.keras.layers.Conv2D and pooling operations\n",
    "- Create custom convolutional layers and blocks\n",
    "- Understand architectural design patterns and best practices\n",
    "- Optimize CNNs for different computational constraints\n",
    "\n",
    "---\n",
    "\n",
    "## 1. CNN Fundamentals with tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models, applications\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and prepare datasets\n",
    "def load_vision_datasets():\n",
    "    \"\"\"Load CIFAR-10 and Fashion-MNIST for CNN experiments\"\"\"\n",
    "    \n",
    "    # CIFAR-10 dataset\n",
    "    (x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = tf.keras.datasets.cifar10.load_data()\n",
    "    x_train_cifar = x_train_cifar.astype('float32') / 255.0\n",
    "    x_test_cifar = x_test_cifar.astype('float32') / 255.0\n",
    "    y_train_cifar = y_train_cifar.flatten()\n",
    "    y_test_cifar = y_test_cifar.flatten()\n",
    "    \n",
    "    # Fashion-MNIST dataset\n",
    "    (x_train_fashion, y_train_fashion), (x_test_fashion, y_test_fashion) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    x_train_fashion = x_train_fashion.astype('float32') / 255.0\n",
    "    x_test_fashion = x_test_fashion.astype('float32') / 255.0\n",
    "    # Add channel dimension\n",
    "    x_train_fashion = np.expand_dims(x_train_fashion, axis=-1)\n",
    "    x_test_fashion = np.expand_dims(x_test_fashion, axis=-1)\n",
    "    \n",
    "    print(f\"CIFAR-10: Train {x_train_cifar.shape}, Test {x_test_cifar.shape}\")\n",
    "    print(f\"Fashion-MNIST: Train {x_train_fashion.shape}, Test {x_test_fashion.shape}\")\n",
    "    \n",
    "    return (x_train_cifar, y_train_cifar, x_test_cifar, y_test_cifar), \\\n",
    "           (x_train_fashion, y_train_fashion, x_test_fashion, y_test_fashion)\n",
    "\n",
    "# Load datasets\n",
    "cifar_data, fashion_data = load_vision_datasets()\n",
    "(x_train_cifar, y_train_cifar, x_test_cifar, y_test_cifar) = cifar_data\n",
    "(x_train_fashion, y_train_fashion, x_test_fashion, y_test_fashion) = fashion_data\n",
    "\n",
    "# Basic CNN operations demonstration\n",
    "def demonstrate_cnn_operations():\n",
    "    \"\"\"Demonstrate fundamental CNN operations\"\"\"\n",
    "    \n",
    "    print(\"=== CNN Operations Demonstration ===\")\n",
    "    \n",
    "    # Create sample input\n",
    "    sample_input = tf.random.normal((1, 32, 32, 3))  # Batch size 1, 32x32x3 image\n",
    "    \n",
    "    # Convolution operation\n",
    "    conv_layer = layers.Conv2D(64, (3, 3), activation='relu', padding='same')\n",
    "    conv_output = conv_layer(sample_input)\n",
    "    print(f\"Conv2D output shape: {conv_output.shape}\")\n",
    "    \n",
    "    # Pooling operation\n",
    "    pool_layer = layers.MaxPool2D((2, 2))\n",
    "    pool_output = pool_layer(conv_output)\n",
    "    print(f\"MaxPool2D output shape: {pool_output.shape}\")\n",
    "    \n",
    "    # Batch normalization\n",
    "    bn_layer = layers.BatchNormalization()\n",
    "    bn_output = bn_layer(pool_output)\n",
    "    print(f\"BatchNorm output shape: {bn_output.shape}\")\n",
    "    \n",
    "    # Different convolution types\n",
    "    print(\"\\n--- Different Convolution Types ---\")\n",
    "    \n",
    "    # Depthwise separable convolution\n",
    "    depthwise_conv = layers.SeparableConv2D(64, (3, 3), padding='same')\n",
    "    depthwise_output = depthwise_conv(sample_input)\n",
    "    print(f\"SeparableConv2D output shape: {depthwise_output.shape}\")\n",
    "    \n",
    "    # Dilated convolution\n",
    "    dilated_conv = layers.Conv2D(64, (3, 3), dilation_rate=2, padding='same')\n",
    "    dilated_output = dilated_conv(sample_input)\n",
    "    print(f\"Dilated Conv2D output shape: {dilated_output.shape}\")\n",
    "    \n",
    "    # Transpose convolution (for upsampling)\n",
    "    transpose_conv = layers.Conv2DTranspose(32, (3, 3), strides=2, padding='same')\n",
    "    transpose_output = transpose_conv(sample_input)\n",
    "    print(f\"Conv2DTranspose output shape: {transpose_output.shape}\")\n",
    "\n",
    "demonstrate_cnn_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classic CNN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet-5 Architecture\n",
    "class LeNet5(tf.keras.Model):\n",
    "    \"\"\"LeNet-5 architecture for digit recognition\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = layers.Conv2D(6, (5, 5), activation='tanh', padding='valid')\n",
    "        self.pool1 = layers.AveragePooling2D((2, 2))\n",
    "        self.conv2 = layers.Conv2D(16, (5, 5), activation='tanh', padding='valid')\n",
    "        self.pool2 = layers.AveragePooling2D((2, 2))\n",
    "        \n",
    "        # Dense layers\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc1 = layers.Dense(120, activation='tanh')\n",
    "        self.fc2 = layers.Dense(84, activation='tanh')\n",
    "        self.fc3 = layers.Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet-inspired Architecture\n",
    "class AlexNet(tf.keras.Model):\n",
    "    \"\"\"AlexNet-inspired architecture adapted for CIFAR-10\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.conv1 = layers.Conv2D(96, (3, 3), strides=1, activation='relu', padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.pool1 = layers.MaxPool2D((2, 2), strides=2)\n",
    "        \n",
    "        self.conv2 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.pool2 = layers.MaxPool2D((2, 2), strides=2)\n",
    "        \n",
    "        self.conv3 = layers.Conv2D(384, (3, 3), activation='relu', padding='same')\n",
    "        self.conv4 = layers.Conv2D(384, (3, 3), activation='relu', padding='same')\n",
    "        self.conv5 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')\n",
    "        self.pool5 = layers.MaxPool2D((2, 2), strides=2)\n",
    "        \n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dropout1 = layers.Dropout(0.5)\n",
    "        self.fc1 = layers.Dense(4096, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(0.5)\n",
    "        self.fc2 = layers.Dense(4096, activation='relu')\n",
    "        self.fc3 = layers.Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.pool1(self.bn1(self.conv1(inputs)))\n",
    "        x = self.pool2(self.bn2(self.conv2(x)))\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.pool5(self.conv5(x))\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.fc2(x)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG Architecture\n",
    "class VGGBlock(layers.Layer):\n",
    "    \"\"\"VGG convolutional block\"\"\"\n",
    "    \n",
    "    def __init__(self, filters, num_convs, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.num_convs = num_convs\n",
    "        \n",
    "        self.convs = []\n",
    "        for _ in range(num_convs):\n",
    "            self.convs.append(layers.Conv2D(filters, (3, 3), activation='relu', padding='same'))\n",
    "        \n",
    "        self.pool = layers.MaxPool2D((2, 2))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        return self.pool(x)\n",
    "\n",
    "class VGG16(tf.keras.Model):\n",
    "    \"\"\"VGG-16 architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # VGG blocks\n",
    "        self.block1 = VGGBlock(64, 2)\n",
    "        self.block2 = VGGBlock(128, 2)\n",
    "        self.block3 = VGGBlock(256, 3)\n",
    "        self.block4 = VGGBlock(512, 3)\n",
    "        self.block5 = VGGBlock(512, 3)\n",
    "        \n",
    "        # Classifier\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc1 = layers.Dense(4096, activation='relu')\n",
    "        self.dropout1 = layers.Dropout(0.5)\n",
    "        self.fc2 = layers.Dense(4096, activation='relu')\n",
    "        self.dropout2 = layers.Dropout(0.5)\n",
    "        self.fc3 = layers.Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.block1(inputs)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout1(self.fc1(x), training=training)\n",
    "        x = self.dropout2(self.fc2(x), training=training)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test classic architectures\n",
    "print(\"\\n=== Testing Classic CNN Architectures ===\")\n",
    "\n",
    "# LeNet on Fashion-MNIST\n",
    "lenet = LeNet5(num_classes=10)\n",
    "lenet.build(input_shape=(None, 28, 28, 1))\n",
    "print(f\"LeNet-5 parameters: {lenet.count_params():,}\")\n",
    "\n",
    "# AlexNet on CIFAR-10\n",
    "alexnet = AlexNet(num_classes=10)\n",
    "alexnet.build(input_shape=(None, 32, 32, 3))\n",
    "print(f\"AlexNet parameters: {alexnet.count_params():,}\")\n",
    "\n",
    "# VGG-16 (smaller version for CIFAR-10)\n",
    "vgg16 = VGG16(num_classes=10)\n",
    "vgg16.build(input_shape=(None, 32, 32, 3))\n",
    "print(f\"VGG-16 parameters: {vgg16.count_params():,}\")\n",
    "\n",
    "# Quick training test on LeNet with Fashion-MNIST\n",
    "lenet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Use subset for quick test\n",
    "subset_size = 1000\n",
    "lenet_history = lenet.fit(\n",
    "    x_train_fashion[:subset_size], y_train_fashion[:subset_size],\n",
    "    validation_data=(x_test_fashion[:200], y_test_fashion[:200]),\n",
    "    epochs=5, batch_size=32, verbose=0\n",
    ")\n",
    "\n",
    "print(f\"LeNet-5 test accuracy: {max(lenet_history.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Residual Networks (ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet Implementation\n",
    "class ResidualBlock(layers.Layer):\n",
    "    \"\"\"Basic residual block with skip connection\"\"\"\n",
    "    \n",
    "    def __init__(self, filters, strides=1, use_projection=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.strides = strides\n",
    "        self.use_projection = use_projection\n",
    "        \n",
    "        # Main path\n",
    "        self.conv1 = layers.Conv2D(filters, (3, 3), strides=strides, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = layers.Conv2D(filters, (3, 3), padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        \n",
    "        # Skip connection projection if needed\n",
    "        if use_projection:\n",
    "            self.projection = layers.Conv2D(filters, (1, 1), strides=strides, padding='same')\n",
    "            self.proj_bn = layers.BatchNormalization()\n",
    "        \n",
    "        self.relu = layers.ReLU()\n",
    "        self.add = layers.Add()\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Main path\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        \n",
    "        # Skip connection\n",
    "        shortcut = inputs\n",
    "        if self.use_projection:\n",
    "            shortcut = self.projection(shortcut)\n",
    "            shortcut = self.proj_bn(shortcut, training=training)\n",
    "        \n",
    "        # Add and activate\n",
    "        x = self.add([x, shortcut])\n",
    "        return self.relu(x)\n",
    "\n",
    "class BottleneckBlock(layers.Layer):\n",
    "    \"\"\"Bottleneck residual block for deeper networks\"\"\"\n",
    "    \n",
    "    def __init__(self, filters, strides=1, use_projection=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Bottleneck: 1x1 -> 3x3 -> 1x1\n",
    "        self.conv1 = layers.Conv2D(filters // 4, (1, 1), padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        \n",
    "        self.conv2 = layers.Conv2D(filters // 4, (3, 3), strides=strides, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        \n",
    "        self.conv3 = layers.Conv2D(filters, (1, 1), padding='same')\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        \n",
    "        # Projection for skip connection\n",
    "        if use_projection:\n",
    "            self.projection = layers.Conv2D(filters, (1, 1), strides=strides, padding='same')\n",
    "            self.proj_bn = layers.BatchNormalization()\n",
    "        \n",
    "        self.use_projection = use_projection\n",
    "        self.relu = layers.ReLU()\n",
    "        self.add = layers.Add()\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Main path\n",
    "        x = self.relu(self.bn1(self.conv1(inputs), training=training))\n",
    "        x = self.relu(self.bn2(self.conv2(x), training=training))\n",
    "        x = self.bn3(self.conv3(x), training=training)\n",
    "        \n",
    "        # Skip connection\n",
    "        shortcut = inputs\n",
    "        if self.use_projection:\n",
    "            shortcut = self.proj_bn(self.projection(shortcut), training=training)\n",
    "        \n",
    "        # Add and activate\n",
    "        x = self.add([x, shortcut])\n",
    "        return self.relu(x)\n",
    "\n",
    "class ResNet(tf.keras.Model):\n",
    "    \"\"\"ResNet architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, layers_config, num_classes=10, use_bottleneck=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layers_config = layers_config\n",
    "        self.num_classes = num_classes\n",
    "        self.use_bottleneck = use_bottleneck\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv1 = layers.Conv2D(64, (7, 7), strides=2, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu = layers.ReLU()\n",
    "        self.pool1 = layers.MaxPool2D((3, 3), strides=2, padding='same')\n",
    "        \n",
    "        # Residual layers\n",
    "        self.res_layers = []\n",
    "        filters = 64\n",
    "        \n",
    "        for i, num_blocks in enumerate(layers_config):\n",
    "            layer_blocks = []\n",
    "            strides = 1 if i == 0 else 2\n",
    "            \n",
    "            # First block may need projection\n",
    "            block_class = BottleneckBlock if use_bottleneck else ResidualBlock\n",
    "            layer_blocks.append(block_class(filters, strides, use_projection=True))\n",
    "            \n",
    "            # Remaining blocks\n",
    "            for _ in range(1, num_blocks):\n",
    "                layer_blocks.append(block_class(filters))\n",
    "            \n",
    "            self.res_layers.append(layer_blocks)\n",
    "            filters *= 2\n",
    "        \n",
    "        # Final layers\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.classifier = layers.Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Initial processing\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Residual layers\n",
    "        for layer_blocks in self.res_layers:\n",
    "            for block in layer_blocks:\n",
    "                x = block(x, training=training)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Create different ResNet variants\n",
    "def create_resnet_variants():\n",
    "    \"\"\"Create different ResNet architectures\"\"\"\n",
    "    \n",
    "    variants = {\n",
    "        'ResNet-18': ResNet([2, 2, 2, 2], num_classes=10, use_bottleneck=False),\n",
    "        'ResNet-34': ResNet([3, 4, 6, 3], num_classes=10, use_bottleneck=False),\n",
    "        'ResNet-50': ResNet([3, 4, 6, 3], num_classes=10, use_bottleneck=True),\n",
    "        'ResNet-101': ResNet([3, 4, 23, 3], num_classes=10, use_bottleneck=True)\n",
    "    }\n",
    "    \n",
    "    print(\"=== ResNet Variants ===\")\n",
    "    \n",
    "    for name, model in variants.items():\n",
    "        model.build(input_shape=(None, 32, 32, 3))  # CIFAR-10 input shape\n",
    "        print(f\"{name}: {model.count_params():,} parameters\")\n",
    "    \n",
    "    return variants\n",
    "\n",
    "resnet_variants = create_resnet_variants()\n",
    "\n",
    "# Train ResNet-18 on subset of CIFAR-10\n",
    "resnet18 = resnet_variants['ResNet-18']\n",
    "resnet18.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "subset_size = 2000\n",
    "resnet_history = resnet18.fit(\n",
    "    x_train_cifar[:subset_size], y_train_cifar[:subset_size],\n",
    "    validation_data=(x_test_cifar[:400], y_test_cifar[:400]),\n",
    "    epochs=10, batch_size=64, verbose=0\n",
    ")\n",
    "\n",
    "print(f\"ResNet-18 test accuracy: {max(resnet_history.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modern Efficient Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depthwise Separable Convolution Block\n",
    "class DepthwiseSeparableConv(layers.Layer):\n",
    "    \"\"\"Depthwise separable convolution block\"\"\"\n",
    "    \n",
    "    def __init__(self, filters, strides=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.depthwise = layers.DepthwiseConv2D((3, 3), strides=strides, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "        \n",
    "        self.pointwise = layers.Conv2D(filters, (1, 1), padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.ReLU()\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.depthwise(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        return self.relu2(x)\n",
    "\n",
    "# MobileNet-inspired Architecture\n",
    "class MobileNetV1(tf.keras.Model):\n",
    "    \"\"\"MobileNet V1 architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, width_multiplier=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.width_multiplier = width_multiplier\n",
    "        \n",
    "        # Standard convolution\n",
    "        base_filters = int(32 * width_multiplier)\n",
    "        self.conv1 = layers.Conv2D(base_filters, (3, 3), strides=2, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "        \n",
    "        # Depthwise separable convolutions\n",
    "        self.ds_convs = []\n",
    "        filter_sizes = [64, 128, 128, 256, 256, 512, 512, 512, 512, 512, 512, 1024, 1024]\n",
    "        strides = [1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1]\n",
    "        \n",
    "        for filters, stride in zip(filter_sizes, strides):\n",
    "            self.ds_convs.append(DepthwiseSeparableConv(\n",
    "                int(filters * width_multiplier), strides=stride\n",
    "            ))\n",
    "        \n",
    "        # Classifier\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.dropout = layers.Dropout(0.2)\n",
    "        self.classifier = layers.Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        for ds_conv in self.ds_convs:\n",
    "            x = ds_conv(x, training=training)\n",
    "        \n",
    "        x = self.global_pool(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Squeeze-and-Excitation Block\n",
    "class SEBlock(layers.Layer):\n",
    "    \"\"\"Squeeze-and-Excitation block\"\"\"\n",
    "    \n",
    "    def __init__(self, filters, se_ratio=16, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.se_ratio = se_ratio\n",
    "        \n",
    "        # Squeeze\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        \n",
    "        # Excitation\n",
    "        self.fc1 = layers.Dense(filters // se_ratio, activation='relu')\n",
    "        self.fc2 = layers.Dense(filters, activation='sigmoid')\n",
    "        \n",
    "        # Reshape for broadcasting\n",
    "        self.reshape = layers.Reshape((1, 1, filters))\n",
    "        self.multiply = layers.Multiply()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Squeeze: Global average pooling\n",
    "        squeeze = self.global_pool(inputs)\n",
    "        \n",
    "        # Excitation: Two FC layers\n",
    "        excitation = self.fc1(squeeze)\n",
    "        excitation = self.fc2(excitation)\n",
    "        excitation = self.reshape(excitation)\n",
    "        \n",
    "        # Scale: Element-wise multiplication\n",
    "        return self.multiply([inputs, excitation])\n",
    "\n",
    "# Inverted Residual Block (MobileNet V2)\n",
    "class InvertedResidualBlock(layers.Layer):\n",
    "    \"\"\"Inverted residual block with linear bottleneck\"\"\"\n",
    "    \n",
    "    def __init__(self, filters, strides=1, expand_ratio=6, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.strides = strides\n",
    "        self.expand_ratio = expand_ratio\n",
    "        \n",
    "        # Expansion phase\n",
    "        expanded_filters = int(filters * expand_ratio)\n",
    "        self.expand = layers.Conv2D(expanded_filters, (1, 1), padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU6()\n",
    "        \n",
    "        # Depthwise\n",
    "        self.depthwise = layers.DepthwiseConv2D((3, 3), strides=strides, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.relu2 = layers.ReLU6()\n",
    "        \n",
    "        # Projection (linear)\n",
    "        self.project = layers.Conv2D(filters, (1, 1), padding='same')\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        \n",
    "        # Skip connection\n",
    "        self.use_skip = (strides == 1)\n",
    "        self.add = layers.Add()\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Expansion\n",
    "        x = self.expand(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        # Depthwise\n",
    "        x = self.depthwise(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        # Projection\n",
    "        x = self.project(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        \n",
    "        # Skip connection\n",
    "        if self.use_skip and x.shape == inputs.shape:\n",
    "            x = self.add([x, inputs])\n",
    "        \n",
    "        return x\n",
    "\n",
    "# EfficientNet-inspired Block\n",
    "class MBConvBlock(layers.Layer):\n",
    "    \"\"\"Mobile Inverted Bottleneck Convolution block\"\"\"\n",
    "    \n",
    "    def __init__(self, filters, strides=1, expand_ratio=1, se_ratio=4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        expanded_filters = int(filters * expand_ratio)\n",
    "        \n",
    "        # Expansion\n",
    "        if expand_ratio != 1:\n",
    "            self.expand = layers.Conv2D(expanded_filters, (1, 1), padding='same')\n",
    "            self.bn1 = layers.BatchNormalization()\n",
    "            self.activation1 = layers.Activation(tf.keras.utils.get_custom_objects().get('swish', 'relu'))\n",
    "        else:\n",
    "            self.expand = None\n",
    "        \n",
    "        # Depthwise\n",
    "        self.depthwise = layers.DepthwiseConv2D((3, 3), strides=strides, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.activation2 = layers.Activation(tf.keras.utils.get_custom_objects().get('swish', 'relu'))\n",
    "        \n",
    "        # Squeeze and Excitation\n",
    "        if se_ratio > 0:\n",
    "            self.se = SEBlock(expanded_filters, se_ratio)\n",
    "        else:\n",
    "            self.se = None\n",
    "        \n",
    "        # Output projection\n",
    "        self.project = layers.Conv2D(filters, (1, 1), padding='same')\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        \n",
    "        # Skip connection\n",
    "        self.use_skip = (strides == 1)\n",
    "        self.dropout = layers.Dropout(0.2)\n",
    "        self.add = layers.Add()\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        \n",
    "        # Expansion\n",
    "        if self.expand is not None:\n",
    "            x = self.expand(x)\n",
    "            x = self.bn1(x, training=training)\n",
    "            x = self.activation1(x)\n",
    "        \n",
    "        # Depthwise\n",
    "        x = self.depthwise(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.activation2(x)\n",
    "        \n",
    "        # Squeeze and Excitation\n",
    "        if self.se is not None:\n",
    "            x = self.se(x)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.project(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "        \n",
    "        # Skip connection with dropout\n",
    "        if self.use_skip and x.shape == inputs.shape:\n",
    "            x = self.dropout(x, training=training)\n",
    "            x = self.add([x, inputs])\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test modern architectures\n",
    "print(\"\\n=== Testing Modern Efficient Architectures ===\")\n",
    "\n",
    "# MobileNet V1\n",
    "mobilenet = MobileNetV1(num_classes=10, width_multiplier=0.5)\n",
    "mobilenet.build(input_shape=(None, 32, 32, 3))\n",
    "print(f\"MobileNet V1 (0.5x): {mobilenet.count_params():,} parameters\")\n",
    "\n",
    "# MobileNet with different width multipliers\n",
    "width_multipliers = [0.25, 0.5, 0.75, 1.0]\n",
    "print(\"\\nMobileNet V1 Parameter Count by Width Multiplier:\")\n",
    "for wm in width_multipliers:\n",
    "    mobile = MobileNetV1(num_classes=10, width_multiplier=wm)\n",
    "    mobile.build(input_shape=(None, 32, 32, 3))\n",
    "    print(f\"  Width {wm}: {mobile.count_params():,} parameters\")\n",
    "\n",
    "# Train MobileNet on subset\n",
    "mobilenet.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "mobile_history = mobilenet.fit(\n",
    "    x_train_cifar[:1000], y_train_cifar[:1000],\n",
    "    validation_data=(x_test_cifar[:200], y_test_cifar[:200]),\n",
    "    epochs=8, batch_size=32, verbose=0\n",
    ")\n",
    "\n",
    "print(f\"MobileNet V1 test accuracy: {max(mobile_history.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom CNN Components and Advanced Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced CNN components\n",
    "class MultiScaleConv(layers.Layer):\n",
    "    \"\"\"Multi-scale convolution with different kernel sizes\"\"\"\n",
    "    \n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Different kernel sizes\n",
    "        self.conv1x1 = layers.Conv2D(filters // 4, (1, 1), padding='same')\n",
    "        self.conv3x3 = layers.Conv2D(filters // 4, (3, 3), padding='same')\n",
    "        self.conv5x5 = layers.Conv2D(filters // 4, (5, 5), padding='same')\n",
    "        \n",
    "        # Max pooling branch\n",
    "        self.maxpool = layers.MaxPool2D((3, 3), strides=1, padding='same')\n",
    "        self.pool_conv = layers.Conv2D(filters // 4, (1, 1), padding='same')\n",
    "        \n",
    "        # Batch normalization and activation\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.activation = layers.ReLU()\n",
    "        self.concat = layers.Concatenate()\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Parallel branches\n",
    "        branch1 = self.conv1x1(inputs)\n",
    "        branch2 = self.conv3x3(inputs)\n",
    "        branch3 = self.conv5x5(inputs)\n",
    "        \n",
    "        branch4 = self.maxpool(inputs)\n",
    "        branch4 = self.pool_conv(branch4)\n",
    "        \n",
    "        # Concatenate all branches\n",
    "        x = self.concat([branch1, branch2, branch3, branch4])\n",
    "        x = self.bn(x, training=training)\n",
    "        return self.activation(x)\n",
    "\n",
    "class DilatedConvBlock(layers.Layer):\n",
    "    \"\"\"Dilated convolution block for enlarged receptive field\"\"\"\n",
    "    \n",
    "    def __init__(self, filters, dilation_rates=[1, 2, 4], **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.dilated_convs = []\n",
    "        for rate in dilation_rates:\n",
    "            self.dilated_convs.append(layers.Conv2D(\n",
    "                filters // len(dilation_rates), (3, 3),\n",
    "                dilation_rate=rate, padding='same'\n",
    "            ))\n",
    "        \n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.activation = layers.ReLU()\n",
    "        self.concat = layers.Concatenate()\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        outputs = []\n",
    "        for conv in self.dilated_convs:\n",
    "            outputs.append(conv(inputs))\n",
    "        \n",
    "        x = self.concat(outputs)\n",
    "        x = self.bn(x, training=training)\n",
    "        return self.activation(x)\n",
    "\n",
    "class AttentionModule(layers.Layer):\n",
    "    \"\"\"Channel and spatial attention module\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Channel attention\n",
    "        self.channel_pool = layers.GlobalAveragePooling2D()\n",
    "        self.channel_fc1 = layers.Dense(None)  # Will be set in build\n",
    "        self.channel_fc2 = layers.Dense(None)  # Will be set in build\n",
    "        \n",
    "        # Spatial attention\n",
    "        self.spatial_conv = layers.Conv2D(1, (7, 7), padding='same', activation='sigmoid')\n",
    "        \n",
    "        self.multiply1 = layers.Multiply()\n",
    "        self.multiply2 = layers.Multiply()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.channel_fc1 = layers.Dense(channels // 8, activation='relu')\n",
    "        self.channel_fc2 = layers.Dense(channels, activation='sigmoid')\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Channel attention\n",
    "        channel_avg = self.channel_pool(inputs)\n",
    "        channel_att = self.channel_fc1(channel_avg)\n",
    "        channel_att = self.channel_fc2(channel_att)\n",
    "        channel_att = tf.expand_dims(tf.expand_dims(channel_att, 1), 1)\n",
    "        \n",
    "        x = self.multiply1([inputs, channel_att])\n",
    "        \n",
    "        # Spatial attention\n",
    "        spatial_avg = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "        spatial_max = tf.reduce_max(x, axis=-1, keepdims=True)\n",
    "        spatial_concat = tf.concat([spatial_avg, spatial_max], axis=-1)\n",
    "        spatial_att = self.spatial_conv(spatial_concat)\n",
    "        \n",
    "        output = self.multiply2([x, spatial_att])\n",
    "        return output\n",
    "\n",
    "# Progressive Resizing Training\n",
    "class ProgressiveResizeCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Progressive resizing during training\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_size=32, target_size=224, resize_epochs=[10, 20]):\n",
    "        super().__init__()\n",
    "        self.sizes = [initial_size] + [target_size // (2 ** (len(resize_epochs) - i)) \n",
    "                                      for i in range(len(resize_epochs))] + [target_size]\n",
    "        self.resize_epochs = [0] + resize_epochs + [float('inf')]\n",
    "        self.current_size = initial_size\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        for i, resize_epoch in enumerate(self.resize_epochs[1:], 1):\n",
    "            if epoch == resize_epoch and i < len(self.sizes):\n",
    "                self.current_size = self.sizes[i]\n",
    "                print(f\"\\nProgressive resize: switching to {self.current_size}x{self.current_size}\")\n",
    "                break\n",
    "\n",
    "# Custom CNN with advanced components\n",
    "class AdvancedCNN(tf.keras.Model):\n",
    "    \"\"\"CNN with advanced custom components\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv1 = layers.Conv2D(64, (7, 7), strides=2, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "        self.pool1 = layers.MaxPool2D((3, 3), strides=2, padding='same')\n",
    "        \n",
    "        # Advanced blocks\n",
    "        self.multiscale1 = MultiScaleConv(128)\n",
    "        self.attention1 = AttentionModule()\n",
    "        \n",
    "        self.dilated_block = DilatedConvBlock(256, [1, 2, 4, 8])\n",
    "        \n",
    "        self.multiscale2 = MultiScaleConv(512)\n",
    "        self.attention2 = AttentionModule()\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.dropout = layers.Dropout(0.5)\n",
    "        self.classifier = layers.Dense(num_classes, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.multiscale1(x, training=training)\n",
    "        x = self.attention1(x)\n",
    "        \n",
    "        x = self.dilated_block(x, training=training)\n",
    "        \n",
    "        x = self.multiscale2(x, training=training)\n",
    "        x = self.attention2(x)\n",
    "        \n",
    "        x = self.global_pool(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Test advanced CNN\n",
    "print(\"\\n=== Testing Advanced CNN Components ===\")\n",
    "\n",
    "advanced_cnn = AdvancedCNN(num_classes=10)\n",
    "advanced_cnn.build(input_shape=(None, 32, 32, 3))\n",
    "print(f\"Advanced CNN parameters: {advanced_cnn.count_params():,}\")\n",
    "\n",
    "# Train with advanced techniques\n",
    "advanced_cnn.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(0.001, weight_decay=0.01),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Use data augmentation\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "# Train on subset with augmentation\n",
    "advanced_history = advanced_cnn.fit(\n",
    "    datagen.flow(x_train_cifar[:1000], y_train_cifar[:1000], batch_size=32),\n",
    "    steps_per_epoch=30,\n",
    "    epochs=10,\n",
    "    validation_data=(x_test_cifar[:200], y_test_cifar[:200]),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Advanced CNN test accuracy: {max(advanced_history.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Architecture Comparison and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive architecture comparison\n",
    "class CNNBenchmark:\n",
    "    \"\"\"Benchmark different CNN architectures\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def benchmark_architecture(self, name, model, train_data, val_data, epochs=5):\n",
    "        \"\"\"Benchmark a single architecture\"\"\"\n",
    "        \n",
    "        print(f\"\\nBenchmarking {name}...\")\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Training\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        history = model.fit(\n",
    "            train_data[0], train_data[1],\n",
    "            validation_data=val_data,\n",
    "            epochs=epochs,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Inference timing\n",
    "        inference_times = []\n",
    "        for _ in range(10):\n",
    "            start = time.time()\n",
    "            _ = model.predict(val_data[0][:100], verbose=0)\n",
    "            inference_times.append(time.time() - start)\n",
    "        \n",
    "        avg_inference_time = np.mean(inference_times)\n",
    "        \n",
    "        # Store results\n",
    "        self.results[name] = {\n",
    "            'parameters': model.count_params(),\n",
    "            'training_time': training_time,\n",
    "            'inference_time': avg_inference_time,\n",
    "            'best_val_accuracy': max(history.history['val_accuracy']),\n",
    "            'final_val_accuracy': history.history['val_accuracy'][-1],\n",
    "            'convergence_epoch': np.argmax(history.history['val_accuracy']) + 1\n",
    "        }\n",
    "        \n",
    "        print(f\"  Parameters: {self.results[name]['parameters']:,}\")\n",
    "        print(f\"  Best Val Acc: {self.results[name]['best_val_accuracy']:.4f}\")\n",
    "        print(f\"  Training Time: {training_time:.2f}s\")\n",
    "        \n",
    "    def print_comparison(self):\n",
    "        \"\"\"Print detailed comparison table\"\"\"\n",
    "        \n",
    "        print(\"\\n=== CNN Architecture Comparison ===\")\n",
    "        print(f\"{'Architecture':<15} {'Params':<10} {'Best Acc':<10} {'Train Time':<12} {'Inference':<12} {'Convergence':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for name, result in self.results.items():\n",
    "            print(f\"{name:<15} {result['parameters']:<10,} \"\n",
    "                  f\"{result['best_val_accuracy']:<10.4f} \"\n",
    "                  f\"{result['training_time']:<12.2f} \"\n",
    "                  f\"{result['inference_time']:<12.4f} \"\n",
    "                  f\"{result['convergence_epoch']:<12}\")\n",
    "        \n",
    "        # Analysis\n",
    "        print(\"\\n=== Analysis ===\")\n",
    "        \n",
    "        # Best accuracy\n",
    "        best_acc_model = max(self.results.items(), key=lambda x: x[1]['best_val_accuracy'])\n",
    "        print(f\"Best Accuracy: {best_acc_model[0]} ({best_acc_model[1]['best_val_accuracy']:.4f})\")\n",
    "        \n",
    "        # Most efficient (params)\n",
    "        most_efficient = min(self.results.items(), key=lambda x: x[1]['parameters'])\n",
    "        print(f\"Most Efficient: {most_efficient[0]} ({most_efficient[1]['parameters']:,} params)\")\n",
    "        \n",
    "        # Fastest training\n",
    "        fastest_train = min(self.results.items(), key=lambda x: x[1]['training_time'])\n",
    "        print(f\"Fastest Training: {fastest_train[0]} ({fastest_train[1]['training_time']:.2f}s)\")\n",
    "        \n",
    "        # Fastest inference\n",
    "        fastest_inference = min(self.results.items(), key=lambda x: x[1]['inference_time'])\n",
    "        print(f\"Fastest Inference: {fastest_inference[0]} ({fastest_inference[1]['inference_time']:.4f}s)\")\n",
    "\n",
    "# Run comprehensive benchmark\n",
    "benchmark = CNNBenchmark()\n",
    "\n",
    "# Prepare smaller datasets for quick benchmarking\n",
    "train_subset = (x_train_cifar[:800], y_train_cifar[:800])\n",
    "val_subset = (x_test_cifar[:200], y_test_cifar[:200])\n",
    "\n",
    "# Benchmark different architectures\n",
    "architectures_to_test = {\n",
    "    'LeNet-5': LeNet5(num_classes=10),\n",
    "    'AlexNet': AlexNet(num_classes=10),\n",
    "    'ResNet-18': resnet_variants['ResNet-18'],\n",
    "    'MobileNet': MobileNetV1(num_classes=10, width_multiplier=0.5),\n",
    "    'Advanced_CNN': AdvancedCNN(num_classes=10)\n",
    "}\n",
    "\n",
    "# Build all models first\n",
    "for name, model in architectures_to_test.items():\n",
    "    if name == 'LeNet-5':\n",
    "        # LeNet expects grayscale, convert CIFAR-10 to grayscale\n",
    "        gray_train = tf.image.rgb_to_grayscale(train_subset[0])\n",
    "        gray_val = tf.image.rgb_to_grayscale(val_subset[0])\n",
    "        gray_train = tf.image.resize(gray_train, [28, 28])\n",
    "        gray_val = tf.image.resize(gray_val, [28, 28])\n",
    "        model.build(input_shape=(None, 28, 28, 1))\n",
    "        benchmark.benchmark_architecture(\n",
    "            name, model, (gray_train, train_subset[1]), (gray_val, val_subset[1]), epochs=3\n",
    "        )\n",
    "    else:\n",
    "        model.build(input_shape=(None, 32, 32, 3))\n",
    "        benchmark.benchmark_architecture(name, model, train_subset, val_subset, epochs=3)\n",
    "\n",
    "benchmark.print_comparison()\n",
    "\n",
    "# CNN Design Best Practices\n",
    "def print_cnn_best_practices():\n",
    "    \"\"\"Print CNN design best practices and guidelines\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CNN ARCHITECTURE DESIGN BEST PRACTICES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    practices = {\n",
    "        \"ðŸ—ï¸ ARCHITECTURAL PATTERNS\": [\n",
    "            \"Use residual connections for networks deeper than 20 layers\",\n",
    "            \"Apply batch normalization after convolutions, before activation\",\n",
    "            \"Use ReLU or its variants (LeakyReLU, ELU, Swish) for activation\",\n",
    "            \"Implement progressive downsampling with pooling or strided convs\",\n",
    "            \"Consider depthwise separable convs for mobile/edge deployment\"\n",
    "        ],\n",
    "        \n",
    "        \"âš¡ EFFICIENCY OPTIMIZATIONS\": [\n",
    "            \"Use global average pooling instead of large fully connected layers\",\n",
    "            \"Apply channel/spatial attention for better feature selection\",\n",
    "            \"Implement early stopping and learning rate scheduling\",\n",
    "            \"Use mixed precision training for faster training on modern GPUs\",\n",
    "            \"Consider progressive resizing for faster convergence\"\n",
    "        ],\n",
    "        \n",
    "        \"ðŸ“Š DATA HANDLING\": [\n",
    "            \"Normalize input data to [0,1] or [-1,1] range\",\n",
    "            \"Apply data augmentation (rotation, flip, zoom, crop)\",\n",
    "            \"Use proper train/validation/test splits\",\n",
    "            \"Implement data pipeline optimization with tf.data\",\n",
    "            \"Consider mixup or cutmix for improved generalization\"\n",
    "        ],\n",
    "        \n",
    "        \"ðŸŽ¯ REGULARIZATION TECHNIQUES\": [\n",
    "            \"Apply dropout (0.2-0.5) in dense layers, not conv layers\",\n",
    "            \"Use L2 weight decay (1e-4 to 1e-2) in optimizer\",\n",
    "            \"Implement label smoothing for better calibration\",\n",
    "            \"Consider stochastic depth for very deep networks\",\n",
    "            \"Apply spectral normalization for improved training stability\"\n",
    "        ],\n",
    "        \n",
    "        \"ðŸ”§ HYPERPARAMETER GUIDELINES\": [\n",
    "            \"Start with learning rate 1e-3, adjust based on validation loss\",\n",
    "            \"Use batch size 32-128 depending on memory constraints\",\n",
    "            \"Apply cosine annealing or reduce-on-plateau LR scheduling\",\n",
    "            \"Set initial weights with He or Xavier initialization\",\n",
    "            \"Monitor gradient norms to detect vanishing/exploding gradients\"\n",
    "        ],\n",
    "        \n",
    "        \"ðŸ“± DEPLOYMENT CONSIDERATIONS\": [\n",
    "            \"Use quantization for mobile/edge deployment\",\n",
    "            \"Consider pruning for model compression\",\n",
    "            \"Implement model distillation for smaller student models\",\n",
    "            \"Use TensorFlow Lite for mobile optimization\",\n",
    "            \"Profile inference time vs accuracy trade-offs\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in practices.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        for item in items:\n",
    "            print(f\"  â€¢ {item}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ARCHITECTURE SELECTION GUIDE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    selection_guide = {\n",
    "        \"High Accuracy (Research)\": \"ResNet-50/101, EfficientNet, Vision Transformers\",\n",
    "        \"Mobile/Edge Deployment\": \"MobileNet, EfficientNet-B0, SqueezeNet\",\n",
    "        \"Real-time Applications\": \"YOLO variants, MobileNet, optimized ResNet\",\n",
    "        \"Limited Data\": \"Transfer learning with pre-trained models\",\n",
    "        \"Custom Domain\": \"Start with ResNet-18/34, add domain-specific layers\"\n",
    "    }\n",
    "    \n",
    "    for use_case, recommendation in selection_guide.items():\n",
    "        print(f\"{use_case:<25}: {recommendation}\")\n",
    "\n",
    "print_cnn_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**File Location:** `notebooks/03_computer_vision/07_cnn_architectures_keras.ipynb`\n",
    "\n",
    "This comprehensive notebook mastered CNN architectures with tf.keras:\n",
    "\n",
    "### Classic Architectures Implemented:\n",
    "1. **LeNet-5**: Pioneer architecture for digit recognition\n",
    "2. **AlexNet**: First deep CNN with ReLU and dropout\n",
    "3. **VGG-16**: Deep networks with small 3x3 filters\n",
    "4. **ResNet**: Residual connections enabling very deep networks\n",
    "\n",
    "### Modern Efficient Architectures:\n",
    "- **MobileNet V1**: Depthwise separable convolutions for mobile\n",
    "- **Inverted Residual Blocks**: Linear bottlenecks for efficiency\n",
    "- **Squeeze-and-Excitation**: Channel attention mechanisms\n",
    "- **EfficientNet-inspired**: Compound scaling strategies\n",
    "\n",
    "### Advanced CNN Components:\n",
    "- **Multi-scale Convolutions**: Parallel filters of different sizes\n",
    "- **Dilated Convolutions**: Enlarged receptive fields without parameters\n",
    "- **Attention Modules**: Channel and spatial attention mechanisms\n",
    "- **Progressive Training**: Dynamic input size scheduling\n",
    "\n",
    "### Key Technical Insights:\n",
    "- **Residual connections** prevent vanishing gradients in deep networks\n",
    "- **Depthwise separable convolutions** reduce parameters by 8-9x\n",
    "- **Batch normalization** accelerates training and improves stability\n",
    "- **Global average pooling** reduces parameters vs fully connected layers\n",
    "- **Attention mechanisms** improve feature selection and model interpretability\n",
    "\n",
    "### Performance Optimizations:\n",
    "- **Parameter efficiency**: MobileNet achieves good accuracy with fewer params\n",
    "- **Training speed**: Batch normalization and proper initialization crucial\n",
    "- **Inference speed**: Depthwise convolutions and efficient blocks\n",
    "- **Memory usage**: Global pooling and architectural choices matter\n",
    "\n",
    "### Best Practices Established:\n",
    "- Use residual connections for networks >20 layers\n",
    "- Apply batch normalization after conv, before activation\n",
    "- Implement proper data augmentation and regularization\n",
    "- Consider deployment constraints when choosing architecture\n",
    "- Profile accuracy vs efficiency trade-offs\n",
    "\n",
    "### Next Steps:\n",
    "- Apply transfer learning with pre-trained models (Notebook 08)\n",
    "- Implement semantic segmentation architectures (Notebook 09)\n",
    "- Explore vision transformers and hybrid architectures\n",
    "- Deploy optimized models to mobile and edge devices\n",
    "\n",
    "This foundation enables building and optimizing CNN architectures for any computer vision task!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
