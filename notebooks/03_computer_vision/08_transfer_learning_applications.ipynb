{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 transfer learning applications\n",
    "**Location: TensorVerseHub/notebooks/03_computer_vision/08_transfer_learning_applications.ipynb**\n",
    "\n",
    "TODO: Implement comprehensive TensorFlow + tf.keras learning content.\n",
    "\n",
    "## Learning Objectives\n",
    "- TODO: Define specific learning objectives\n",
    "- TODO: List key TensorFlow concepts covered\n",
    "- TODO: Outline tf.keras integration points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "# TODO: Add comprehensive implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Applications with tf.keras\n",
    "\n",
    "**File Location:** `notebooks/03_computer_vision/08_transfer_learning_applications.ipynb`\n",
    "\n",
    "Master transfer learning with tf.keras.applications, implementing fine-tuning strategies, feature extraction, and domain adaptation techniques. Build production-ready models by leveraging pre-trained networks and optimizing for custom datasets.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master tf.keras.applications pre-trained models\n",
    "- Implement feature extraction and fine-tuning strategies\n",
    "- Apply progressive unfreezing and discriminative learning rates\n",
    "- Handle domain adaptation and custom dataset challenges\n",
    "- Build production transfer learning pipelines\n",
    "- Optimize models for different data constraints\n",
    "\n",
    "---\n",
    "\n",
    "## 1. tf.keras.applications and Pre-trained Models\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup directories and data\n",
    "def setup_transfer_learning_environment():\n",
    "    \"\"\"Setup environment for transfer learning experiments\"\"\"\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs('transfer_learning_data', exist_ok=True)\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    print(\"=== Available Pre-trained Models in tf.keras.applications ===\")\n",
    "    \n",
    "    # List of available models with their properties\n",
    "    models_info = {\n",
    "        'VGG16': {'params': '138M', 'top1_acc': '71.3%', 'size': '528MB'},\n",
    "        'VGG19': {'params': '143M', 'top1_acc': '71.1%', 'size': '549MB'},\n",
    "        'ResNet50': {'params': '25.6M', 'top1_acc': '74.9%', 'size': '99MB'},\n",
    "        'ResNet101': {'params': '44.7M', 'top1_acc': '76.4%', 'size': '171MB'},\n",
    "        'ResNet152': {'params': '60.4M', 'top1_acc': '76.6%', 'size': '232MB'},\n",
    "        'InceptionV3': {'params': '23.9M', 'top1_acc': '77.9%', 'size': '92MB'},\n",
    "        'InceptionResNetV2': {'params': '55.9M', 'top1_acc': '80.3%', 'size': '215MB'},\n",
    "        'Xception': {'params': '22.9M', 'top1_acc': '79.0%', 'size': '88MB'},\n",
    "        'DenseNet121': {'params': '8.1M', 'top1_acc': '75.0%', 'size': '33MB'},\n",
    "        'DenseNet201': {'params': '20.2M', 'top1_acc': '77.3%', 'size': '80MB'},\n",
    "        'MobileNet': {'params': '4.3M', 'top1_acc': '70.4%', 'size': '16MB'},\n",
    "        'MobileNetV2': {'params': '3.5M', 'top1_acc': '71.3%', 'size': '14MB'},\n",
    "        'EfficientNetB0': {'params': '5.3M', 'top1_acc': '77.1%', 'size': '29MB'},\n",
    "        'EfficientNetB7': {'params': '66.7M', 'top1_acc': '84.4%', 'size': '256MB'}\n",
    "    }\n",
    "    \n",
    "    print(f\"{'Model':<20} {'Parameters':<12} {'Top-1 Acc':<12} {'Size':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    for model_name, info in models_info.items():\n",
    "        print(f\"{model_name:<20} {info['params']:<12} {info['top1_acc']:<12} {info['size']:<10}\")\n",
    "    \n",
    "    return models_info\n",
    "\n",
    "models_info = setup_transfer_learning_environment()\n",
    "\n",
    "# Load and prepare datasets\n",
    "def prepare_datasets():\n",
    "    \"\"\"Prepare CIFAR-10 and CIFAR-100 for transfer learning\"\"\"\n",
    "    \n",
    "    # CIFAR-10\n",
    "    (x_train_10, y_train_10), (x_test_10, y_test_10) = tf.keras.datasets.cifar10.load_data()\n",
    "    \n",
    "    # CIFAR-100\n",
    "    (x_train_100, y_train_100), (x_test_100, y_test_100) = tf.keras.datasets.cifar100.load_data()\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    x_train_10 = x_train_10.astype('float32') / 255.0\n",
    "    x_test_10 = x_test_10.astype('float32') / 255.0\n",
    "    x_train_100 = x_train_100.astype('float32') / 255.0\n",
    "    x_test_100 = x_test_100.astype('float32') / 255.0\n",
    "    \n",
    "    # Flatten labels\n",
    "    y_train_10 = y_train_10.flatten()\n",
    "    y_test_10 = y_test_10.flatten()\n",
    "    y_train_100 = y_train_100.flatten()\n",
    "    y_test_100 = y_test_100.flatten()\n",
    "    \n",
    "    print(\"Dataset Information:\")\n",
    "    print(f\"CIFAR-10: {x_train_10.shape} train, {x_test_10.shape} test, {len(np.unique(y_train_10))} classes\")\n",
    "    print(f\"CIFAR-100: {x_train_100.shape} train, {x_test_100.shape} test, {len(np.unique(y_train_100))} classes\")\n",
    "    \n",
    "    return (x_train_10, y_train_10, x_test_10, y_test_10), (x_train_100, y_train_100, x_test_100, y_test_100)\n",
    "\n",
    "cifar10_data, cifar100_data = prepare_datasets()\n",
    "(x_train_10, y_train_10, x_test_10, y_test_10) = cifar10_data\n",
    "(x_train_100, y_train_100, x_test_100, y_test_100) = cifar100_data\n",
    "\n",
    "# Demonstrate loading pre-trained models\n",
    "def demonstrate_pretrained_models():\n",
    "    \"\"\"Demonstrate loading and using pre-trained models\"\"\"\n",
    "    \n",
    "    print(\"\\n=== Loading Pre-trained Models ===\")\n",
    "    \n",
    "    # Load different models\n",
    "    models_to_test = [\n",
    "        ('ResNet50', applications.ResNet50),\n",
    "        ('MobileNetV2', applications.MobileNetV2),\n",
    "        ('EfficientNetB0', applications.EfficientNetB0)\n",
    "    ]\n",
    "    \n",
    "    for name, model_class in models_to_test:\n",
    "        print(f\"\\n--- {name} ---\")\n",
    "        \n",
    "        # Load with ImageNet weights\n",
    "        model = model_class(\n",
    "            weights='imagenet',\n",
    "            include_top=False,  # Exclude final classification layer\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "        \n",
    "        print(f\"Parameters: {model.count_params():,}\")\n",
    "        print(f\"Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}\")\n",
    "        print(f\"Layers: {len(model.layers)}\")\n",
    "        \n",
    "        # Test with random input\n",
    "        test_input = tf.random.normal((1, 224, 224, 3))\n",
    "        features = model(test_input)\n",
    "        print(f\"Feature shape: {features.shape}\")\n",
    "        \n",
    "        # Show some layer names\n",
    "        print(f\"First 5 layers: {[layer.name for layer in model.layers[:5]]}\")\n",
    "        print(f\"Last 5 layers: {[layer.name for layer in model.layers[-5:]]}\")\n",
    "\n",
    "demonstrate_pretrained_models()\n",
    "```\n",
    "\n",
    "## 2. Feature Extraction Strategy\n",
    "\n",
    "```python\n",
    "# Feature extraction implementation\n",
    "class FeatureExtractor:\n",
    "    \"\"\"Feature extraction using pre-trained models\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='ResNet50', input_shape=(224, 224, 3)):\n",
    "        self.model_name = model_name\n",
    "        self.input_shape = input_shape\n",
    "        self.base_model = None\n",
    "        self.feature_extractor = None\n",
    "        \n",
    "    def build_feature_extractor(self, pooling='avg'):\n",
    "        \"\"\"Build feature extractor from pre-trained model\"\"\"\n",
    "        \n",
    "        # Map model names to classes\n",
    "        model_map = {\n",
    "            'ResNet50': applications.ResNet50,\n",
    "            'MobileNetV2': applications.MobileNetV2,\n",
    "            'EfficientNetB0': applications.EfficientNetB0,\n",
    "            'VGG16': applications.VGG16,\n",
    "            'InceptionV3': applications.InceptionV3\n",
    "        }\n",
    "        \n",
    "        if self.model_name not in model_map:\n",
    "            raise ValueError(f\"Model {self.model_name} not supported\")\n",
    "            \n",
    "        # Load base model\n",
    "        self.base_model = model_map[self.model_name](\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=self.input_shape,\n",
    "            pooling=pooling\n",
    "        )\n",
    "        \n",
    "        # Freeze all layers for feature extraction\n",
    "        self.base_model.trainable = False\n",
    "        \n",
    "        print(f\"Feature extractor built with {self.model_name}\")\n",
    "        print(f\"Feature dimension: {self.base_model.output.shape[-1]}\")\n",
    "        \n",
    "        return self.base_model\n",
    "    \n",
    "    def create_classifier(self, num_classes, dropout_rate=0.2):\n",
    "        \"\"\"Create classifier on top of feature extractor\"\"\"\n",
    "        \n",
    "        if self.base_model is None:\n",
    "            raise ValueError(\"Build feature extractor first\")\n",
    "            \n",
    "        # Create full model\n",
    "        inputs = tf.keras.Input(shape=self.input_shape)\n",
    "        \n",
    "        # Preprocessing for ImageNet models\n",
    "        x = applications.imagenet_utils.preprocess_input(inputs)\n",
    "        \n",
    "        # Feature extraction\n",
    "        features = self.base_model(x, training=False)\n",
    "        \n",
    "        # Classification head\n",
    "        x = layers.Dropout(dropout_rate)(features)\n",
    "        predictions = layers.Dense(num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = tf.keras.Model(inputs, predictions)\n",
    "        return model\n",
    "    \n",
    "    def extract_features(self, images, batch_size=32):\n",
    "        \"\"\"Extract features from images\"\"\"\n",
    "        \n",
    "        if self.base_model is None:\n",
    "            raise ValueError(\"Build feature extractor first\")\n",
    "            \n",
    "        # Preprocess images\n",
    "        preprocessed = applications.imagenet_utils.preprocess_input(images * 255.0)\n",
    "        \n",
    "        # Resize to model input size\n",
    "        if images.shape[1:3] != self.input_shape[:2]:\n",
    "            preprocessed = tf.image.resize(preprocessed, self.input_shape[:2])\n",
    "        \n",
    "        # Extract features in batches\n",
    "        features = self.base_model.predict(preprocessed, batch_size=batch_size, verbose=0)\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Demonstrate feature extraction\n",
    "def demonstrate_feature_extraction():\n",
    "    \"\"\"Demonstrate feature extraction approach\"\"\"\n",
    "    \n",
    "    print(\"\\n=== Feature Extraction Demonstration ===\")\n",
    "    \n",
    "    # Create feature extractor\n",
    "    extractor = FeatureExtractor('ResNet50')\n",
    "    base_model = extractor.build_feature_extractor()\n",
    "    \n",
    "    # Create classifier for CIFAR-10\n",
    "    model = extractor.create_classifier(num_classes=10, dropout_rate=0.3)\n",
    "    \n",
    "    print(f\"Full model parameters: {model.count_params():,}\")\n",
    "    print(f\"Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}\")\n",
    "    \n",
    "    # Compile and test\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Resize CIFAR-10 to 224x224 for pre-trained models\n",
    "    def resize_data(x, target_size=(224, 224)):\n",
    "        return tf.image.resize(x, target_size).numpy()\n",
    "    \n",
    "    x_train_resized = resize_data(x_train_10[:1000])  # Use subset for demo\n",
    "    x_test_resized = resize_data(x_test_10[:200])\n",
    "    \n",
    "    # Train feature extraction model\n",
    "    history = model.fit(\n",
    "        x_train_resized, y_train_10[:1000],\n",
    "        validation_data=(x_test_resized, y_test_10[:200]),\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Feature extraction best val accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "feature_model, feature_history = demonstrate_feature_extraction()\n",
    "```\n",
    "\n",
    "## 3. Fine-tuning Strategies\n",
    "\n",
    "```python\n",
    "# Advanced fine-tuning implementations\n",
    "class AdvancedFineTuner:\n",
    "    \"\"\"Advanced fine-tuning with progressive unfreezing and discriminative learning rates\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='ResNet50', input_shape=(224, 224, 3)):\n",
    "        self.model_name = model_name\n",
    "        self.input_shape = input_shape\n",
    "        self.base_model = None\n",
    "        self.model = None\n",
    "        \n",
    "    def build_model(self, num_classes):\n",
    "        \"\"\"Build model for fine-tuning\"\"\"\n",
    "        \n",
    "        model_map = {\n",
    "            'ResNet50': applications.ResNet50,\n",
    "            'MobileNetV2': applications.MobileNetV2,\n",
    "            'EfficientNetB0': applications.EfficientNetB0,\n",
    "            'VGG16': applications.VGG16\n",
    "        }\n",
    "        \n",
    "        # Load base model\n",
    "        self.base_model = model_map[self.model_name](\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=self.input_shape\n",
    "        )\n",
    "        \n",
    "        # Initially freeze all layers\n",
    "        self.base_model.trainable = False\n",
    "        \n",
    "        # Build complete model\n",
    "        inputs = tf.keras.Input(shape=self.input_shape)\n",
    "        x = applications.imagenet_utils.preprocess_input(inputs)\n",
    "        \n",
    "        # Base model features\n",
    "        x = self.base_model(x, training=False)\n",
    "        \n",
    "        # Global pooling and regularization\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # Additional dense layers for fine-tuning\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # Final classification\n",
    "        predictions = layers.Dense(num_classes, activation='softmax')(x)\n",
    "        \n",
    "        self.model = tf.keras.Model(inputs, predictions)\n",
    "        return self.model\n",
    "    \n",
    "    def progressive_unfreeze(self, stage=1):\n",
    "        \"\"\"Progressively unfreeze layers for fine-tuning\"\"\"\n",
    "        \n",
    "        if self.base_model is None:\n",
    "            raise ValueError(\"Build model first\")\n",
    "            \n",
    "        total_layers = len(self.base_model.layers)\n",
    "        \n",
    "        if stage == 1:\n",
    "            # Stage 1: Only train classifier\n",
    "            self.base_model.trainable = False\n",
    "            print(\"Stage 1: Training classifier only\")\n",
    "            \n",
    "        elif stage == 2:\n",
    "            # Stage 2: Unfreeze top 25% of layers\n",
    "            unfreeze_from = int(total_layers * 0.75)\n",
    "            self.base_model.trainable = True\n",
    "            \n",
    "            for layer in self.base_model.layers[:unfreeze_from]:\n",
    "                layer.trainable = False\n",
    "                \n",
    "            print(f\"Stage 2: Unfrozen layers from {unfreeze_from} to {total_layers}\")\n",
    "            \n",
    "        elif stage == 3:\n",
    "            # Stage 3: Unfreeze top 50% of layers\n",
    "            unfreeze_from = int(total_layers * 0.5)\n",
    "            self.base_model.trainable = True\n",
    "            \n",
    "            for layer in self.base_model.layers[:unfreeze_from]:\n",
    "                layer.trainable = False\n",
    "                \n",
    "            print(f\"Stage 3: Unfrozen layers from {unfreeze_from} to {total_layers}\")\n",
    "            \n",
    "        elif stage == 4:\n",
    "            # Stage 4: Fine-tune all layers\n",
    "            self.base_model.trainable = True\n",
    "            print(\"Stage 4: All layers unfrozen\")\n",
    "        \n",
    "        # Count trainable parameters\n",
    "        trainable_params = sum([tf.keras.backend.count_params(w) for w in self.model.trainable_weights])\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    def compile_with_discriminative_lr(self, stage=1):\n",
    "        \"\"\"Compile model with discriminative learning rates\"\"\"\n",
    "        \n",
    "        if stage == 1:\n",
    "            # High learning rate for classifier\n",
    "            optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "        elif stage == 2:\n",
    "            # Lower learning rate for pre-trained layers\n",
    "            optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "        elif stage == 3:\n",
    "            # Even lower learning rate\n",
    "            optimizer = tf.keras.optimizers.Adam(0.00005)\n",
    "        else:\n",
    "            # Very low learning rate for full fine-tuning\n",
    "            optimizer = tf.keras.optimizers.Adam(0.00001)\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"Compiled with learning rate: {optimizer.learning_rate}\")\n",
    "\n",
    "# Custom callbacks for fine-tuning\n",
    "class ProgressiveUnfreezeCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback to progressively unfreeze layers during training\"\"\"\n",
    "    \n",
    "    def __init__(self, fine_tuner, unfreeze_epochs=[10, 20, 30]):\n",
    "        super().__init__()\n",
    "        self.fine_tuner = fine_tuner\n",
    "        self.unfreeze_epochs = unfreeze_epochs\n",
    "        self.current_stage = 1\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch in self.unfreeze_epochs and self.current_stage < 4:\n",
    "            self.current_stage += 1\n",
    "            print(f\"\\nEpoch {epoch}: Progressing to stage {self.current_stage}\")\n",
    "            \n",
    "            # Unfreeze more layers\n",
    "            self.fine_tuner.progressive_unfreeze(self.current_stage)\n",
    "            \n",
    "            # Recompile with new learning rate\n",
    "            self.fine_tuner.compile_with_discriminative_lr(self.current_stage)\n",
    "\n",
    "class GradualWarmupScheduler(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Gradual warmup learning rate scheduler\"\"\"\n",
    "    \n",
    "    def __init__(self, warmup_epochs=5, base_lr=0.001, warmup_lr=0.0001):\n",
    "        super().__init__()\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.base_lr = base_lr\n",
    "        self.warmup_lr = warmup_lr\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            lr = self.warmup_lr + (self.base_lr - self.warmup_lr) * epoch / self.warmup_epochs\n",
    "            tf.keras.backend.set_value(self.model.optimizer.learning_rate, lr)\n",
    "            print(f\"Warmup LR: {lr:.6f}\")\n",
    "\n",
    "# Demonstrate advanced fine-tuning\n",
    "def demonstrate_fine_tuning():\n",
    "    \"\"\"Demonstrate advanced fine-tuning strategies\"\"\"\n",
    "    \n",
    "    print(\"\\n=== Advanced Fine-tuning Demonstration ===\")\n",
    "    \n",
    "    # Create fine-tuner\n",
    "    fine_tuner = AdvancedFineTuner('MobileNetV2')\n",
    "    model = fine_tuner.build_model(num_classes=10)\n",
    "    \n",
    "    # Prepare data (resize for ImageNet models)\n",
    "    def prepare_fine_tune_data(x, y, subset_size=1500):\n",
    "        x_resized = tf.image.resize(x[:subset_size], (224, 224)).numpy()\n",
    "        return x_resized, y[:subset_size]\n",
    "    \n",
    "    x_train_ft, y_train_ft = prepare_fine_tune_data(x_train_10, y_train_10)\n",
    "    x_val_ft, y_val_ft = prepare_fine_tune_data(x_test_10, y_test_10, 300)\n",
    "    \n",
    "    print(f\"Fine-tuning data: {x_train_ft.shape} train, {x_val_ft.shape} val\")\n",
    "    \n",
    "    # Stage 1: Train classifier only\n",
    "    fine_tuner.progressive_unfreeze(stage=1)\n",
    "    fine_tuner.compile_with_discriminative_lr(stage=1)\n",
    "    \n",
    "    print(\"Stage 1: Training classifier...\")\n",
    "    stage1_history = model.fit(\n",
    "        x_train_ft, y_train_ft,\n",
    "        validation_data=(x_val_ft, y_val_ft),\n",
    "        epochs=8,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    stage1_acc = max(stage1_history.history['val_accuracy'])\n",
    "    print(f\"Stage 1 best accuracy: {stage1_acc:.4f}\")\n",
    "    \n",
    "    # Stage 2: Fine-tune top layers\n",
    "    fine_tuner.progressive_unfreeze(stage=2)\n",
    "    fine_tuner.compile_with_discriminative_lr(stage=2)\n",
    "    \n",
    "    print(\"Stage 2: Fine-tuning top layers...\")\n",
    "    stage2_history = model.fit(\n",
    "        x_train_ft, y_train_ft,\n",
    "        validation_data=(x_val_ft, y_val_ft),\n",
    "        epochs=8,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    stage2_acc = max(stage2_history.history['val_accuracy'])\n",
    "    print(f\"Stage 2 best accuracy: {stage2_acc:.4f}\")\n",
    "    \n",
    "    # Compare with feature extraction\n",
    "    improvement = stage2_acc - max(feature_history.history['val_accuracy'])\n",
    "    print(f\"Fine-tuning improvement: {improvement:.4f}\")\n",
    "    \n",
    "    return model, fine_tuner, (stage1_history, stage2_history)\n",
    "\n",
    "fine_tuned_model, fine_tuner, fine_tune_histories = demonstrate_fine_tuning()\n",
    "```\n",
    "\n",
    "## 4. Domain Adaptation Techniques\n",
    "\n",
    "```python\n",
    "# Domain adaptation for different datasets\n",
    "class DomainAdapter:\n",
    "    \"\"\"Domain adaptation techniques for transfer learning\"\"\"\n",
    "    \n",
    "    def __init__(self, source_model, target_classes):\n",
    "        self.source_model = source_model\n",
    "        self.target_classes = target_classes\n",
    "        self.adapted_model = None\n",
    "        \n",
    "    def layer_wise_adaptation(self, adaptation_layers=['top', 'middle']):\n",
    "        \"\"\"Adapt specific layers for new domain\"\"\"\n",
    "        \n",
    "        # Get source model without top layers\n",
    "        if hasattr(self.source_model, 'layers'):\n",
    "            base_layers = self.source_model.layers[:-3]  # Remove last 3 layers\n",
    "        else:\n",
    "            base_layers = self.source_model.get_layer('global_average_pooling2d')\n",
    "            \n",
    "        # Build adapted model\n",
    "        inputs = self.source_model.input\n",
    "        x = inputs\n",
    "        \n",
    "        # Pass through base layers\n",
    "        for layer in base_layers[:-1]:  # Skip the last few layers\n",
    "            if isinstance(layer, tf.keras.layers.InputLayer):\n",
    "                continue\n",
    "            x = layer(x)\n",
    "            \n",
    "        # Add adaptation layers\n",
    "        if 'middle' in adaptation_layers:\n",
    "            # Add middle adaptation layer\n",
    "            x = layers.Dense(1024, activation='relu', name='adaptation_middle')(x)\n",
    "            x = layers.BatchNormalization(name='bn_adaptation_middle')(x)\n",
    "            x = layers.Dropout(0.5, name='dropout_adaptation_middle')(x)\n",
    "        \n",
    "        # Global pooling if needed\n",
    "        if len(x.shape) > 2:\n",
    "            x = layers.GlobalAveragePooling2D(name='adaptation_pool')(x)\n",
    "        \n",
    "        if 'top' in adaptation_layers:\n",
    "            # Add top adaptation layer\n",
    "            x = layers.Dense(512, activation='relu', name='adaptation_top')(x)\n",
    "            x = layers.BatchNormalization(name='bn_adaptation_top')(x)\n",
    "            x = layers.Dropout(0.4, name='dropout_adaptation_top')(x)\n",
    "        \n",
    "        # Final classification for target domain\n",
    "        outputs = layers.Dense(\n",
    "            self.target_classes, \n",
    "            activation='softmax', \n",
    "            name='target_classification'\n",
    "        )(x)\n",
    "        \n",
    "        self.adapted_model = tf.keras.Model(inputs, outputs, name='domain_adapted_model')\n",
    "        return self.adapted_model\n",
    "    \n",
    "    def freeze_source_layers(self, freeze_ratio=0.8):\n",
    "        \"\"\"Freeze bottom layers from source model\"\"\"\n",
    "        \n",
    "        if self.adapted_model is None:\n",
    "            raise ValueError(\"Create adapted model first\")\n",
    "            \n",
    "        total_layers = len(self.adapted_model.layers)\n",
    "        freeze_until = int(total_layers * freeze_ratio)\n",
    "        \n",
    "        for i, layer in enumerate(self.adapted_model.layers):\n",
    "            if i < freeze_until and 'adaptation' not in layer.name:\n",
    "                layer.trainable = False\n",
    "            else:\n",
    "                layer.trainable = True\n",
    "                \n",
    "        trainable_params = sum([tf.keras.backend.count_params(w) \n",
    "                               for w in self.adapted_model.trainable_weights])\n",
    "        print(f\"Frozen {freeze_until}/{total_layers} layers\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "class MultiSourceAdapter:\n",
    "    \"\"\"Combine knowledge from multiple pre-trained models\"\"\"\n",
    "    \n",
    "    def __init__(self, model_names=['ResNet50', 'MobileNetV2'], input_shape=(224, 224, 3)):\n",
    "        self.model_names = model_names\n",
    "        self.input_shape = input_shape\n",
    "        self.source_models = []\n",
    "        self.ensemble_model = None\n",
    "        \n",
    "    def load_source_models(self):\n",
    "        \"\"\"Load multiple source models\"\"\"\n",
    "        \n",
    "        model_map = {\n",
    "            'ResNet50': applications.ResNet50,\n",
    "            'MobileNetV2': applications.MobileNetV2,\n",
    "            'EfficientNetB0': applications.EfficientNetB0\n",
    "        }\n",
    "        \n",
    "        for name in self.model_names:\n",
    "            model = model_map[name](\n",
    "                weights='imagenet',\n",
    "                include_top=False,\n",
    "                input_shape=self.input_shape,\n",
    "                pooling='avg'\n",
    "            )\n",
    "            model.trainable = False  # Freeze for feature extraction\n",
    "            self.source_models.append((name, model))\n",
    "            print(f\"Loaded {name}: {model.output.shape[-1]} features\")\n",
    "    \n",
    "    def create_ensemble_adapter(self, num_classes):\n",
    "        \"\"\"Create ensemble model combining multiple sources\"\"\"\n",
    "        \n",
    "        if not self.source_models:\n",
    "            self.load_source_models()\n",
    "            \n",
    "        inputs = tf.keras.Input(shape=self.input_shape)\n",
    "        x = applications.imagenet_utils.preprocess_input(inputs)\n",
    "        \n",
    "        # Extract features from each source model\n",
    "        features = []\n",
    "        for name, model in self.source_models:\n",
    "            feature = model(x, training=False)\n",
    "            features.append(feature)\n",
    "        \n",
    "        # Combine features\n",
    "        if len(features) > 1:\n",
    "            combined = layers.Concatenate(name='feature_fusion')(features)\n",
    "        else:\n",
    "            combined = features[0]\n",
    "        \n",
    "        # Adaptation layers\n",
    "        x = layers.Dense(1024, activation='relu', name='fusion_dense1')(combined)\n",
    "        x = layers.BatchNormalization(name='fusion_bn1')(x)\n",
    "        x = layers.Dropout(0.5, name='fusion_dropout1')(x)\n",
    "        \n",
    "        x = layers.Dense(512, activation='relu', name='fusion_dense2')(x)\n",
    "        x = layers.BatchNormalization(name='fusion_bn2')(x)\n",
    "        x = layers.Dropout(0.3, name='fusion_dropout2')(x)\n",
    "        \n",
    "        # Final classification\n",
    "        outputs = layers.Dense(num_classes, activation='softmax', name='ensemble_output')(x)\n",
    "        \n",
    "        self.ensemble_model = tf.keras.Model(inputs, outputs, name='multi_source_adapter')\n",
    "        return self.ensemble_model\n",
    "\n",
    "# Test domain adaptation\n",
    "def test_domain_adaptation():\n",
    "    \"\"\"Test domain adaptation techniques\"\"\"\n",
    "    \n",
    "    print(\"\\n=== Domain Adaptation Testing ===\")\n",
    "    \n",
    "    # Single source adaptation\n",
    "    print(\"--- Single Source Adaptation ---\")\n",
    "    adapter = DomainAdapter(fine_tuned_model, target_classes=100)  # CIFAR-100 has 100 classes\n",
    "    adapted_model = adapter.layer_wise_adaptation(['top', 'middle'])\n",
    "    adapter.freeze_source_layers(freeze_ratio=0.7)\n",
    "    \n",
    "    print(f\"Adapted model parameters: {adapted_model.count_params():,}\")\n",
    "    \n",
    "    # Compile and test on CIFAR-100\n",
    "    adapted_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(0.0001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Prepare CIFAR-100 data (resize for ImageNet models)\n",
    "    x_train_100_resized = tf.image.resize(x_train_100[:800], (224, 224)).numpy()\n",
    "    x_test_100_resized = tf.image.resize(x_test_100[:200], (224, 224)).numpy()\n",
    "    \n",
    "    # Train adapted model\n",
    "    adaptation_history = adapted_model.fit(\n",
    "        x_train_100_resized, y_train_100[:800],\n",
    "        validation_data=(x_test_100_resized, y_test_100[:200]),\n",
    "        epochs=10,\n",
    "        batch_size=16,\n",
    "        verbose=0,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)]\n",
    "    )\n",
    "    \n",
    "    adaptation_acc = max(adaptation_history.history['val_accuracy'])\n",
    "    print(f\"Domain adaptation accuracy (CIFAR-100): {adaptation_acc:.4f}\")\n",
    "    \n",
    "    # Multi-source adaptation\n",
    "    print(\"\\n--- Multi-Source Adaptation ---\")\n",
    "    multi_adapter = MultiSourceAdapter(['ResNet50', 'MobileNetV2'])\n",
    "    ensemble_model = multi_adapter.create_ensemble_adapter(num_classes=10)\n",
    "    \n",
    "    print(f\"Ensemble model parameters: {ensemble_model.count_params():,}\")\n",
    "    \n",
    "    # Compile and test\n",
    "    ensemble_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Test on CIFAR-10\n",
    "    x_train_resized = tf.image.resize(x_train_10[:800], (224, 224)).numpy()\n",
    "    x_test_resized = tf.image.resize(x_test_10[:200], (224, 224)).numpy()\n",
    "    \n",
    "    ensemble_history = ensemble_model.fit(\n",
    "        x_train_resized, y_train_10[:800],\n",
    "        validation_data=(x_test_resized, y_test_10[:200]),\n",
    "        epochs=8,\n",
    "        batch_size=16,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    ensemble_acc = max(ensemble_history.history['val_accuracy'])\n",
    "    print(f\"Multi-source ensemble accuracy: {ensemble_acc:.4f}\")\n",
    "    \n",
    "    return adapted_model, ensemble_model\n",
    "\n",
    "adapted_model, ensemble_model = test_domain_adaptation()\n",
    "```\n",
    "\n",
    "## 5. Production Transfer Learning Pipeline\n",
    "\n",
    "```python\n",
    "# Production-ready transfer learning pipeline\n",
    "class ProductionTransferLearningPipeline:\n",
    "    \"\"\"Complete production pipeline for transfer learning\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.history = {}\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"Build transfer learning model based on config\"\"\"\n",
    "        \n",
    "        # Load base model\n",
    "        model_map = {\n",
    "            'ResNet50': applications.ResNet50,\n",
    "            'MobileNetV2': applications.MobileNetV2,\n",
    "            'EfficientNetB0': applications.EfficientNetB0,\n",
    "            'VGG16': applications.VGG16,\n",
    "            'InceptionV3': applications.InceptionV3\n",
    "        }\n",
    "        \n",
    "        base_model = model_map[self.config['base_model']](\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=self.config['input_shape']\n",
    "        )\n",
    "        \n",
    "        # Build complete model\n",
    "        inputs = tf.keras.Input(shape=self.config['input_shape'])\n",
    "        \n",
    "        # Preprocessing\n",
    "        x = applications.imagenet_utils.preprocess_input(inputs)\n",
    "        \n",
    "        # Base model\n",
    "        if self.config['strategy'] == 'feature_extraction':\n",
    "            base_model.trainable = False\n",
    "            x = base_model(x, training=False)\n",
    "        else:  # fine_tuning\n",
    "            base_model.trainable = True\n",
    "            x = base_model(x, training=True)\n",
    "        \n",
    "        # Custom head\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        \n",
    "        # Add custom layers based on config\n",
    "        for units in self.config['dense_layers']:\n",
    "            x = layers.Dense(units, activation='relu')(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = layers.Dropout(self.config['dropout_rate'])(x)\n",
    "        \n",
    "        # Final classification\n",
    "        predictions = layers.Dense(\n",
    "            self.config['num_classes'],\n",
    "            activation='softmax'\n",
    "        )(x)\n",
    "        \n",
    "        self.model = tf.keras.Model(inputs, predictions)\n",
    "        \n",
    "        # Freeze layers for fine-tuning strategy\n",
    "        if self.config['strategy'] == 'fine_tuning':\n",
    "            self._setup_layer_freezing()\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def _setup_layer_freezing(self):\n",
    "        \"\"\"Setup layer freezing for fine-tuning\"\"\"\n",
    "        \n",
    "        base_model = self.model.layers[2]  # Base model is typically the 3rd layer\n",
    "        total_layers = len(base_model.layers)\n",
    "        \n",
    "        if self.config['freeze_ratio'] > 0:\n",
    "            freeze_until = int(total_layers * self.config['freeze_ratio'])\n",
    "            \n",
    "            for layer in base_model.layers[:freeze_until]:\n",
    "                layer.trainable = False\n",
    "        \n",
    "        print(f\"Frozen {int(total_layers * self.config['freeze_ratio'])}/{total_layers} base layers\")\n",
    "    \n",
    "    def create_data_generators(self, train_data, val_data):\n",
    "        \"\"\"Create data generators with augmentation\"\"\"\n",
    "        \n",
    "        # Training data generator with augmentation\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rotation_range=self.config['augmentation']['rotation_range'],\n",
    "            width_shift_range=self.config['augmentation']['width_shift_range'],\n",
    "            height_shift_range=self.config['augmentation']['height_shift_range'],\n",
    "            horizontal_flip=self.config['augmentation']['horizontal_flip'],\n",
    "            zoom_range=self.config['augmentation']['zoom_range'],\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "        \n",
    "        # Validation data generator (no augmentation)\n",
    "        val_datagen = ImageDataGenerator()\n",
    "        \n",
    "        # Create generators\n",
    "        train_generator = train_datagen.flow(\n",
    "            train_data[0], train_data[1],\n",
    "            batch_size=self.config['batch_size']\n",
    "        )\n",
    "        \n",
    "        val_generator = val_datagen.flow(\n",
    "            val_data[0], val_data[1],\n",
    "            batch_size=self.config['batch_size']\n",
    "        )\n",
    "        \n",
    "        return train_generator, val_generator\n",
    "    \n",
    "    def train(self, train_data, val_data):\n",
    "        \"\"\"Execute training pipeline\"\"\"\n",
    "        \n",
    "        print(\"=== Starting Production Transfer Learning Pipeline ===\")\n",
    "        \n",
    "        # Build model\n",
    "        model = self.build_model()\n",
    "        \n",
    "        # Setup training\n",
    "        optimizer_map = {\n",
    "            'adam': tf.keras.optimizers.Adam,\n",
    "            'adamw': tf.keras.optimizers.AdamW,\n",
    "            'sgd': tf.keras.optimizers.SGD\n",
    "        }\n",
    "        \n",
    "        optimizer = optimizer_map[self.config['optimizer']](\n",
    "            learning_rate=self.config['learning_rate']\n",
    "        )\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy', 'top_k_categorical_accuracy']\n",
    "        )\n",
    "        \n",
    "        print(f\"Model built: {model.count_params():,} parameters\")\n",
    "        print(f\"Trainable: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}\")\n",
    "        \n",
    "        # Create data generators\n",
    "        train_gen, val_gen = self.create_data_generators(train_data, val_data)\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                'best_model.h5',\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                mode='max'\n",
    "            ),\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=self.config['patience'],\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.2,\n",
    "                patience=self.config['lr_patience'],\n",
    "                min_lr=1e-7\n",
    "            ),\n",
    "            tf.keras.callbacks.CSVLogger('training_log.csv')\n",
    "        ]\n",
    "        \n",
    "        # Training\n",
    "        steps_per_epoch = len(train_data[0]) // self.config['batch_size']\n",
    "        validation_steps = len(val_data[0]) // self.config['batch_size']\n",
    "        \n",
    "        self.history = model.fit(\n",
    "            train_gen,\n",
    "            epochs=self.config['epochs'],\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=val_gen,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Results\n",
    "        best_acc = max(self.history.history['val_accuracy'])\n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Best validation accuracy: {best_acc:.4f}\")\n",
    "        \n",
    "        return model, self.history\n",
    "    \n",
    "    def evaluate_and_save(self, test_data, save_path='production_model'):\n",
    "        \"\"\"Evaluate model and save for production\"\"\"\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Train model first\")\n",
    "        \n",
    "        # Final evaluation\n",
    "        test_loss, test_acc, test_top5 = self.model.evaluate(\n",
    "            test_data[0], test_data[1], verbose=0\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nFinal Test Results:\")\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"Test Top-5 Accuracy: {test_top5:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}\")\n",
    "        \n",
    "        # Save model in multiple formats\n",
    "        self.model.save(f'{save_path}.h5')  # Full model\n",
    "        self.model.save(save_path)  # SavedModel format\n",
    "        \n",
    "        # Convert to TFLite for mobile deployment\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        with open(f'{save_path}.tflite', 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        \n",
    "        print(f\"Model saved in multiple formats at {save_path}\")\n",
    "        \n",
    "        # Generate model summary\n",
    "        summary = {\n",
    "            'config': self.config,\n",
    "            'test_accuracy': float(test_acc),\n",
    "            'test_top5_accuracy': float(test_top5),\n",
    "            'test_loss': float(test_loss),\n",
    "            'total_parameters': int(self.model.count_params()),\n",
    "            'trainable_parameters': int(sum([tf.keras.backend.count_params(w) \n",
    "                                           for w in self.model.trainable_weights])),\n",
    "            'training_epochs': len(self.history.history['loss']),\n",
    "            'best_val_accuracy': float(max(self.history.history['val_accuracy']))\n",
    "        }\n",
    "        \n",
    "        import json\n",
    "        with open(f'{save_path}_summary.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Production configuration\n",
    "production_config = {\n",
    "    'base_model': 'MobileNetV2',\n",
    "    'input_shape': (224, 224, 3),\n",
    "    'num_classes': 10,\n",
    "    'strategy': 'fine_tuning',  # 'feature_extraction' or 'fine_tuning'\n",
    "    'freeze_ratio': 0.7,  # Freeze bottom 70% of layers\n",
    "    'dense_layers': [512, 256],  # Custom dense layers\n",
    "    'dropout_rate': 0.3,\n",
    "    'optimizer': 'adam',\n",
    "    'learning_rate': 0.0001,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 20,\n",
    "    'patience': 5,\n",
    "    'lr_patience': 3,\n",
    "    'augmentation': {\n",
    "        'rotation_range': 20,\n",
    "        'width_shift_range': 0.2,\n",
    "        'height_shift_range': 0.2,\n",
    "        'horizontal_flip': True,\n",
    "        'zoom_range': 0.2\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run production pipeline\n",
    "def run_production_pipeline():\n",
    "    \"\"\"Run complete production transfer learning pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n=== Running Production Pipeline ===\")\n",
    "    \n",
    "    # Prepare data\n",
    "    def prepare_production_data(x, y, target_size=(224, 224)):\n",
    "        x_resized = tf.image.resize(x, target_size).numpy()\n",
    "        return x_resized, y\n",
    "    \n",
    "    # Use larger subsets for production demo\n",
    "    train_data = prepare_production_data(x_train_10[:2000], y_train_10[:2000])\n",
    "    val_data = prepare_production_data(x_test_10[:400], y_test_10[:400])\n",
    "    test_data = prepare_production_data(x_test_10[400:600], y_test_10[400:600])\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = ProductionTransferLearningPipeline(production_config)\n",
    "    \n",
    "    # Train\n",
    "    model, history = pipeline.train(train_data, val_data)\n",
    "    \n",
    "    # Evaluate and save\n",
    "    summary = pipeline.evaluate_and_save(test_data, 'cifar10_transfer_model')\n",
    "    \n",
    "    return pipeline, summary\n",
    "\n",
    "# Execute production pipeline\n",
    "production_pipeline, production_summary = run_production_pipeline()\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Production pipeline completed!\")\n",
    "print(f\"ðŸ“Š Final test accuracy: {production_summary['test_accuracy']:.4f}\")\n",
    "print(f\"ðŸ“ˆ Model efficiency: {production_summary['trainable_parameters']:,} trainable params\")\n",
    "```\n",
    "\n",
    "## 6. Transfer Learning Best Practices and Comparison\n",
    "\n",
    "```python\n",
    "# Comprehensive comparison of transfer learning strategies\n",
    "class TransferLearningComparison:\n",
    "    \"\"\"Compare different transfer learning approaches\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def compare_strategies(self, train_data, val_data, strategies, epochs=8):\n",
    "        \"\"\"Compare different transfer learning strategies\"\"\"\n",
    "        \n",
    "        print(\"=== Transfer Learning Strategy Comparison ===\")\n",
    "        \n",
    "        for strategy_name, config in strategies.items():\n",
    "            print(f\"\\nTesting {strategy_name}...\")\n",
    "            \n",
    "            # Build model based on strategy\n",
    "            if config['type'] == 'feature_extraction':\n",
    "                model = self._build_feature_extraction_model(config)\n",
    "            elif config['type'] == 'fine_tuning':\n",
    "                model = self._build_fine_tuning_model(config)\n",
    "            elif config['type'] == 'from_scratch':\n",
    "                model = self._build_from_scratch_model(config)\n",
    "            \n",
    "            # Compile\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(config['lr']),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            # Train\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            history = model.fit(\n",
    "                train_data[0], train_data[1],\n",
    "                validation_data=val_data,\n",
    "                epochs=epochs,\n",
    "                batch_size=32,\n",
    "                verbose=0,\n",
    "                callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)]\n",
    "            )\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Store results\n",
    "            self.results[strategy_name] = {\n",
    "                'best_val_acc': max(history.history['val_accuracy']),\n",
    "                'final_val_acc': history.history['val_accuracy'][-1],\n",
    "                'training_time': training_time,\n",
    "                'parameters': model.count_params(),\n",
    "                'trainable_params': sum([tf.keras.backend.count_params(w) \n",
    "                                       for w in model.trainable_weights]),\n",
    "                'epochs_trained': len(history.history['loss'])\n",
    "            }\n",
    "            \n",
    "            print(f\"  Best accuracy: {self.results[strategy_name]['best_val_acc']:.4f}\")\n",
    "            print(f\"  Training time: {training_time:.2f}s\")\n",
    "            print(f\"  Trainable params: {self.results[strategy_name]['trainable_params']:,}\")\n",
    "    \n",
    "    def _build_feature_extraction_model(self, config):\n",
    "        \"\"\"Build feature extraction model\"\"\"\n",
    "        base_model = applications.MobileNetV2(\n",
    "            weights='imagenet', include_top=False, \n",
    "            input_shape=(224, 224, 3), pooling='avg'\n",
    "        )\n",
    "        base_model.trainable = False\n",
    "        \n",
    "        inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "        x = applications.imagenet_utils.preprocess_input(inputs)\n",
    "        x = base_model(x, training=False)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        outputs = layers.Dense(10, activation='softmax')(x)\n",
    "        \n",
    "        return tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    def _build_fine_tuning_model(self, config):\n",
    "        \"\"\"Build fine-tuning model\"\"\"\n",
    "        base_model = applications.MobileNetV2(\n",
    "            weights='imagenet', include_top=False, input_shape=(224, 224, 3)\n",
    "        )\n",
    "        \n",
    "        # Freeze bottom layers\n",
    "        for layer in base_model.layers[:-20]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "        x = applications.imagenet_utils.preprocess_input(inputs)\n",
    "        x = base_model(x, training=True)\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        outputs = layers.Dense(10, activation='softmax')(x)\n",
    "        \n",
    "        return tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    def _build_from_scratch_model(self, config):\n",
    "        \"\"\"Build model from scratch\"\"\"\n",
    "        model = tf.keras.Sequential([\n",
    "            layers.Conv2D(32, 3, activation='relu', input_shape=(224, 224, 3)),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(64, 3, activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(128, 3, activation='relu'),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.Dense(512, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    def print_comparison(self):\n",
    "        \"\"\"Print detailed comparison\"\"\"\n",
    "        \n",
    "        print(\"\\n=== Transfer Learning Comparison Results ===\")\n",
    "        print(f\"{'Strategy':<20} {'Best Acc':<10} {'Train Time':<12} {'Trainable Params':<15} {'Efficiency':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for name, result in self.results.items():\n",
    "            efficiency = result['best_val_acc'] / (result['trainable_params'] / 1000000)  # Acc per million params\n",
    "            print(f\"{name:<20} {result['best_val_acc']:<10.4f} \"\n",
    "                  f\"{result['training_time']:<12.2f} {result['trainable_params']:<15,} \"\n",
    "                  f\"{efficiency:<10.2f}\")\n",
    "        \n",
    "        # Best in each category\n",
    "        print(\"\\n=== Best Performers ===\")\n",
    "        \n",
    "        best_acc = max(self.results.items(), key=lambda x: x[1]['best_val_acc'])\n",
    "        fastest = min(self.results.items(), key=lambda x: x[1]['training_time'])\n",
    "        most_efficient = min(self.results.items(), key=lambda x: x[1]['trainable_params'])\n",
    "        \n",
    "        print(f\"Best Accuracy: {best_acc[0]} ({best_acc[1]['best_val_acc']:.4f})\")\n",
    "        print(f\"Fastest Training: {fastest[0]} ({fastest[1]['training_time']:.2f}s)\")\n",
    "        print(f\"Most Parameter Efficient: {most_efficient[0]} ({most_efficient[1]['trainable_params']:,} params)\")\n",
    "\n",
    "# Define strategies to compare\n",
    "comparison_strategies = {\n",
    "    'Feature_Extraction': {\n",
    "        'type': 'feature_extraction',\n",
    "        'lr': 0.001\n",
    "    },\n",
    "    'Fine_Tuning_Conservative': {\n",
    "        'type': 'fine_tuning',\n",
    "        'lr': 0.0001\n",
    "    },\n",
    "    'Fine_Tuning_Aggressive': {\n",
    "        'type': 'fine_tuning', \n",
    "        'lr': 0.001\n",
    "    },\n",
    "    'From_Scratch': {\n",
    "        'type': 'from_scratch',\n",
    "        'lr': 0.001\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run comparison\n",
    "comparison = TransferLearningComparison()\n",
    "\n",
    "# Prepare comparison data (smaller subset for speed)\n",
    "comp_train = (\n",
    "    tf.image.resize(x_train_10[:800], (224, 224)).numpy(),\n",
    "    y_train_10[:800]\n",
    ")\n",
    "comp_val = (\n",
    "    tf.image.resize(x_test_10[:200], (224, 224)).numpy(),\n",
    "    y_test_10[:200]\n",
    ")\n",
    "\n",
    "comparison.compare_strategies(comp_train, comp_val, comparison_strategies, epochs=6)\n",
    "comparison.print_comparison()\n",
    "\n",
    "# Transfer Learning Best Practices Summary\n",
    "def print_transfer_learning_best_practices():\n",
    "    \"\"\"Print comprehensive transfer learning best practices\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRANSFER LEARNING BEST PRACTICES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    practices = {\n",
    "        \"ðŸŽ¯ STRATEGY SELECTION\": [\n",
    "            \"Use feature extraction for small datasets (<1000 samples per class)\",\n",
    "            \"Apply fine-tuning for medium datasets (1000-10000 samples per class)\",\n",
    "            \"Consider from-scratch training for very large datasets (>100k samples)\",\n",
    "            \"Start with feature extraction, then progress to fine-tuning\",\n",
    "            \"Use ensemble of multiple pre-trained models for best accuracy\"\n",
    "        ],\n",
    "        \n",
    "        \"ðŸ—ï¸ MODEL ARCHITECTURE\": [\n",
    "            \"Choose pre-trained model based on your domain similarity to ImageNet\",\n",
    "            \"Use global average pooling instead of flattening before classifier\",\n",
    "            \"Add batch normalization and dropout in custom layers\",\n",
    "            \"Keep classifier simple (1-2 dense layers) to avoid overfitting\",\n",
    "            \"Consider the efficiency vs accuracy trade-off for deployment\"\n",
    "        ],\n",
    "        \n",
    "        \"â„ï¸ LAYER FREEZING STRATEGIES\": [\n",
    "            \"Freeze all layers for feature extraction\",\n",
    "            \"Unfreeze top 10-25% layers for fine-tuning\",\n",
    "            \"Use progressive unfreezing: start conservative, gradually unfreeze\",\n",
    "            \"Never unfreeze batch normalization layers in early stages\",\n",
    "            \"Consider layer-wise discriminative learning rates\"\n",
    "        ],\n",
    "        \n",
    "        \"ðŸ“š LEARNING RATE GUIDELINES\": [\n",
    "            \"Use 10x lower LR for pre-trained layers vs new layers\",\n",
    "            \"Start with 1e-4 for fine-tuning, 1e-3 for feature extraction\",\n",
    "            \"Apply learning rate warmup for the first few epochs\",\n",
    "            \"Use cosine decay or reduce-on-plateau scheduling\",\n",
    "            \"Monitor gradient norms to detect learning rate issues\"\n",
    "        ],\n",
    "        \n",
    "        \"ðŸ’¾ DATA HANDLING\": [\n",
    "            \"Match input preprocessing to pre-trained model requirements\",\n",
    "            \"Apply data augmentation carefully - avoid changing domain too much\",\n",
    "            \"Use progressive resizing: start small, gradually increase size\",\n",
    "            \"Consider domain-specific augmentations over generic ones\",\n",
    "            \"Maintain original aspect ratios when possible\"\n",
    "        ],\n",
    "        \n",
    "        \"ðŸ”„ TRAINING PROCESS\": [\n",
    "            \"Train in stages: classifier â†’ top layers â†’ more layers\",\n",
    "            \"Use early stopping and model checkpointing\",\n",
    "            \"Monitor both training and validation metrics closely\",\n",
    "            \"Save model at each stage for rollback capability\",\n",
    "            \"Apply regularization (dropout, weight decay) appropriately\"\n",
    "        ],\n",
    "        \n",
    "        \"ðŸŽª ADVANCED TECHNIQUES\": [\n",
    "            \"Use mixup or cutmix for improved generalization\",\n",
    "            \"Apply label smoothing for better calibration\",\n",
    "            \"Consider pseudo-labeling for semi-supervised learning\",\n",
    "            \"Use knowledge distillation from larger to smaller models\",\n",
    "            \"Implement test-time augmentation for better predictions\"\n",
    "        ],\n",
    "        \n",
    "        \"ðŸ“± PRODUCTION DEPLOYMENT\": [\n",
    "            \"Quantize models for mobile/edge deployment\",\n",
    "            \"Use TensorFlow Lite for mobile optimization\",\n",
    "            \"Consider model pruning for size reduction\",\n",
    "            \"Profile inference speed vs accuracy trade-offs\",\n",
    "            \"Implement proper model versioning and A/B testing\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in practices.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        for item in items:\n",
    "            print(f\"  â€¢ {item}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DECISION FLOWCHART\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    flowchart = {\n",
    "        \"Small Dataset (<1k/class)\": \"Feature Extraction + Data Augmentation\",\n",
    "        \"Medium Dataset (1k-10k/class)\": \"Fine-tuning with Progressive Unfreezing\", \n",
    "        \"Large Dataset (>10k/class)\": \"Full Fine-tuning or Training from Scratch\",\n",
    "        \"Similar Domain to ImageNet\": \"Any pre-trained CNN (ResNet, EfficientNet)\",\n",
    "        \"Very Different Domain\": \"Generic features + Domain Adaptation\",\n",
    "        \"Mobile/Edge Deployment\": \"MobileNet, EfficientNet-B0 with quantization\",\n",
    "        \"High Accuracy Required\": \"EfficientNet-B4+, Ensemble methods\",\n",
    "        \"Fast Training Required\": \"Feature Extraction with MobileNet\",\n",
    "        \"Limited Compute\": \"MobileNet + Feature Extraction\"\n",
    "    }\n",
    "    \n",
    "    for scenario, recommendation in flowchart.items():\n",
    "        print(f\"{scenario:<30}: {recommendation}\")\n",
    "\n",
    "print_transfer_learning_best_practices()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**File Location:** `notebooks/03_computer_vision/08_transfer_learning_applications.ipynb`\n",
    "\n",
    "This comprehensive notebook mastered transfer learning with tf.keras.applications:\n",
    "\n",
    "### Core Transfer Learning Strategies:\n",
    "1. **Feature Extraction**: Freeze pre-trained model, train only classifier\n",
    "2. **Fine-tuning**: Unfreeze top layers, use discriminative learning rates  \n",
    "3. **Progressive Unfreezing**: Gradually unfreeze layers during training\n",
    "4. **Domain Adaptation**: Adapt models to different domains/datasets\n",
    "\n",
    "### Pre-trained Models Mastered:\n",
    "- **Classic CNNs**: VGG16/19, ResNet50/101/152, InceptionV3\n",
    "- **Efficient Models**: MobileNet, MobileNetV2, EfficientNet series\n",
    "- **Specialized**: Xception, DenseNet, InceptionResNetV2\n",
    "- **Multi-source**: Ensemble approaches combining multiple models\n",
    "\n",
    "### Advanced Techniques Implemented:\n",
    "- **Progressive Unfreezing**: Stage-wise layer unfreezing\n",
    "- **Discriminative Learning Rates**: Different LR for different layers\n",
    "- **Domain Adaptation**: Layer-wise adaptation for new domains\n",
    "- **Multi-source Fusion**: Combining multiple pre-trained models\n",
    "- **Production Pipeline**: Complete automated transfer learning system\n",
    "\n",
    "### Key Performance Insights:\n",
    "- **Feature extraction** works well for small datasets (<1k samples/class)\n",
    "- **Fine-tuning** provides 5-15% accuracy improvement over feature extraction\n",
    "- **Progressive unfreezing** prevents catastrophic forgetting\n",
    "- **MobileNet** offers best efficiency/accuracy trade-off for mobile\n",
    "- **EfficientNet** achieves state-of-the-art accuracy with reasonable compute\n",
    "\n",
    "### Production Best Practices:\n",
    "- Match preprocessing to pre-trained model requirements\n",
    "- Use progressive training strategies to prevent overfitting\n",
    "- Apply appropriate data augmentation for target domain\n",
    "- Implement proper model versioning and deployment pipelines\n",
    "- Consider inference speed vs accuracy trade-offs\n",
    "\n",
    "### Strategic Decision Framework:\n",
    "- **Small data**: Feature extraction + aggressive augmentation\n",
    "- **Medium data**: Fine-tuning with progressive unfreezing\n",
    "- **Large data**: Full fine-tuning or training from scratch\n",
    "- **Mobile deployment**: MobileNet + quantization\n",
    "- **High accuracy**: EfficientNet + ensemble methods\n",
    "\n",
    "### Next Steps:\n",
    "- Apply to semantic segmentation tasks (U-Net, DeepLab)\n",
    "- Implement object detection with transfer learning\n",
    "- Explore vision transformers and hybrid approaches\n",
    "- Deploy optimized models to production environments\n",
    "\n",
    "This foundation enables leveraging pre-trained models effectively for any computer vision task with minimal training data and compute resources!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
