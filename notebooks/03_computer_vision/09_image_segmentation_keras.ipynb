{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 image segmentation keras\n",
    "**Location: TensorVerseHub/notebooks/03_computer_vision/09_image_segmentation_keras.ipynb**\n",
    "\n",
    "TODO: Implement comprehensive TensorFlow + tf.keras learning content.\n",
    "\n",
    "## Learning Objectives\n",
    "- TODO: Define specific learning objectives\n",
    "- TODO: List key TensorFlow concepts covered\n",
    "- TODO: Outline tf.keras integration points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "# TODO: Add comprehensive implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation with tf.keras\n",
    "\n",
    "**File Location:** `notebooks/03_computer_vision/09_image_segmentation_keras.ipynb`\n",
    "\n",
    "Master image segmentation using tf.keras Functional API, implementing U-Net, DeepLab, and custom segmentation architectures. Build semantic segmentation, instance segmentation, and medical imaging models with advanced loss functions and evaluation metrics.\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement U-Net architecture with tf.keras Functional API\n",
    "- Build DeepLab and advanced segmentation models\n",
    "- Master segmentation-specific loss functions and metrics\n",
    "- Handle multi-class and binary segmentation tasks\n",
    "- Create custom segmentation datasets and data augmentation\n",
    "- Deploy segmentation models for real-world applications\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Segmentation Fundamentals and Data Preparation\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models, utils\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create synthetic segmentation dataset\n",
    "def create_synthetic_segmentation_data(num_samples=1000, image_size=128):\n",
    "    \"\"\"Create synthetic dataset for segmentation experiments\"\"\"\n",
    "    \n",
    "    print(\"Creating synthetic segmentation dataset...\")\n",
    "    \n",
    "    images = []\n",
    "    masks = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Create synthetic image with geometric shapes\n",
    "        img = np.zeros((image_size, image_size, 3), dtype=np.uint8)\n",
    "        mask = np.zeros((image_size, image_size), dtype=np.uint8)\n",
    "        \n",
    "        # Add background noise\n",
    "        noise = np.random.randint(0, 50, (image_size, image_size, 3), dtype=np.uint8)\n",
    "        img = img + noise\n",
    "        \n",
    "        # Add random shapes\n",
    "        num_shapes = np.random.randint(1, 4)\n",
    "        \n",
    "        for shape_id in range(1, num_shapes + 1):\n",
    "            shape_type = np.random.choice(['circle', 'rectangle', 'triangle'])\n",
    "            \n",
    "            if shape_type == 'circle':\n",
    "                center = (np.random.randint(20, image_size-20), np.random.randint(20, image_size-20))\n",
    "                radius = np.random.randint(10, 30)\n",
    "                color = np.random.randint(100, 255, 3).tolist()\n",
    "                \n",
    "                cv2.circle(img, center, radius, color, -1)\n",
    "                cv2.circle(mask, center, radius, shape_id, -1)\n",
    "                \n",
    "            elif shape_type == 'rectangle':\n",
    "                pt1 = (np.random.randint(0, image_size//2), np.random.randint(0, image_size//2))\n",
    "                pt2 = (pt1[0] + np.random.randint(20, 50), pt1[1] + np.random.randint(20, 50))\n",
    "                color = np.random.randint(100, 255, 3).tolist()\n",
    "                \n",
    "                cv2.rectangle(img, pt1, pt2, color, -1)\n",
    "                cv2.rectangle(mask, pt1, pt2, shape_id, -1)\n",
    "        \n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "    \n",
    "    images = np.array(images, dtype=np.float32) / 255.0\n",
    "    masks = np.array(masks, dtype=np.uint8)\n",
    "    \n",
    "    print(f\"Created dataset: {images.shape} images, {masks.shape} masks\")\n",
    "    print(f\"Unique mask values: {np.unique(masks)}\")\n",
    "    \n",
    "    return images, masks\n",
    "\n",
    "# Create datasets\n",
    "images, masks = create_synthetic_segmentation_data(num_samples=800, image_size=128)\n",
    "\n",
    "# Split into train/validation/test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(images, masks, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "# Visualization function\n",
    "def visualize_segmentation(images, masks, predictions=None, num_samples=4):\n",
    "    \"\"\"Visualize segmentation results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3 if predictions is None else 4, figsize=(15, num_samples*3))\n",
    "    \n",
    "    for i in range(min(num_samples, len(images))):\n",
    "        # Original image\n",
    "        axes[i, 0].imshow(images[i])\n",
    "        axes[i, 0].set_title('Original Image')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Ground truth mask\n",
    "        axes[i, 1].imshow(masks[i], cmap='viridis')\n",
    "        axes[i, 1].set_title('Ground Truth Mask')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Overlay\n",
    "        overlay = images[i].copy()\n",
    "        colored_mask = plt.cm.viridis(masks[i] / masks[i].max())[:, :, :3]\n",
    "        overlay = 0.7 * overlay + 0.3 * colored_mask\n",
    "        axes[i, 2].imshow(overlay)\n",
    "        axes[i, 2].set_title('Overlay')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Predictions if provided\n",
    "        if predictions is not None:\n",
    "            pred_mask = np.argmax(predictions[i], axis=-1)\n",
    "            axes[i, 3].imshow(pred_mask, cmap='viridis')\n",
    "            axes[i, 3].set_title('Prediction')\n",
    "            axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize sample data\n",
    "visualize_segmentation(X_train, y_train, num_samples=3)\n",
    "\n",
    "# Data augmentation for segmentation\n",
    "class SegmentationDataGenerator:\n",
    "    \"\"\"Custom data generator for segmentation with synchronized augmentations\"\"\"\n",
    "    \n",
    "    def __init__(self, rotation_range=20, width_shift_range=0.1, height_shift_range=0.1,\n",
    "                 zoom_range=0.1, horizontal_flip=True, brightness_range=None):\n",
    "        self.rotation_range = rotation_range\n",
    "        self.width_shift_range = width_shift_range\n",
    "        self.height_shift_range = height_shift_range\n",
    "        self.zoom_range = zoom_range\n",
    "        self.horizontal_flip = horizontal_flip\n",
    "        self.brightness_range = brightness_range or [0.8, 1.2]\n",
    "    \n",
    "    def augment(self, image, mask):\n",
    "        \"\"\"Apply synchronized augmentations to image and mask\"\"\"\n",
    "        \n",
    "        # Convert to tensor for TensorFlow operations\n",
    "        image_tensor = tf.constant(image, dtype=tf.float32)\n",
    "        mask_tensor = tf.constant(mask, dtype=tf.float32)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        image_tensor = tf.expand_dims(image_tensor, 0)\n",
    "        mask_tensor = tf.expand_dims(tf.expand_dims(mask_tensor, 0), -1)\n",
    "        \n",
    "        # Random horizontal flip\n",
    "        if self.horizontal_flip and tf.random.uniform([]) > 0.5:\n",
    "            image_tensor = tf.image.flip_left_right(image_tensor)\n",
    "            mask_tensor = tf.image.flip_left_right(mask_tensor)\n",
    "        \n",
    "        # Random rotation\n",
    "        if self.rotation_range > 0:\n",
    "            angle = tf.random.uniform([], -self.rotation_range, self.rotation_range) * np.pi / 180\n",
    "            image_tensor = tf.image.rot90(image_tensor, k=tf.cast(angle / (np.pi/2), tf.int32))\n",
    "            mask_tensor = tf.image.rot90(mask_tensor, k=tf.cast(angle / (np.pi/2), tf.int32))\n",
    "        \n",
    "        # Brightness adjustment (only for image)\n",
    "        if self.brightness_range:\n",
    "            brightness_factor = tf.random.uniform([], self.brightness_range[0], self.brightness_range[1])\n",
    "            image_tensor = tf.image.adjust_brightness(image_tensor, brightness_factor - 1.0)\n",
    "            image_tensor = tf.clip_by_value(image_tensor, 0.0, 1.0)\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        image_tensor = tf.squeeze(image_tensor, 0)\n",
    "        mask_tensor = tf.squeeze(mask_tensor, [0, -1])\n",
    "        \n",
    "        return image_tensor.numpy(), mask_tensor.numpy().astype(np.uint8)\n",
    "    \n",
    "    def flow(self, images, masks, batch_size=32):\n",
    "        \"\"\"Generate augmented batches\"\"\"\n",
    "        \n",
    "        num_samples = len(images)\n",
    "        indices = np.arange(num_samples)\n",
    "        \n",
    "        while True:\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for start_idx in range(0, num_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, num_samples)\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                batch_images = []\n",
    "                batch_masks = []\n",
    "                \n",
    "                for idx in batch_indices:\n",
    "                    img, mask = self.augment(images[idx], masks[idx])\n",
    "                    batch_images.append(img)\n",
    "                    batch_masks.append(mask)\n",
    "                \n",
    "                yield np.array(batch_images), np.array(batch_masks)\n",
    "\n",
    "# Test data augmentation\n",
    "augmenter = SegmentationDataGenerator()\n",
    "aug_gen = augmenter.flow(X_train[:8], y_train[:8], batch_size=4)\n",
    "aug_images, aug_masks = next(aug_gen)\n",
    "\n",
    "print(\"Testing data augmentation:\")\n",
    "visualize_segmentation(aug_images, aug_masks, num_samples=2)\n",
    "```\n",
    "\n",
    "## 2. U-Net Architecture Implementation\n",
    "\n",
    "```python\n",
    "# U-Net implementation with tf.keras Functional API\n",
    "def conv_block(inputs, filters, dropout_rate=0.0):\n",
    "    \"\"\"Convolutional block for U-Net\"\"\"\n",
    "    \n",
    "    x = layers.Conv2D(filters, 3, activation='relu', padding='same',\n",
    "                      kernel_initializer='he_normal')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    if dropout_rate > 0:\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters, 3, activation='relu', padding='same',\n",
    "                      kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def encoder_block(inputs, filters, pool=True, dropout_rate=0.0):\n",
    "    \"\"\"Encoder block with optional pooling\"\"\"\n",
    "    \n",
    "    x = conv_block(inputs, filters, dropout_rate)\n",
    "    \n",
    "    if pool:\n",
    "        pooled = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        return x, pooled\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def decoder_block(inputs, skip_features, filters, dropout_rate=0.0):\n",
    "    \"\"\"Decoder block with skip connections\"\"\"\n",
    "    \n",
    "    # Upsampling\n",
    "    x = layers.Conv2DTranspose(filters, (2, 2), strides=2, padding='same')(inputs)\n",
    "    \n",
    "    # Concatenate with skip connection\n",
    "    x = layers.Concatenate()([x, skip_features])\n",
    "    \n",
    "    # Convolutional block\n",
    "    x = conv_block(x, filters, dropout_rate)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_unet(input_shape, num_classes, filters=64, dropout_rate=0.1):\n",
    "    \"\"\"Build U-Net model for segmentation\"\"\"\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder path (Contracting path)\n",
    "    e1, p1 = encoder_block(inputs, filters, pool=True, dropout_rate=dropout_rate)\n",
    "    e2, p2 = encoder_block(p1, filters * 2, pool=True, dropout_rate=dropout_rate)\n",
    "    e3, p3 = encoder_block(p2, filters * 4, pool=True, dropout_rate=dropout_rate)\n",
    "    e4, p4 = encoder_block(p3, filters * 8, pool=True, dropout_rate=dropout_rate)\n",
    "    \n",
    "    # Bottleneck\n",
    "    bottleneck = encoder_block(p4, filters * 16, pool=False, dropout_rate=dropout_rate)\n",
    "    \n",
    "    # Decoder path (Expanding path)\n",
    "    d4 = decoder_block(bottleneck, e4, filters * 8, dropout_rate=dropout_rate)\n",
    "    d3 = decoder_block(d4, e3, filters * 4, dropout_rate=dropout_rate)\n",
    "    d2 = decoder_block(d3, e2, filters * 2, dropout_rate=dropout_rate)\n",
    "    d1 = decoder_block(d2, e1, filters, dropout_rate=dropout_rate)\n",
    "    \n",
    "    # Output layer\n",
    "    if num_classes == 1:\n",
    "        activation = 'sigmoid'\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "    \n",
    "    outputs = layers.Conv2D(num_classes, 1, activation=activation, padding='same')(d1)\n",
    "    \n",
    "    model = models.Model(inputs, outputs, name='U-Net')\n",
    "    return model\n",
    "\n",
    "# Attention U-Net implementation\n",
    "def attention_gate(gate_signal, skip_connection, filters):\n",
    "    \"\"\"Attention gate for focusing on relevant features\"\"\"\n",
    "    \n",
    "    # Gate signal processing\n",
    "    gate = layers.Conv2D(filters, 1, padding='same')(gate_signal)\n",
    "    gate = layers.BatchNormalization()(gate)\n",
    "    \n",
    "    # Skip connection processing  \n",
    "    skip = layers.Conv2D(filters, 1, padding='same')(skip_connection)\n",
    "    skip = layers.BatchNormalization()(skip)\n",
    "    \n",
    "    # Attention mechanism\n",
    "    attention = layers.Add()([gate, skip])\n",
    "    attention = layers.Activation('relu')(attention)\n",
    "    attention = layers.Conv2D(1, 1, activation='sigmoid', padding='same')(attention)\n",
    "    \n",
    "    # Apply attention\n",
    "    attended = layers.Multiply()([skip_connection, attention])\n",
    "    \n",
    "    return attended\n",
    "\n",
    "def build_attention_unet(input_shape, num_classes, filters=64):\n",
    "    \"\"\"Build Attention U-Net model\"\"\"\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    e1, p1 = encoder_block(inputs, filters, pool=True)\n",
    "    e2, p2 = encoder_block(p1, filters * 2, pool=True)\n",
    "    e3, p3 = encoder_block(p2, filters * 4, pool=True)\n",
    "    e4, p4 = encoder_block(p3, filters * 8, pool=True)\n",
    "    \n",
    "    # Bottleneck\n",
    "    bottleneck = encoder_block(p4, filters * 16, pool=False)\n",
    "    \n",
    "    # Decoder with attention gates\n",
    "    # Upsampling bottleneck\n",
    "    up4 = layers.Conv2DTranspose(filters * 8, (2, 2), strides=2, padding='same')(bottleneck)\n",
    "    att4 = attention_gate(up4, e4, filters * 4)\n",
    "    merge4 = layers.Concatenate()([up4, att4])\n",
    "    d4 = conv_block(merge4, filters * 8)\n",
    "    \n",
    "    up3 = layers.Conv2DTranspose(filters * 4, (2, 2), strides=2, padding='same')(d4)\n",
    "    att3 = attention_gate(up3, e3, filters * 2)\n",
    "    merge3 = layers.Concatenate()([up3, att3])\n",
    "    d3 = conv_block(merge3, filters * 4)\n",
    "    \n",
    "    up2 = layers.Conv2DTranspose(filters * 2, (2, 2), strides=2, padding='same')(d3)\n",
    "    att2 = attention_gate(up2, e2, filters)\n",
    "    merge2 = layers.Concatenate()([up2, att2])\n",
    "    d2 = conv_block(merge2, filters * 2)\n",
    "    \n",
    "    up1 = layers.Conv2DTranspose(filters, (2, 2), strides=2, padding='same')(d2)\n",
    "    att1 = attention_gate(up1, e1, filters // 2)\n",
    "    merge1 = layers.Concatenate()([up1, att1])\n",
    "    d1 = conv_block(merge1, filters)\n",
    "    \n",
    "    # Output\n",
    "    outputs = layers.Conv2D(num_classes, 1, activation='softmax', padding='same')(d1)\n",
    "    \n",
    "    model = models.Model(inputs, outputs, name='Attention-U-Net')\n",
    "    return model\n",
    "\n",
    "# Build and test U-Net models\n",
    "print(\"=== Building U-Net Models ===\")\n",
    "\n",
    "# Standard U-Net\n",
    "num_classes = len(np.unique(y_train))\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "unet_model = build_unet(input_shape, num_classes, filters=32, dropout_rate=0.1)\n",
    "print(f\"U-Net model built: {unet_model.count_params():,} parameters\")\n",
    "\n",
    "# Attention U-Net\n",
    "attention_unet_model = build_attention_unet(input_shape, num_classes, filters=32)\n",
    "print(f\"Attention U-Net model built: {attention_unet_model.count_params():,} parameters\")\n",
    "\n",
    "# Display model architectures\n",
    "utils.plot_model(unet_model, show_shapes=True, show_layer_names=True, \n",
    "                 to_file='unet_architecture.png', dpi=150)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(plt.imread('unet_architecture.png'))\n",
    "plt.axis('off')\n",
    "plt.title('U-Net Architecture')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## 3. DeepLab and Advanced Segmentation Models\n",
    "\n",
    "```python\n",
    "# DeepLab v3+ implementation\n",
    "def dilated_conv_block(inputs, filters, dilation_rate, dropout_rate=0.1):\n",
    "    \"\"\"Dilated convolution block for DeepLab\"\"\"\n",
    "    \n",
    "    x = layers.Conv2D(filters, 3, padding='same', dilation_rate=dilation_rate,\n",
    "                      kernel_initializer='he_normal')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    if dropout_rate > 0:\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def atrous_spatial_pyramid_pooling(inputs, filters=256):\n",
    "    \"\"\"ASPP module for DeepLab\"\"\"\n",
    "    \n",
    "    # Image pooling\n",
    "    shape = tf.shape(inputs)\n",
    "    pool = layers.GlobalAveragePooling2D()(inputs)\n",
    "    pool = layers.Reshape((1, 1, -1))(pool)\n",
    "    pool = layers.Conv2D(filters, 1, activation='relu')(pool)\n",
    "    pool = tf.image.resize(pool, (shape[1], shape[2]))\n",
    "    \n",
    "    # Dilated convolutions with different rates\n",
    "    conv1x1 = layers.Conv2D(filters, 1, activation='relu', padding='same')(inputs)\n",
    "    conv3x3_1 = dilated_conv_block(inputs, filters, dilation_rate=6)\n",
    "    conv3x3_2 = dilated_conv_block(inputs, filters, dilation_rate=12)\n",
    "    conv3x3_3 = dilated_conv_block(inputs, filters, dilation_rate=18)\n",
    "    \n",
    "    # Concatenate all features\n",
    "    concat = layers.Concatenate()([pool, conv1x1, conv3x3_1, conv3x3_2, conv3x3_3])\n",
    "    \n",
    "    # Final 1x1 convolution\n",
    "    output = layers.Conv2D(filters, 1, activation='relu', padding='same')(concat)\n",
    "    output = layers.Dropout(0.1)(output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def build_deeplab_v3plus(input_shape, num_classes, backbone='mobilenetv2'):\n",
    "    \"\"\"Build DeepLab v3+ model\"\"\"\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Backbone encoder\n",
    "    if backbone == 'mobilenetv2':\n",
    "        base_model = tf.keras.applications.MobileNetV2(\n",
    "            input_tensor=inputs, weights='imagenet', include_top=False\n",
    "        )\n",
    "        \n",
    "        # Extract features at different scales\n",
    "        low_level_features = base_model.get_layer('block_1_expand_relu').output  # 1/4\n",
    "        high_level_features = base_model.get_layer('out_relu').output  # 1/32\n",
    "        \n",
    "    else:  # Custom lightweight encoder for our synthetic data\n",
    "        # Encoder\n",
    "        x = layers.Conv2D(32, 3, strides=2, padding='same', activation='relu')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        low_level_features = x  # 1/2\n",
    "        \n",
    "        x = layers.Conv2D(64, 3, strides=2, padding='same', activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Conv2D(128, 3, strides=2, padding='same', activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Conv2D(256, 3, strides=2, padding='same', activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        high_level_features = x  # 1/16\n",
    "    \n",
    "    # ASPP module\n",
    "    aspp_features = atrous_spatial_pyramid_pooling(high_level_features)\n",
    "    \n",
    "    # Upsample ASPP features\n",
    "    aspp_upsampled = tf.image.resize(aspp_features, \n",
    "                                   tf.shape(low_level_features)[1:3])\n",
    "    \n",
    "    # Process low-level features\n",
    "    low_level_processed = layers.Conv2D(48, 1, activation='relu', \n",
    "                                      padding='same')(low_level_features)\n",
    "    \n",
    "    # Combine features\n",
    "    combined = layers.Concatenate()([aspp_upsampled, low_level_processed])\n",
    "    \n",
    "    # Decoder\n",
    "    x = layers.Conv2D(256, 3, activation='relu', padding='same')(combined)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    # Final upsampling and output\n",
    "    x = tf.image.resize(x, tf.shape(inputs)[1:3])\n",
    "    outputs = layers.Conv2D(num_classes, 1, activation='softmax', \n",
    "                          padding='same')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs, name='DeepLab-v3+')\n",
    "    return model\n",
    "\n",
    "# PSPNet (Pyramid Scene Parsing Network)\n",
    "def pyramid_pooling_module(inputs, pool_sizes=[1, 2, 3, 6], filters=512):\n",
    "    \"\"\"Pyramid pooling module\"\"\"\n",
    "    \n",
    "    feature_maps = []\n",
    "    \n",
    "    for pool_size in pool_sizes:\n",
    "        # Global average pooling\n",
    "        pooled = layers.AveragePooling2D(pool_size=pool_size, \n",
    "                                       strides=pool_size)(inputs)\n",
    "        \n",
    "        # 1x1 conv to reduce channels\n",
    "        conv = layers.Conv2D(filters // len(pool_sizes), 1, \n",
    "                           activation='relu', padding='same')(pooled)\n",
    "        \n",
    "        # Upsample back to original size\n",
    "        upsampled = tf.image.resize(conv, tf.shape(inputs)[1:3])\n",
    "        feature_maps.append(upsampled)\n",
    "    \n",
    "    # Add original features\n",
    "    feature_maps.append(inputs)\n",
    "    \n",
    "    # Concatenate all features\n",
    "    concatenated = layers.Concatenate()(feature_maps)\n",
    "    \n",
    "    # Final convolution\n",
    "    output = layers.Conv2D(filters, 3, activation='relu', \n",
    "                         padding='same')(concatenated)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def build_pspnet(input_shape, num_classes, backbone_filters=512):\n",
    "    \"\"\"Build PSPNet model\"\"\"\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Backbone (simplified ResNet-like)\n",
    "    x = layers.Conv2D(64, 7, strides=2, padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    \n",
    "    # Residual blocks (simplified)\n",
    "    for filters in [64, 128, 256, backbone_filters]:\n",
    "        residual = x\n",
    "        \n",
    "        x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        # Adjust residual if needed\n",
    "        if residual.shape[-1] != filters:\n",
    "            residual = layers.Conv2D(filters, 1, padding='same')(residual)\n",
    "        \n",
    "        x = layers.Add()([x, residual])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        \n",
    "        # Downsample for some blocks\n",
    "        if filters in [128, 256]:\n",
    "            x = layers.MaxPooling2D(2)(x)\n",
    "    \n",
    "    # Pyramid pooling module\n",
    "    pyramid_features = pyramid_pooling_module(x, filters=backbone_filters)\n",
    "    \n",
    "    # Classification head\n",
    "    x = layers.Conv2D(256, 3, activation='relu', padding='same')(pyramid_features)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    \n",
    "    # Upsample and output\n",
    "    x = tf.image.resize(x, tf.shape(inputs)[1:3])\n",
    "    outputs = layers.Conv2D(num_classes, 1, activation='softmax', \n",
    "                          padding='same')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs, name='PSPNet')\n",
    "    return model\n",
    "\n",
    "# Build advanced models\n",
    "print(\"\\n=== Building Advanced Segmentation Models ===\")\n",
    "\n",
    "# DeepLab v3+\n",
    "deeplab_model = build_deeplab_v3plus(input_shape, num_classes, backbone='custom')\n",
    "print(f\"DeepLab v3+ model built: {deeplab_model.count_params():,} parameters\")\n",
    "\n",
    "# PSPNet\n",
    "pspnet_model = build_pspnet(input_shape, num_classes)\n",
    "print(f\"PSPNet model built: {pspnet_model.count_params():,} parameters\")\n",
    "\n",
    "# Model comparison\n",
    "models_dict = {\n",
    "    'U-Net': unet_model,\n",
    "    'Attention U-Net': attention_unet_model,\n",
    "    'DeepLab v3+': deeplab_model,\n",
    "    'PSPNet': pspnet_model\n",
    "}\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "for name, model in models_dict.items():\n",
    "    print(f\"{name}: {model.count_params():,} parameters\")\n",
    "```\n",
    "\n",
    "## 4. Advanced Loss Functions and Metrics\n",
    "\n",
    "```python\n",
    "# Segmentation-specific loss functions\n",
    "class SegmentationLosses:\n",
    "    \"\"\"Collection of segmentation loss functions\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "        \"\"\"Dice coefficient for binary segmentation\"\"\"\n",
    "        y_true_flat = tf.reshape(y_true, [-1])\n",
    "        y_pred_flat = tf.reshape(y_pred, [-1])\n",
    "        \n",
    "        intersection = tf.reduce_sum(y_true_flat * y_pred_flat)\n",
    "        union = tf.reduce_sum(y_true_flat) + tf.reduce_sum(y_pred_flat)\n",
    "        \n",
    "        dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "        return dice\n",
    "    \n",
    "    @staticmethod\n",
    "    def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "        \"\"\"Dice loss function\"\"\"\n",
    "        return 1 - SegmentationLosses.dice_coefficient(y_true, y_pred, smooth)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tversky_coefficient(y_true, y_pred, alpha=0.7, beta=0.3, smooth=1e-6):\n",
    "        \"\"\"Tversky coefficient - generalization of Dice\"\"\"\n",
    "        y_true_flat = tf.reshape(y_true, [-1])\n",
    "        y_pred_flat = tf.reshape(y_pred, [-1])\n",
    "        \n",
    "        true_pos = tf.reduce_sum(y_true_flat * y_pred_flat)\n",
    "        false_neg = tf.reduce_sum(y_true_flat * (1 - y_pred_flat))\n",
    "        false_pos = tf.reduce_sum((1 - y_true_flat) * y_pred_flat)\n",
    "        \n",
    "        tversky = (true_pos + smooth) / (true_pos + alpha * false_neg + beta * false_pos + smooth)\n",
    "        return tversky\n",
    "    \n",
    "    @staticmethod\n",
    "    def tversky_loss(y_true, y_pred, alpha=0.7, beta=0.3):\n",
    "        \"\"\"Tversky loss - good for imbalanced datasets\"\"\"\n",
    "        return 1 - SegmentationLosses.tversky_coefficient(y_true, y_pred, alpha, beta)\n",
    "    \n",
    "    @staticmethod\n",
    "    def focal_loss(y_true, y_pred, alpha=0.8, gamma=2.0):\n",
    "        \"\"\"Focal loss for handling class imbalance\"\"\"\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        focal_weight = y_true * (1 - y_pred) ** gamma + (1 - y_true) * y_pred ** gamma\n",
    "        \n",
    "        focal_loss = alpha_factor * focal_weight * (-y_true * tf.math.log(y_pred) - \n",
    "                                                   (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        \n",
    "        return tf.reduce_mean(focal_loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def combo_loss(y_true, y_pred, alpha=0.5, ce_ratio=0.5):\n",
    "        \"\"\"Combination of Dice loss and Cross-entropy\"\"\"\n",
    "        dice = SegmentationLosses.dice_loss(y_true, y_pred)\n",
    "        ce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "        \n",
    "        combo = (ce_ratio * ce) + ((1 - ce_ratio) * dice)\n",
    "        return combo\n",
    "    \n",
    "    @staticmethod\n",
    "    def boundary_loss(y_true, y_pred):\n",
    "        \"\"\"Boundary loss for precise edge segmentation\"\"\"\n",
    "        # Compute gradients to detect boundaries\n",
    "        def compute_gradient(tensor):\n",
    "            dy, dx = tf.image.image_gradients(tensor)\n",
    "            return tf.sqrt(dx**2 + dy**2)\n",
    "        \n",
    "        true_grad = compute_gradient(y_true)\n",
    "        pred_grad = compute_gradient(y_pred)\n",
    "        \n",
    "        boundary_loss = tf.reduce_mean(tf.square(true_grad - pred_grad))\n",
    "        return boundary_loss\n",
    "\n",
    "# Segmentation metrics\n",
    "class SegmentationMetrics:\n",
    "    \"\"\"Collection of segmentation evaluation metrics\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def iou_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "        \"\"\"Intersection over Union (IoU) / Jaccard Index\"\"\"\n",
    "        y_true_flat = tf.reshape(y_true, [-1])\n",
    "        y_pred_flat = tf.reshape(y_pred, [-1])\n",
    "        \n",
    "        intersection = tf.reduce_sum(y_true_flat * y_pred_flat)\n",
    "        union = tf.reduce_sum(y_true_flat) + tf.reduce_sum(y_pred_flat) - intersection\n",
    "        \n",
    "        iou = (intersection + smooth) / (union + smooth)\n",
    "        return iou\n",
    "    \n",
    "    @staticmethod\n",
    "    def mean_iou(y_true, y_pred, num_classes):\n",
    "        \"\"\"Mean IoU across all classes\"\"\"\n",
    "        ious = []\n",
    "        \n",
    "        for class_idx in range(num_classes):\n",
    "            true_class = tf.cast(tf.equal(y_true, class_idx), tf.float32)\n",
    "            pred_class = tf.cast(tf.equal(tf.argmax(y_pred, axis=-1), class_idx), tf.float32)\n",
    "            \n",
    "            iou = SegmentationMetrics.iou_coefficient(true_class, pred_class)\n",
    "            ious.append(iou)\n",
    "        \n",
    "        return tf.reduce_mean(ious)\n",
    "    \n",
    "    @staticmethod\n",
    "    def pixel_accuracy(y_true, y_pred):\n",
    "        \"\"\"Pixel-wise accuracy\"\"\"\n",
    "        pred_classes = tf.argmax(y_pred, axis=-1)\n",
    "        correct_pixels = tf.cast(tf.equal(y_true, pred_classes), tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct_pixels)\n",
    "        return accuracy\n",
    "    \n",
    "    @staticmethod\n",
    "    def frequency_weighted_iou(y_true, y_pred, num_classes):\n",
    "        \"\"\"Frequency weighted IoU\"\"\"\n",
    "        total_pixels = tf.cast(tf.size(y_true), tf.float32)\n",
    "        ious = []\n",
    "        frequencies = []\n",
    "        \n",
    "        for class_idx in range(num_classes):\n",
    "            true_class = tf.cast(tf.equal(y_true, class_idx), tf.float32)\n",
    "            pred_class = tf.cast(tf.equal(tf.argmax(y_pred, axis=-1), class_idx), tf.float32)\n",
    "            \n",
    "            iou = SegmentationMetrics.iou_coefficient(true_class, pred_class)\n",
    "            frequency = tf.reduce_sum(true_class) / total_pixels\n",
    "            \n",
    "            ious.append(iou)\n",
    "            frequencies.append(frequency)\n",
    "        \n",
    "        ious = tf.stack(ious)\n",
    "        frequencies = tf.stack(frequencies)\n",
    "        \n",
    "        weighted_iou = tf.reduce_sum(frequencies * ious)\n",
    "        return weighted_iou\n",
    "\n",
    "# Custom metric classes for Keras\n",
    "class MeanIoU(tf.keras.metrics.Metric):\n",
    "    \"\"\"Mean IoU metric for Keras\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, name='mean_iou', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.total_iou = self.add_weight(name='total_iou', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        mean_iou = SegmentationMetrics.mean_iou(y_true, y_pred, self.num_classes)\n",
    "        self.total_iou.assign_add(mean_iou)\n",
    "        self.count.assign_add(1)\n",
    "    \n",
    "    def result(self):\n",
    "        return self.total_iou / self.count\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.total_iou.assign(0)\n",
    "        self.count.assign(0)\n",
    "\n",
    "class DiceCoefficient(tf.keras.metrics.Metric):\n",
    "    \"\"\"Dice coefficient metric for Keras\"\"\"\n",
    "    \n",
    "    def __init__(self, name='dice_coefficient', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total_dice = self.add_weight(name='total_dice', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        dice = SegmentationLosses.dice_coefficient(y_true, y_pred)\n",
    "        self.total_dice.assign_add(dice)\n",
    "        self.count.assign_add(1)\n",
    "    \n",
    "    def result(self):\n",
    "        return self.total_dice / self.count\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.total_dice.assign(0)\n",
    "        self.count.assign(0)\n",
    "\n",
    "# Test loss functions and metrics\n",
    "print(\"\\n=== Testing Loss Functions and Metrics ===\")\n",
    "\n",
    "# Create sample predictions for testing\n",
    "sample_true = tf.random.uniform([8, 64, 64], maxval=4, dtype=tf.int32)\n",
    "sample_pred = tf.random.uniform([8, 64, 64, 4])\n",
    "\n",
    "# Test metrics\n",
    "mean_iou_metric = MeanIoU(num_classes=4)\n",
    "mean_iou_metric.update_state(sample_true, sample_pred)\n",
    "print(f\"Mean IoU: {mean_iou_metric.result():.4f}\")\n",
    "\n",
    "dice_metric = DiceCoefficient()\n",
    "sample_true_binary = tf.cast(sample_true > 0, tf.float32)\n",
    "sample_pred_binary = sample_pred[:, :, :, 1]  # Take one class\n",
    "dice_metric.update_state(sample_true_binary, sample_pred_binary)\n",
    "print(f\"Dice Coefficient: {dice_metric.result():.4f}\")\n",
    "\n",
    "# Test loss functions\n",
    "dice_loss_val = SegmentationLosses.dice_loss(sample_true_binary, sample_pred_binary)\n",
    "focal_loss_val = SegmentationLosses.focal_loss(sample_true_binary, sample_pred_binary)\n",
    "print(f\"Dice Loss: {dice_loss_val:.4f}\")\n",
    "print(f\"Focal Loss: {focal_loss_val:.4f}\")\n",
    "```\n",
    "\n",
    "## 5. Model Training and Evaluation\n",
    "\n",
    "```python\n",
    "# Prepare data for multi-class segmentation\n",
    "def prepare_multiclass_data(X, y, num_classes):\n",
    "    \"\"\"Convert masks to one-hot encoding\"\"\"\n",
    "    y_categorical = utils.to_categorical(y, num_classes=num_classes)\n",
    "    return X, y_categorical\n",
    "\n",
    "# Prepare training data\n",
    "X_train_prep, y_train_prep = prepare_multiclass_data(X_train, y_train, num_classes)\n",
    "X_val_prep, y_val_prep = prepare_multiclass_data(X_val, y_val, num_classes)\n",
    "X_test_prep, y_test_prep = prepare_multiclass_data(X_test, y_test, num_classes)\n",
    "\n",
    "print(f\"Prepared data shapes:\")\n",
    "print(f\"X_train: {X_train_prep.shape}, y_train: {y_train_prep.shape}\")\n",
    "print(f\"X_val: {X_val_prep.shape}, y_val: {y_val_prep.shape}\")\n",
    "\n",
    "# Training configuration\n",
    "def compile_segmentation_model(model, loss_type='categorical_crossentropy', learning_rate=0.001):\n",
    "    \"\"\"Compile segmentation model with appropriate loss and metrics\"\"\"\n",
    "    \n",
    "    # Select loss function\n",
    "    if loss_type == 'dice':\n",
    "        loss_fn = SegmentationLosses.dice_loss\n",
    "    elif loss_type == 'focal':\n",
    "        loss_fn = SegmentationLosses.focal_loss\n",
    "    elif loss_type == 'combo':\n",
    "        loss_fn = SegmentationLosses.combo_loss\n",
    "    else:\n",
    "        loss_fn = loss_type\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=loss_fn,\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            MeanIoU(num_classes=num_classes),\n",
    "            tf.keras.metrics.CategoricalAccuracy()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Training callbacks\n",
    "def get_training_callbacks(model_name, patience=7):\n",
    "    \"\"\"Get training callbacks for segmentation models\"\"\"\n",
    "    \n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_mean_iou',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_mean_iou',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            f'{model_name}_best.h5',\n",
    "            monitor='val_mean_iou',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.CSVLogger(f'{model_name}_training.log')\n",
    "    ]\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "# Train models\n",
    "def train_segmentation_model(model, model_name, X_train, y_train, X_val, y_val, \n",
    "                           epochs=30, batch_size=16):\n",
    "    \"\"\"Train segmentation model\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "    \n",
    "    # Compile model\n",
    "    model = compile_segmentation_model(model, loss_type='categorical_crossentropy')\n",
    "    \n",
    "    # Get callbacks\n",
    "    callbacks = get_training_callbacks(model_name)\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Model comparison training\n",
    "training_results = {}\n",
    "\n",
    "# Train U-Net (fastest to train for demonstration)\n",
    "unet_trained, unet_history = train_segmentation_model(\n",
    "    unet_model, 'unet', X_train_prep, y_train_prep, X_val_prep, y_val_prep,\n",
    "    epochs=5, batch_size=8  # Reduced for demo\n",
    ")\n",
    "training_results['U-Net'] = unet_history\n",
    "\n",
    "print(\"U-Net training completed!\")\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_segmentation_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_loss, test_acc, test_miou, test_cat_acc = model.evaluate(\n",
    "        X_test, y_test, verbose=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test Mean IoU: {test_miou:.4f}\")\n",
    "    print(f\"Test Categorical Accuracy: {test_cat_acc:.4f}\")\n",
    "    \n",
    "    # Visualize predictions\n",
    "    visualize_segmentation(X_test[:4], np.argmax(y_test[:4], axis=-1), \n",
    "                         predictions[:4], num_samples=4)\n",
    "    \n",
    "    return {\n",
    "        'loss': test_loss,\n",
    "        'accuracy': test_acc,\n",
    "        'mean_iou': test_miou,\n",
    "        'categorical_accuracy': test_cat_acc,\n",
    "        'predictions': predictions\n",
    "    }\n",
    "\n",
    "# Evaluate U-Net\n",
    "unet_results = evaluate_segmentation_model(\n",
    "    unet_trained, X_test_prep, y_test_prep, 'U-Net'\n",
    ")\n",
    "\n",
    "# Training history visualization\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history.history['loss'], label='Train Loss')\n",
    "    axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n",
    "    axes[0, 0].set_title(f'{model_name} - Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    axes[0, 1].set_title(f'{model_name} - Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Mean IoU\n",
    "    axes[1, 0].plot(history.history['mean_iou'], label='Train Mean IoU')\n",
    "    axes[1, 0].plot(history.history['val_mean_iou'], label='Val Mean IoU')\n",
    "    axes[1, 0].set_title(f'{model_name} - Mean IoU')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Mean IoU')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Learning Rate (if available)\n",
    "    if 'lr' in history.history:\n",
    "        axes[1, 1].plot(history.history['lr'], label='Learning Rate')\n",
    "        axes[1, 1].set_title(f'{model_name} - Learning Rate')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Learning Rate')\n",
    "        axes[1, 1].set_yscale('log')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True)\n",
    "    else:\n",
    "        axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(unet_history, 'U-Net')\n",
    "```\n",
    "\n",
    "## 6. Real-World Application: Medical Image Segmentation\n",
    "\n",
    "```python\n",
    "# Medical image segmentation example\n",
    "def create_medical_dataset(num_samples=200, image_size=256):\n",
    "    \"\"\"Create synthetic medical imaging dataset\"\"\"\n",
    "    \n",
    "    print(\"Creating synthetic medical dataset (simulating organ segmentation)...\")\n",
    "    \n",
    "    images = []\n",
    "    masks = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Create medical-like image with organ structures\n",
    "        img = np.random.normal(0.3, 0.1, (image_size, image_size, 1))\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Create organ-like structures\n",
    "        mask = np.zeros((image_size, image_size), dtype=np.uint8)\n",
    "        \n",
    "        # Add organ structures (simplified)\n",
    "        # Large organ (like liver)\n",
    "        center1 = (image_size//3, image_size//3)\n",
    "        axes1 = (image_size//6, image_size//8)\n",
    "        \n",
    "        # Create elliptical structure\n",
    "        y, x = np.ogrid[:image_size, :image_size]\n",
    "        ellipse1 = ((x - center1[0])/axes1[0])**2 + ((y - center1[1])/axes1[1])**2 <= 1\n",
    "        mask[ellipse1] = 1  # Organ class\n",
    "        img[ellipse1] = img[ellipse1] + 0.2\n",
    "        \n",
    "        # Smaller organ (like kidney)\n",
    "        center2 = (2*image_size//3, 2*image_size//3)\n",
    "        axes2 = (image_size//12, image_size//10)\n",
    "        \n",
    "        ellipse2 = ((x - center2[0])/axes2[0])**2 + ((y - center2[1])/axes2[1])**2 <= 1\n",
    "        mask[ellipse2] = 2  # Different organ class\n",
    "        img[ellipse2] = img[ellipse2] + 0.3\n",
    "        \n",
    "        # Add noise and contrast variations\n",
    "        img = img + np.random.normal(0, 0.05, img.shape)\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Convert to 3-channel for consistency\n",
    "        img = np.repeat(img, 3, axis=-1)\n",
    "        \n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "    \n",
    "    images = np.array(images, dtype=np.float32)\n",
    "    masks = np.array(masks, dtype=np.uint8)\n",
    "    \n",
    "    print(f\"Medical dataset created: {images.shape} images, {masks.shape} masks\")\n",
    "    print(f\"Classes: {np.unique(masks)} (0=background, 1=organ1, 2=organ2)\")\n",
    "    \n",
    "    return images, masks\n",
    "\n",
    "# Create medical dataset\n",
    "med_images, med_masks = create_medical_dataset(num_samples=150, image_size=128)\n",
    "\n",
    "# Split medical data\n",
    "med_X_train, med_X_test, med_y_train, med_y_test = train_test_split(\n",
    "    med_images, med_masks, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Visualize medical data\n",
    "print(\"Medical segmentation dataset:\")\n",
    "visualize_segmentation(med_X_train, med_y_train, num_samples=3)\n",
    "\n",
    "# Build specialized medical segmentation model\n",
    "def build_medical_unet(input_shape, num_classes, filters=64):\n",
    "    \"\"\"Build U-Net optimized for medical imaging\"\"\"\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder with more aggressive pooling for medical images\n",
    "    e1, p1 = encoder_block(inputs, filters, pool=True, dropout_rate=0.1)\n",
    "    e2, p2 = encoder_block(p1, filters * 2, pool=True, dropout_rate=0.1)\n",
    "    e3, p3 = encoder_block(p2, filters * 4, pool=True, dropout_rate=0.2)\n",
    "    e4, p4 = encoder_block(p3, filters * 8, pool=True, dropout_rate=0.2)\n",
    "    \n",
    "    # Bottleneck with higher capacity\n",
    "    bottleneck = encoder_block(p4, filters * 16, pool=False, dropout_rate=0.3)\n",
    "    \n",
    "    # Decoder with residual connections\n",
    "    d4 = decoder_block(bottleneck, e4, filters * 8, dropout_rate=0.2)\n",
    "    d3 = decoder_block(d4, e3, filters * 4, dropout_rate=0.2)\n",
    "    d2 = decoder_block(d3, e2, filters * 2, dropout_rate=0.1)\n",
    "    d1 = decoder_block(d2, e1, filters, dropout_rate=0.1)\n",
    "    \n",
    "    # Multi-scale output supervision (deep supervision)\n",
    "    # Main output\n",
    "    main_output = layers.Conv2D(num_classes, 1, activation='softmax', \n",
    "                              padding='same', name='main_output')(d1)\n",
    "    \n",
    "    # Auxiliary outputs for deep supervision\n",
    "    aux_output1 = layers.Conv2D(num_classes, 1, activation='softmax', \n",
    "                               padding='same', name='aux_output1')(d2)\n",
    "    aux_output1 = tf.image.resize(aux_output1, tf.shape(main_output)[1:3])\n",
    "    \n",
    "    aux_output2 = layers.Conv2D(num_classes, 1, activation='softmax', \n",
    "                               padding='same', name='aux_output2')(d3)\n",
    "    aux_output2 = tf.image.resize(aux_output2, tf.shape(main_output)[1:3])\n",
    "    \n",
    "    model = models.Model(inputs, [main_output, aux_output1, aux_output2], \n",
    "                        name='Medical-U-Net')\n",
    "    return model\n",
    "\n",
    "# Build medical model\n",
    "med_num_classes = len(np.unique(med_y_train))\n",
    "med_input_shape = med_X_train.shape[1:]\n",
    "\n",
    "medical_unet = build_medical_unet(med_input_shape, med_num_classes, filters=32)\n",
    "print(f\"Medical U-Net built: {medical_unet.count_params():,} parameters\")\n",
    "\n",
    "# Prepare medical data\n",
    "med_X_train_prep, med_y_train_prep = prepare_multiclass_data(med_X_train, med_y_train, med_num_classes)\n",
    "med_X_test_prep, med_y_test_prep = prepare_multiclass_data(med_X_test, med_y_test, med_num_classes)\n",
    "\n",
    "# Custom loss for deep supervision\n",
    "def deep_supervision_loss(y_true, y_pred_list, weights=[1.0, 0.5, 0.25]):\n",
    "    \"\"\"Deep supervision loss combining multiple outputs\"\"\"\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i, (y_pred, weight) in enumerate(zip(y_pred_list, weights)):\n",
    "        loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "        total_loss += weight * loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# Compile medical model with custom loss\n",
    "medical_unet.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        'main_output': 'categorical_crossentropy',\n",
    "        'aux_output1': 'categorical_crossentropy', \n",
    "        'aux_output2': 'categorical_crossentropy'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'main_output': 1.0,\n",
    "        'aux_output1': 0.5,\n",
    "        'aux_output2': 0.25\n",
    "    },\n",
    "    metrics={\n",
    "        'main_output': [MeanIoU(num_classes=med_num_classes)],\n",
    "        'aux_output1': [MeanIoU(num_classes=med_num_classes)],\n",
    "        'aux_output2': [MeanIoU(num_classes=med_num_classes)]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train medical model (quick demo)\n",
    "print(\"\\n=== Training Medical Segmentation Model ===\")\n",
    "\n",
    "med_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_main_output_mean_iou',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        mode='max'\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_main_output_mean_iou',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        mode='max'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Prepare target data for multiple outputs\n",
    "med_y_train_dict = {\n",
    "    'main_output': med_y_train_prep,\n",
    "    'aux_output1': med_y_train_prep,\n",
    "    'aux_output2': med_y_train_prep\n",
    "}\n",
    "\n",
    "med_y_test_dict = {\n",
    "    'main_output': med_y_test_prep,\n",
    "    'aux_output1': med_y_test_prep,\n",
    "    'aux_output2': med_y_test_prep\n",
    "}\n",
    "\n",
    "med_history = medical_unet.fit(\n",
    "    med_X_train_prep, med_y_train_dict,\n",
    "    validation_data=(med_X_test_prep, med_y_test_dict),\n",
    "    epochs=3,  # Reduced for demo\n",
    "    batch_size=4,\n",
    "    callbacks=med_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate medical model\n",
    "med_predictions = medical_unet.predict(med_X_test_prep, verbose=0)\n",
    "main_predictions = med_predictions[0]  # Use main output\n",
    "\n",
    "# Visualize medical segmentation results\n",
    "print(\"\\nMedical Segmentation Results:\")\n",
    "visualize_segmentation(med_X_test[:4], med_y_test[:4], \n",
    "                      main_predictions[:4], num_samples=4)\n",
    "\n",
    "# Calculate medical-specific metrics\n",
    "def calculate_medical_metrics(y_true, y_pred, class_names=['Background', 'Organ1', 'Organ2']):\n",
    "    \"\"\"Calculate medical imaging specific metrics\"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        # Binary masks for current class\n",
    "        true_binary = (y_true == class_idx).astype(np.float32)\n",
    "        pred_binary = (np.argmax(y_pred, axis=-1) == class_idx).astype(np.float32)\n",
    "        \n",
    "        # Dice coefficient\n",
    "        dice = SegmentationLosses.dice_coefficient(true_binary, pred_binary).numpy()\n",
    "        \n",
    "        # Sensitivity (Recall)\n",
    "        true_pos = np.sum(true_binary * pred_binary)\n",
    "        false_neg = np.sum(true_binary * (1 - pred_binary))\n",
    "        sensitivity = true_pos / (true_pos + false_neg + 1e-6)\n",
    "        \n",
    "        # Specificity\n",
    "        true_neg = np.sum((1 - true_binary) * (1 - pred_binary))\n",
    "        false_pos = np.sum((1 - true_binary) * pred_binary)\n",
    "        specificity = true_neg / (true_neg + false_pos + 1e-6)\n",
    "        \n",
    "        metrics[class_name] = {\n",
    "            'dice': dice,\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate medical metrics\n",
    "medical_metrics = calculate_medical_metrics(med_y_test, main_predictions)\n",
    "\n",
    "print(\"\\nMedical Segmentation Metrics:\")\n",
    "for class_name, metrics in medical_metrics.items():\n",
    "    print(f\"{class_name}:\")\n",
    "    print(f\"  Dice: {metrics['dice']:.4f}\")\n",
    "    print(f\"  Sensitivity: {metrics['sensitivity']:.4f}\")\n",
    "    print(f\"  Specificity: {metrics['specificity']:.4f}\")\n",
    "```\n",
    "\n",
    "## 7. Model Deployment and Inference\n",
    "\n",
    "```python\n",
    "# Model deployment utilities\n",
    "class SegmentationInference:\n",
    "    \"\"\"Segmentation model inference pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, input_shape, num_classes):\n",
    "        self.model = tf.keras.models.load_model(model_path, compile=False)\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def preprocess_image(self, image_path):\n",
    "        \"\"\"Preprocess single image for inference\"\"\"\n",
    "        \n",
    "        if isinstance(image_path, str):\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            image = image_path\n",
    "        \n",
    "        # Resize to model input size\n",
    "        image = cv2.resize(image, self.input_shape[:2])\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Add batch dimension\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def predict(self, image):\n",
    "        \"\"\"Make prediction on single image\"\"\"\n",
    "        \n",
    "        preprocessed = self.preprocess_image(image)\n",
    "        prediction = self.model.predict(preprocessed, verbose=0)\n",
    "        \n",
    "        # Handle multiple outputs (deep supervision)\n",
    "        if isinstance(prediction, list):\n",
    "            prediction = prediction[0]  # Use main output\n",
    "        \n",
    "        return prediction[0]  # Remove batch dimension\n",
    "    \n",
    "    def predict_batch(self, images):\n",
    "        \"\"\"Make predictions on batch of images\"\"\"\n",
    "        \n",
    "        batch = np.stack([self.preprocess_image(img)[0] for img in images])\n",
    "        predictions = self.model.predict(batch, verbose=0)\n",
    "        \n",
    "        if isinstance(predictions, list):\n",
    "            predictions = predictions[0]\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def postprocess_prediction(self, prediction, threshold=0.5):\n",
    "        \"\"\"Postprocess prediction to get final mask\"\"\"\n",
    "        \n",
    "        if self.num_classes > 1:\n",
    "            # Multi-class: use argmax\n",
    "            mask = np.argmax(prediction, axis=-1)\n",
    "        else:\n",
    "            # Binary: use threshold\n",
    "            mask = (prediction[:, :, 0] > threshold).astype(np.uint8)\n",
    "        \n",
    "        return mask.astype(np.uint8)\n",
    "    \n",
    "    def visualize_prediction(self, image, prediction, save_path=None):\n",
    "        \"\"\"Visualize prediction result\"\"\"\n",
    "        \n",
    "        mask = self.postprocess_prediction(prediction)\n",
    "        \n",
    "        # Create overlay\n",
    "        if isinstance(image, str):\n",
    "            image = cv2.imread(image)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        image = cv2.resize(image, self.input_shape[:2])\n",
    "        overlay = image.copy()\n",
    "        \n",
    "        # Apply colormap to mask\n",
    "        colored_mask = plt.cm.viridis(mask / mask.max())[:, :, :3]\n",
    "        overlay = (0.7 * image/255.0 + 0.3 * colored_mask * 255).astype(np.uint8)\n",
    "        \n",
    "        # Plot results\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        axes[0].imshow(image)\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(mask, cmap='viridis')\n",
    "        axes[1].set_title('Segmentation Mask')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(overlay)\n",
    "        axes[2].set_title('Overlay')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# Save trained model for deployment\n",
    "unet_trained.save('segmentation_unet_model.h5')\n",
    "print(\"Model saved for deployment!\")\n",
    "\n",
    "# Create inference pipeline\n",
    "inference_pipeline = SegmentationInference(\n",
    "    model_path='segmentation_unet_model.h5',\n",
    "    input_shape=input_shape,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Test inference on new data\n",
    "test_image = X_test[0]\n",
    "prediction = inference_pipeline.predict(test_image)\n",
    "\n",
    "print(\"Inference test completed!\")\n",
    "inference_pipeline.visualize_prediction(test_image, prediction)\n",
    "\n",
    "# Batch inference example\n",
    "batch_predictions = inference_pipeline.predict_batch(X_test[:4])\n",
    "print(f\"Batch inference completed: {batch_predictions.shape}\")\n",
    "\n",
    "# Performance optimization for deployment\n",
    "@tf.function\n",
    "def optimized_inference(model, inputs):\n",
    "    \"\"\"TensorFlow function for optimized inference\"\"\"\n",
    "    return model(inputs, training=False)\n",
    "\n",
    "# Convert to TensorFlow Lite for mobile deployment\n",
    "def convert_to_tflite(model, save_path):\n",
    "    \"\"\"Convert model to TensorFlow Lite format\"\"\"\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    # Optimization settings\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_types = [tf.float16]\n",
    "    \n",
    "    # Convert model\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Save model\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(f\"TFLite model saved to: {save_path}\")\n",
    "    \n",
    "    return tflite_model\n",
    "\n",
    "# Convert U-Net to TFLite\n",
    "tflite_model = convert_to_tflite(unet_trained, 'segmentation_model.tflite')\n",
    "\n",
    "# TensorFlow Lite inference example\n",
    "def tflite_inference(tflite_model_path, input_image):\n",
    "    \"\"\"Run inference with TensorFlow Lite model\"\"\"\n",
    "    \n",
    "    # Load TFLite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    # Get input and output tensors\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Preprocess input\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_image = cv2.resize(input_image, (input_shape[1], input_shape[2]))\n",
    "    input_image = np.expand_dims(input_image, axis=0).astype(np.float32)\n",
    "    \n",
    "    # Set input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image)\n",
    "    \n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get output\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    return output_data[0]\n",
    "\n",
    "# Test TFLite inference\n",
    "test_tflite_pred = tflite_inference('segmentation_model.tflite', X_test[0])\n",
    "print(f\"TFLite inference shape: {test_tflite_pred.shape}\")\n",
    "\n",
    "# Production deployment utilities\n",
    "class SegmentationAPI:\n",
    "    \"\"\"REST API wrapper for segmentation model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, input_shape, num_classes):\n",
    "        self.inference = SegmentationInference(model_path, input_shape, num_classes)\n",
    "        \n",
    "    def create_flask_app(self):\n",
    "        \"\"\"Create Flask API for segmentation model\"\"\"\n",
    "        \n",
    "        from flask import Flask, request, jsonify\n",
    "        import base64\n",
    "        from io import BytesIO\n",
    "        from PIL import Image\n",
    "        \n",
    "        app = Flask(__name__)\n",
    "        \n",
    "        @app.route('/health', methods=['GET'])\n",
    "        def health_check():\n",
    "            return jsonify({'status': 'healthy'})\n",
    "        \n",
    "        @app.route('/segment', methods=['POST'])\n",
    "        def segment_image():\n",
    "            try:\n",
    "                # Get image from request\n",
    "                data = request.json\n",
    "                image_data = base64.b64decode(data['image'])\n",
    "                image = Image.open(BytesIO(image_data))\n",
    "                image = np.array(image)\n",
    "                \n",
    "                # Make prediction\n",
    "                prediction = self.inference.predict(image)\n",
    "                mask = self.inference.postprocess_prediction(prediction)\n",
    "                \n",
    "                # Convert mask to base64\n",
    "                mask_pil = Image.fromarray((mask * 255 / mask.max()).astype(np.uint8))\n",
    "                buffer = BytesIO()\n",
    "                mask_pil.save(buffer, format='PNG')\n",
    "                mask_b64 = base64.b64encode(buffer.getvalue()).decode()\n",
    "                \n",
    "                return jsonify({\n",
    "                    'status': 'success',\n",
    "                    'mask': mask_b64,\n",
    "                    'shape': mask.shape,\n",
    "                    'classes_found': np.unique(mask).tolist()\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                return jsonify({'status': 'error', 'message': str(e)})\n",
    "        \n",
    "        return app\n",
    "\n",
    "# Create API instance\n",
    "api = SegmentationAPI('segmentation_unet_model.h5', input_shape, num_classes)\n",
    "flask_app = api.create_flask_app()\n",
    "\n",
    "print(\"Flask API created for segmentation model!\")\n",
    "\n",
    "# Performance benchmarking\n",
    "def benchmark_model(model, test_data, num_runs=50):\n",
    "    \"\"\"Benchmark model inference speed\"\"\"\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    # Warm up\n",
    "    for _ in range(5):\n",
    "        _ = model.predict(test_data[:1], verbose=0)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        _ = model.predict(test_data[:1], verbose=0)\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    print(f\"Average inference time: {avg_time*1000:.2f}  {std_time*1000:.2f} ms\")\n",
    "    print(f\"Throughput: {1/avg_time:.2f} images/second\")\n",
    "    \n",
    "    return avg_time, std_time\n",
    "\n",
    "# Benchmark U-Net\n",
    "print(\"\\n=== Model Benchmarking ===\")\n",
    "avg_time, std_time = benchmark_model(unet_trained, X_test[:10])\n",
    "\n",
    "# Model comparison summary\n",
    "def create_model_comparison_report():\n",
    "    \"\"\"Create comprehensive model comparison report\"\"\"\n",
    "    \n",
    "    comparison_data = {\n",
    "        'Model': ['U-Net', 'Attention U-Net', 'DeepLab v3+', 'PSPNet'],\n",
    "        'Parameters': [\n",
    "            unet_model.count_params(),\n",
    "            attention_unet_model.count_params(), \n",
    "            deeplab_model.count_params(),\n",
    "            pspnet_model.count_params()\n",
    "        ],\n",
    "        'Architecture': ['Encoder-Decoder', 'Attention + Skip', 'ASPP + Decoder', 'Pyramid Pooling'],\n",
    "        'Best_For': [\n",
    "            'General segmentation',\n",
    "            'Fine detail preservation', \n",
    "            'Multi-scale features',\n",
    "            'Scene parsing'\n",
    "        ],\n",
    "        'Complexity': ['Low', 'Medium', 'High', 'Medium']\n",
    "    }\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\n=== Model Architecture Comparison ===\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    return df\n",
    "\n",
    "comparison_report = create_model_comparison_report()\n",
    "\n",
    "print(\"\\n=== Advanced Applications ===\")\n",
    "\n",
    "# Instance segmentation preview\n",
    "def build_mask_rcnn_backbone(input_shape):\n",
    "    \"\"\"Simple Mask R-CNN inspired backbone\"\"\"\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Feature pyramid backbone\n",
    "    # Level 1\n",
    "    c1 = layers.Conv2D(64, 3, padding='same', activation='relu')(inputs)\n",
    "    c1 = layers.Conv2D(64, 3, padding='same', activation='relu')(c1)\n",
    "    p1 = layers.MaxPooling2D(2)(c1)\n",
    "    \n",
    "    # Level 2  \n",
    "    c2 = layers.Conv2D(128, 3, padding='same', activation='relu')(p1)\n",
    "    c2 = layers.Conv2D(128, 3, padding='same', activation='relu')(c2)\n",
    "    p2 = layers.MaxPooling2D(2)(c2)\n",
    "    \n",
    "    # Level 3\n",
    "    c3 = layers.Conv2D(256, 3, padding='same', activation='relu')(p2)\n",
    "    c3 = layers.Conv2D(256, 3, padding='same', activation='relu')(c3)\n",
    "    p3 = layers.MaxPooling2D(2)(c3)\n",
    "    \n",
    "    # Simplified mask head (normally would include RPN + ROI pooling)\n",
    "    mask_head = layers.Conv2D(256, 3, padding='same', activation='relu')(p3)\n",
    "    mask_head = layers.Conv2DTranspose(128, 2, strides=2, padding='same', activation='relu')(mask_head)\n",
    "    mask_head = layers.Conv2DTranspose(64, 2, strides=2, padding='same', activation='relu')(mask_head)\n",
    "    mask_head = layers.Conv2DTranspose(32, 2, strides=2, padding='same', activation='relu')(mask_head)\n",
    "    \n",
    "    # Output\n",
    "    masks = layers.Conv2D(num_classes, 1, activation='sigmoid', padding='same')(mask_head)\n",
    "    \n",
    "    model = models.Model(inputs, masks, name='SimpleInstanceSeg')\n",
    "    return model\n",
    "\n",
    "# Real-time segmentation optimization\n",
    "def create_lightweight_segmentation_model(input_shape, num_classes):\n",
    "    \"\"\"Create lightweight model for real-time applications\"\"\"\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Efficient encoder using depthwise separable convolutions\n",
    "    def depthwise_conv_block(x, filters, strides=1):\n",
    "        x = layers.DepthwiseConv2D(3, strides=strides, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        \n",
    "        x = layers.Conv2D(filters, 1, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # Encoder\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding='same', activation='relu')(inputs)\n",
    "    e1 = depthwise_conv_block(x, 64)\n",
    "    \n",
    "    e2 = depthwise_conv_block(e1, 128, strides=2)\n",
    "    e3 = depthwise_conv_block(e2, 256, strides=2)\n",
    "    e4 = depthwise_conv_block(e3, 512, strides=2)\n",
    "    \n",
    "    # Lightweight decoder\n",
    "    d3 = layers.Conv2DTranspose(256, 2, strides=2, padding='same', activation='relu')(e4)\n",
    "    d3 = layers.Add()([d3, e3])\n",
    "    \n",
    "    d2 = layers.Conv2DTranspose(128, 2, strides=2, padding='same', activation='relu')(d3)\n",
    "    d2 = layers.Add()([d2, e2])\n",
    "    \n",
    "    d1 = layers.Conv2DTranspose(64, 2, strides=2, padding='same', activation='relu')(d2)\n",
    "    d1 = layers.Add()([d1, e1])\n",
    "    \n",
    "    # Output\n",
    "    outputs = layers.Conv2DTranspose(num_classes, 2, strides=2, padding='same', \n",
    "                                   activation='softmax')(d1)\n",
    "    \n",
    "    model = models.Model(inputs, outputs, name='LightweightSegNet')\n",
    "    return model\n",
    "\n",
    "# Build lightweight model\n",
    "lightweight_model = create_lightweight_segmentation_model(input_shape, num_classes)\n",
    "print(f\"Lightweight model: {lightweight_model.count_params():,} parameters\")\n",
    "\n",
    "print(\"\\n=== Deployment Checklist ===\")\n",
    "deployment_checklist = [\n",
    "    \" Model trained and validated\",\n",
    "    \" Inference pipeline created\", \n",
    "    \" TensorFlow Lite conversion completed\",\n",
    "    \" API wrapper implemented\",\n",
    "    \" Performance benchmarked\",\n",
    "    \" Lightweight variant available\"\n",
    "]\n",
    "\n",
    "for item in deployment_checklist:\n",
    "    print(item)\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "This comprehensive image segmentation notebook demonstrates production-ready segmentation with tf.keras:\n",
    "\n",
    "**Core Architectures Implemented:**\n",
    "- **U-Net**: Standard encoder-decoder with skip connections for general segmentation\n",
    "- **Attention U-Net**: Enhanced U-Net with attention gates for fine detail preservation  \n",
    "- **DeepLab v3+**: ASPP module with multi-scale feature extraction\n",
    "- **PSPNet**: Pyramid pooling for scene understanding\n",
    "- **Medical U-Net**: Deep supervision for medical imaging applications\n",
    "\n",
    "**Advanced Features:**\n",
    "- **Synchronized data augmentation** for image-mask pairs\n",
    "- **Comprehensive loss functions**: Dice, Focal, Tversky, Boundary losses\n",
    "- **Medical-specific metrics**: Sensitivity, Specificity, class-wise Dice coefficients\n",
    "- **Multi-output training** with deep supervision for improved accuracy\n",
    "\n",
    "**Production Deployment:**\n",
    "- **Optimized inference pipeline** with preprocessing and postprocessing\n",
    "- **TensorFlow Lite conversion** for mobile deployment\n",
    "- **REST API wrapper** with Flask for web deployment\n",
    "- **Performance benchmarking** tools for production optimization\n",
    "- **Lightweight models** for real-time applications\n",
    "\n",
    "**Key Technical Achievements:**\n",
    "- **Modular architecture** supporting easy model comparison\n",
    "- **Medical imaging pipeline** with specialized evaluation metrics\n",
    "- **Real-time inference** optimization with TensorFlow functions\n",
    "- **Cross-platform deployment** ready for mobile and web\n",
    "- **Comprehensive evaluation framework** with visualization tools\n",
    "\n",
    "The notebook provides a complete foundation for building, training, and deploying segmentation models across various domains including medical imaging, autonomous driving, and general computer vision applications, with production-ready code and deployment strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
