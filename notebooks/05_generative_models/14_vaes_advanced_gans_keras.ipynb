{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 vaes advanced gans keras\n",
    "**Location: TensorVerseHub/notebooks/05_generative_models/14_vaes_advanced_gans_keras.ipynb**\n",
    "\n",
    "TODO: Implement comprehensive TensorFlow + tf.keras learning content.\n",
    "\n",
    "## Learning Objectives\n",
    "- TODO: Define specific learning objectives\n",
    "- TODO: List key TensorFlow concepts covered\n",
    "- TODO: Outline tf.keras integration points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "# TODO: Add comprehensive implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAEs and Advanced GANs with tf.keras\n",
    "\n",
    "**File Location:** `notebooks/05_generative_models/14_vaes_advanced_gans_keras.ipynb`\n",
    "\n",
    "Master Variational Autoencoders (VAEs) and advanced GAN architectures including StyleGAN, Progressive GAN, and CycleGAN using tf.keras. Explore latent space manipulation, disentangled representations, and cutting-edge generative modeling techniques.\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement Variational Autoencoders with reparameterization trick\n",
    "- Build StyleGAN and Progressive GAN architectures\n",
    "- Master CycleGAN for unpaired image-to-image translation\n",
    "- Explore latent space interpolation and manipulation\n",
    "- Apply disentangled representation learning\n",
    "- Implement advanced loss functions and training strategies\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Variational Autoencoder (VAE) Implementation\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Variational Autoencoder implementation\n",
    "class VAE(tf.keras.Model):\n",
    "    \"\"\"Variational Autoencoder with reparameterization trick\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=2, intermediate_dim=64, input_shape=(28, 28, 1), beta=1.0):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.beta = beta  # β-VAE parameter for disentanglement\n",
    "        \n",
    "        # Encoder architecture\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Flatten(input_shape=input_shape),\n",
    "            layers.Dense(intermediate_dim, activation='relu'),\n",
    "            layers.Dense(intermediate_dim, activation='relu'),\n",
    "        ], name='encoder')\n",
    "        \n",
    "        # Latent space projections\n",
    "        self.z_mean = layers.Dense(latent_dim, name='z_mean')\n",
    "        self.z_log_var = layers.Dense(latent_dim, name='z_log_var')\n",
    "        \n",
    "        # Decoder architecture\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Dense(intermediate_dim, activation='relu', input_shape=(latent_dim,)),\n",
    "            layers.Dense(intermediate_dim, activation='relu'),\n",
    "            layers.Dense(np.prod(input_shape), activation='sigmoid'),\n",
    "            layers.Reshape(input_shape)\n",
    "        ], name='decoder')\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent parameters\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        z_mean = self.z_mean(h)\n",
    "        z_log_var = self.z_log_var(h)\n",
    "        return z_mean, z_log_var\n",
    "    \n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        \"\"\"Reparameterization trick for backpropagation through random sampling\"\"\"\n",
    "        batch_size = tf.shape(z_mean)[0]\n",
    "        epsilon = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent vector to reconstruction\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass through VAE\"\"\"\n",
    "        z_mean, z_log_var = self.encode(inputs)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, z_mean, z_log_var\n",
    "    \n",
    "    def compute_loss(self, inputs):\n",
    "        \"\"\"Compute VAE loss (reconstruction + KL divergence)\"\"\"\n",
    "        reconstructed, z_mean, z_log_var = self(inputs)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                tf.keras.losses.binary_crossentropy(inputs, reconstructed), axis=(1, 2)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        )\n",
    "        \n",
    "        # Total loss with β weighting\n",
    "        total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        \n",
    "        return total_loss, reconstruction_loss, kl_loss\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        \"\"\"Custom training step for VAE\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss, reconstruction_loss, kl_loss = self.compute_loss(data)\n",
    "        \n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'reconstruction_loss': reconstruction_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n",
    "\n",
    "# Convolutional VAE for better image handling\n",
    "class ConvVAE(tf.keras.Model):\n",
    "    \"\"\"Convolutional Variational Autoencoder for image data\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=128, input_shape=(64, 64, 3), beta=1.0):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.beta = beta\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Conv2D(32, 3, strides=2, padding='same', activation='relu'),\n",
    "            layers.Conv2D(64, 3, strides=2, padding='same', activation='relu'),\n",
    "            layers.Conv2D(128, 3, strides=2, padding='same', activation='relu'),\n",
    "            layers.Conv2D(256, 3, strides=2, padding='same', activation='relu'),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(512, activation='relu')\n",
    "        ], name='conv_encoder')\n",
    "        \n",
    "        # Latent projections\n",
    "        self.z_mean = layers.Dense(latent_dim)\n",
    "        self.z_log_var = layers.Dense(latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_dense = layers.Dense(4 * 4 * 256, activation='relu')\n",
    "        self.decoder_reshape = layers.Reshape((4, 4, 256))\n",
    "        \n",
    "        self.decoder_conv = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu'),\n",
    "            layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu'),\n",
    "            layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='relu'),\n",
    "            layers.Conv2DTranspose(input_shape[-1], 3, strides=2, padding='same', activation='sigmoid')\n",
    "        ], name='conv_decoder')\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.z_mean(h), self.z_log_var(h)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = self.decoder_dense(z)\n",
    "        h = self.decoder_reshape(h)\n",
    "        return self.decoder_conv(h)\n",
    "    \n",
    "    def reparameterize(self, z_mean, z_log_var):\n",
    "        batch_size = tf.shape(z_mean)[0]\n",
    "        epsilon = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = self.encode(inputs)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, z_mean, z_log_var\n",
    "    \n",
    "    def compute_loss(self, inputs):\n",
    "        reconstructed, z_mean, z_log_var = self(inputs)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                tf.keras.losses.binary_crossentropy(inputs, reconstructed), axis=(1, 2, 3)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
    "        )\n",
    "        \n",
    "        return reconstruction_loss + self.beta * kl_loss, reconstruction_loss, kl_loss\n",
    "\n",
    "# Load and prepare data\n",
    "def load_mnist_data():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    \n",
    "    print(f\"MNIST data: Train {x_train.shape}, Test {x_test.shape}\")\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "# Test VAE implementation\n",
    "print(\"=== VAE Implementation Test ===\")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist_data()\n",
    "\n",
    "# Create and compile VAE\n",
    "vae = VAE(latent_dim=2, input_shape=(28, 28, 1), beta=1.0)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam(1e-3))\n",
    "\n",
    "# Build model with sample data\n",
    "sample_data = x_train[:100]\n",
    "_ = vae(sample_data)\n",
    "\n",
    "print(\"VAE Architecture:\")\n",
    "print(f\"Encoder: {vae.encoder.count_params():,} parameters\")\n",
    "print(f\"Decoder: {vae.decoder.count_params():,} parameters\")\n",
    "print(f\"Total: {vae.count_params():,} parameters\")\n",
    "\n",
    "# Train VAE for demo\n",
    "print(\"\\nTraining VAE (demo - 5 epochs)...\")\n",
    "history = vae.fit(x_train[:5000], x_train[:5000], \n",
    "                  epochs=5, batch_size=128, \n",
    "                  validation_data=(x_test[:1000], x_test[:1000]),\n",
    "                  verbose=1)\n",
    "\n",
    "# Visualize VAE results\n",
    "def visualize_vae_results(vae, test_data, n_samples=10):\n",
    "    \"\"\"Visualize VAE reconstruction and generation\"\"\"\n",
    "    \n",
    "    # Original vs Reconstructed\n",
    "    test_samples = test_data[:n_samples]\n",
    "    reconstructed, z_mean, z_log_var = vae(test_samples)\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Original images\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(3, n_samples, i + 1)\n",
    "        plt.imshow(test_samples[i, :, :, 0], cmap='gray')\n",
    "        plt.title(f'Original {i+1}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # Reconstructed images\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(3, n_samples, n_samples + i + 1)\n",
    "        plt.imshow(reconstructed[i, :, :, 0], cmap='gray')\n",
    "        plt.title(f'Reconstructed {i+1}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # Generated images\n",
    "    random_z = tf.random.normal((n_samples, vae.latent_dim))\n",
    "    generated = vae.decode(random_z)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(3, n_samples, 2 * n_samples + i + 1)\n",
    "        plt.imshow(generated[i, :, :, 0], cmap='gray')\n",
    "        plt.title(f'Generated {i+1}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('VAE Results: Original, Reconstructed, Generated')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_vae_results(vae, x_test)\n",
    "```\n",
    "\n",
    "## 2. Latent Space Analysis and Manipulation\n",
    "\n",
    "```python\n",
    "# Latent space analysis tools\n",
    "class LatentSpaceAnalyzer:\n",
    "    \"\"\"Tools for analyzing and manipulating VAE latent space\"\"\"\n",
    "    \n",
    "    def __init__(self, vae_model):\n",
    "        self.vae = vae_model\n",
    "    \n",
    "    def plot_latent_space(self, data, labels, n_samples=2000):\n",
    "        \"\"\"Plot 2D latent space with class labels\"\"\"\n",
    "        \n",
    "        if self.vae.latent_dim != 2:\n",
    "            print(\"Latent space visualization only available for 2D latent space\")\n",
    "            return\n",
    "        \n",
    "        # Encode samples\n",
    "        sample_data = data[:n_samples]\n",
    "        sample_labels = labels[:n_samples]\n",
    "        \n",
    "        z_mean, _ = self.vae.encode(sample_data)\n",
    "        \n",
    "        # Plot latent space\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        scatter = plt.scatter(z_mean[:, 0], z_mean[:, 1], c=sample_labels, \n",
    "                            cmap='tab10', alpha=0.6, s=20)\n",
    "        plt.colorbar(scatter)\n",
    "        plt.xlabel('Latent Dimension 1')\n",
    "        plt.ylabel('Latent Dimension 2')\n",
    "        plt.title('VAE Latent Space Visualization')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    def generate_latent_grid(self, grid_size=15, latent_range=3.0):\n",
    "        \"\"\"Generate images from a grid of latent space points\"\"\"\n",
    "        \n",
    "        if self.vae.latent_dim != 2:\n",
    "            print(\"Grid generation only available for 2D latent space\")\n",
    "            return\n",
    "        \n",
    "        # Create grid of latent points\n",
    "        x = np.linspace(-latent_range, latent_range, grid_size)\n",
    "        y = np.linspace(-latent_range, latent_range, grid_size)\n",
    "        \n",
    "        figure = np.zeros((28 * grid_size, 28 * grid_size))\n",
    "        \n",
    "        for i, yi in enumerate(x):\n",
    "            for j, xi in enumerate(y):\n",
    "                z_sample = np.array([[xi, yi]])\n",
    "                generated = self.vae.decode(z_sample)\n",
    "                digit = generated[0].numpy().reshape(28, 28)\n",
    "                \n",
    "                figure[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = digit\n",
    "        \n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.imshow(figure, cmap='gray')\n",
    "        plt.title(f'VAE Latent Space Grid ({grid_size}x{grid_size})')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    def interpolate_between_points(self, point1, point2, n_steps=10):\n",
    "        \"\"\"Interpolate between two points in latent space\"\"\"\n",
    "        \n",
    "        # Linear interpolation\n",
    "        alphas = np.linspace(0, 1, n_steps)\n",
    "        interpolated_points = []\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            interpolated = alpha * point2 + (1 - alpha) * point1\n",
    "            interpolated_points.append(interpolated)\n",
    "        \n",
    "        interpolated_points = np.array(interpolated_points)\n",
    "        \n",
    "        # Generate images\n",
    "        generated_images = self.vae.decode(interpolated_points)\n",
    "        \n",
    "        # Plot interpolation\n",
    "        plt.figure(figsize=(20, 4))\n",
    "        for i, img in enumerate(generated_images):\n",
    "            plt.subplot(1, n_steps, i + 1)\n",
    "            plt.imshow(img[:, :, 0], cmap='gray')\n",
    "            plt.title(f'α={alphas[i]:.2f}')\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.suptitle('Latent Space Interpolation')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return generated_images\n",
    "    \n",
    "    def find_latent_directions(self, data, labels, class1=0, class2=1):\n",
    "        \"\"\"Find meaningful directions in latent space\"\"\"\n",
    "        \n",
    "        # Get latent representations for two classes\n",
    "        mask1 = labels == class1\n",
    "        mask2 = labels == class2\n",
    "        \n",
    "        data1 = data[mask1][:500]\n",
    "        data2 = data[mask2][:500]\n",
    "        \n",
    "        z_mean1, _ = self.vae.encode(data1)\n",
    "        z_mean2, _ = self.vae.encode(data2)\n",
    "        \n",
    "        # Calculate direction vector\n",
    "        direction = np.mean(z_mean2, axis=0) - np.mean(z_mean1, axis=0)\n",
    "        direction = direction / np.linalg.norm(direction)\n",
    "        \n",
    "        return direction\n",
    "\n",
    "# Beta-VAE for disentanglement\n",
    "class BetaVAE(VAE):\n",
    "    \"\"\"β-VAE for disentangled representation learning\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=10, beta=4.0, **kwargs):\n",
    "        super().__init__(latent_dim=latent_dim, beta=beta, **kwargs)\n",
    "        self.beta = beta\n",
    "    \n",
    "    def disentanglement_metric(self, data, labels, n_samples=1000):\n",
    "        \"\"\"Simple disentanglement metric based on mutual information\"\"\"\n",
    "        \n",
    "        sample_data = data[:n_samples]\n",
    "        sample_labels = labels[:n_samples]\n",
    "        \n",
    "        z_mean, _ = self.encode(sample_data)\n",
    "        \n",
    "        # Calculate mutual information between latent dimensions and labels\n",
    "        mi_scores = []\n",
    "        \n",
    "        for dim in range(self.latent_dim):\n",
    "            # Discretize latent dimension\n",
    "            z_dim = z_mean[:, dim].numpy()\n",
    "            z_discretized = np.digitize(z_dim, bins=np.linspace(z_dim.min(), z_dim.max(), 10))\n",
    "            \n",
    "            # Calculate mutual information (simplified)\n",
    "            mi = self.mutual_information(z_discretized, sample_labels)\n",
    "            mi_scores.append(mi)\n",
    "        \n",
    "        return np.array(mi_scores)\n",
    "    \n",
    "    def mutual_information(self, x, y):\n",
    "        \"\"\"Simplified mutual information calculation\"\"\"\n",
    "        # This is a simplified version - use proper MI calculation in practice\n",
    "        from sklearn.metrics import mutual_info_score\n",
    "        return mutual_info_score(x, y)\n",
    "\n",
    "# Test latent space analysis\n",
    "print(\"=== Latent Space Analysis ===\")\n",
    "\n",
    "analyzer = LatentSpaceAnalyzer(vae)\n",
    "\n",
    "# Plot latent space (only for 2D)\n",
    "analyzer.plot_latent_space(x_test, y_test, n_samples=2000)\n",
    "\n",
    "# Generate latent grid\n",
    "analyzer.generate_latent_grid(grid_size=10, latent_range=2.0)\n",
    "\n",
    "# Test interpolation\n",
    "print(\"\\nTesting latent space interpolation...\")\n",
    "# Find two random points in latent space\n",
    "point1 = np.random.normal(0, 1, (1, vae.latent_dim))\n",
    "point2 = np.random.normal(0, 1, (1, vae.latent_dim))\n",
    "\n",
    "interpolated_images = analyzer.interpolate_between_points(point1, point2, n_steps=10)\n",
    "\n",
    "# Test Beta-VAE\n",
    "print(\"\\n=== Beta-VAE for Disentanglement ===\")\n",
    "\n",
    "beta_vae = BetaVAE(latent_dim=10, beta=4.0, input_shape=(28, 28, 1))\n",
    "beta_vae.compile(optimizer=tf.keras.optimizers.Adam(1e-3))\n",
    "\n",
    "# Build model\n",
    "_ = beta_vae(sample_data)\n",
    "\n",
    "print(\"Training Beta-VAE (demo - 3 epochs)...\")\n",
    "beta_history = beta_vae.fit(x_train[:3000], x_train[:3000],\n",
    "                           epochs=3, batch_size=128, verbose=1)\n",
    "\n",
    "# Analyze disentanglement\n",
    "disentanglement_scores = beta_vae.disentanglement_metric(x_test, y_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(disentanglement_scores)), disentanglement_scores)\n",
    "plt.xlabel('Latent Dimension')\n",
    "plt.ylabel('Disentanglement Score')\n",
    "plt.title('β-VAE Disentanglement Analysis')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## 3. StyleGAN Implementation\n",
    "\n",
    "```python\n",
    "# StyleGAN implementation (simplified version)\n",
    "class StyleGANGenerator(tf.keras.Model):\n",
    "    \"\"\"Simplified StyleGAN generator architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=512, num_layers=8, img_size=64):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Mapping network\n",
    "        self.mapping_network = tf.keras.Sequential([\n",
    "            layers.Dense(latent_dim, activation='relu') for _ in range(8)\n",
    "        ] + [layers.Dense(latent_dim)], name='mapping_network')\n",
    "        \n",
    "        # Synthesis network components\n",
    "        self.constant = tf.Variable(\n",
    "            tf.random.normal((1, 4, 4, 512)), trainable=True, name='constant'\n",
    "        )\n",
    "        \n",
    "        # Style modulation layers\n",
    "        self.style_mods = []\n",
    "        self.to_rgb_layers = []\n",
    "        \n",
    "        channels = [512, 512, 256, 128, 64, 32, 16]\n",
    "        resolutions = [4, 8, 16, 32, 64, 128, 256]\n",
    "        \n",
    "        for i in range(min(len(channels), num_layers)):\n",
    "            # Style modulation\n",
    "            self.style_mods.append(\n",
    "                StyleModulation(channels[i], name=f'style_mod_{i}')\n",
    "            )\n",
    "            \n",
    "            # To RGB conversion\n",
    "            self.to_rgb_layers.append(\n",
    "                layers.Conv2D(3, 1, padding='same', name=f'to_rgb_{i}')\n",
    "            )\n",
    "    \n",
    "    def call(self, latent_codes, truncation_psi=1.0):\n",
    "        batch_size = tf.shape(latent_codes)[0]\n",
    "        \n",
    "        # Map latent codes to intermediate latent space W\n",
    "        w = self.mapping_network(latent_codes)\n",
    "        \n",
    "        # Apply truncation trick\n",
    "        if truncation_psi < 1.0:\n",
    "            w_avg = tf.reduce_mean(w, axis=0, keepdims=True)\n",
    "            w = w_avg + truncation_psi * (w - w_avg)\n",
    "        \n",
    "        # Start with learned constant\n",
    "        x = tf.tile(self.constant, [batch_size, 1, 1, 1])\n",
    "        \n",
    "        # Progressive synthesis\n",
    "        for i, (style_mod, to_rgb) in enumerate(zip(self.style_mods, self.to_rgb_layers)):\n",
    "            # Apply style modulation\n",
    "            x = style_mod(x, w)\n",
    "            \n",
    "            # Upsample (except first layer)\n",
    "            if i > 0:\n",
    "                x = tf.image.resize(x, [x.shape[1] * 2, x.shape[2] * 2])\n",
    "            \n",
    "            # Convert to RGB if this is the output layer\n",
    "            if i == len(self.style_mods) - 1:\n",
    "                rgb = to_rgb(x)\n",
    "                return tf.nn.tanh(rgb)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class StyleModulation(tf.keras.layers.Layer):\n",
    "    \"\"\"Style modulation layer for StyleGAN\"\"\"\n",
    "    \n",
    "    def __init__(self, channels, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.channels = channels\n",
    "        \n",
    "        # Convolutional layer\n",
    "        self.conv = layers.Conv2D(channels, 3, padding='same')\n",
    "        \n",
    "        # Style transformation\n",
    "        self.style_transform = layers.Dense(channels * 2)  # For scale and bias\n",
    "        \n",
    "        # Noise injection\n",
    "        self.noise_strength = tf.Variable(0.0, trainable=True)\n",
    "        \n",
    "    def call(self, x, style_code):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        height, width = tf.shape(x)[1], tf.shape(x)[2]\n",
    "        \n",
    "        # Apply convolution\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        # Add noise\n",
    "        noise = tf.random.normal([batch_size, height, width, 1])\n",
    "        x = x + self.noise_strength * noise\n",
    "        \n",
    "        # Style modulation (AdaIN)\n",
    "        style_params = self.style_transform(style_code)\n",
    "        style_scale, style_bias = tf.split(style_params, 2, axis=-1)\n",
    "        \n",
    "        # Normalize features\n",
    "        x_mean, x_var = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n",
    "        x_normalized = (x - x_mean) / tf.sqrt(x_var + 1e-8)\n",
    "        \n",
    "        # Apply style\n",
    "        style_scale = tf.reshape(style_scale, [-1, 1, 1, self.channels])\n",
    "        style_bias = tf.reshape(style_bias, [-1, 1, 1, self.channels])\n",
    "        \n",
    "        return style_scale * x_normalized + style_bias\n",
    "\n",
    "# Progressive GAN implementation\n",
    "class ProgressiveGAN:\n",
    "    \"\"\"Progressive GAN with growing architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=512, max_resolution=64):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_resolution = max_resolution\n",
    "        self.current_resolution = 4\n",
    "        self.generators = {}\n",
    "        self.discriminators = {}\n",
    "        self.build_networks()\n",
    "    \n",
    "    def build_networks(self):\n",
    "        \"\"\"Build progressive networks for different resolutions\"\"\"\n",
    "        \n",
    "        resolutions = [4, 8, 16, 32, 64, 128, 256]\n",
    "        \n",
    "        for res in resolutions:\n",
    "            if res <= self.max_resolution:\n",
    "                self.generators[res] = self.build_generator(res)\n",
    "                self.discriminators[res] = self.build_discriminator(res)\n",
    "    \n",
    "    def build_generator(self, resolution):\n",
    "        \"\"\"Build generator for specific resolution\"\"\"\n",
    "        \n",
    "        model = tf.keras.Sequential(name=f'generator_{resolution}x{resolution}')\n",
    "        \n",
    "        # Calculate number of upsampling layers\n",
    "        num_layers = int(np.log2(resolution)) - 1\n",
    "        \n",
    "        # Initial dense layer\n",
    "        model.add(layers.Dense(4 * 4 * 512, input_shape=(self.latent_dim,)))\n",
    "        model.add(layers.Reshape((4, 4, 512)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        # Progressive upsampling\n",
    "        channels = 512\n",
    "        for i in range(num_layers):\n",
    "            channels = max(16, channels // 2)\n",
    "            \n",
    "            model.add(layers.Conv2DTranspose(\n",
    "                channels, 4, strides=2, padding='same'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(layers.Conv2D(3, 3, padding='same', activation='tanh'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_discriminator(self, resolution):\n",
    "        \"\"\"Build discriminator for specific resolution\"\"\"\n",
    "        \n",
    "        model = tf.keras.Sequential(name=f'discriminator_{resolution}x{resolution}')\n",
    "        \n",
    "        # Input layer\n",
    "        model.add(layers.Conv2D(32, 3, padding='same', \n",
    "                               input_shape=(resolution, resolution, 3)))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        # Progressive downsampling\n",
    "        channels = 32\n",
    "        current_res = resolution\n",
    "        \n",
    "        while current_res > 4:\n",
    "            channels = min(512, channels * 2)\n",
    "            model.add(layers.Conv2D(channels, 4, strides=2, padding='same'))\n",
    "            model.add(layers.BatchNormalization())\n",
    "            model.add(layers.LeakyReLU(alpha=0.2))\n",
    "            model.add(layers.Dropout(0.3))\n",
    "            current_res //= 2\n",
    "        \n",
    "        # Final layers\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(1))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def grow_network(self, new_resolution):\n",
    "        \"\"\"Grow network to new resolution\"\"\"\n",
    "        \n",
    "        if new_resolution in self.generators:\n",
    "            self.current_resolution = new_resolution\n",
    "            print(f\"Switched to {new_resolution}x{new_resolution} resolution\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Test StyleGAN components\n",
    "print(\"=== StyleGAN Implementation Test ===\")\n",
    "\n",
    "# Create StyleGAN generator\n",
    "stylegan_gen = StyleGANGenerator(latent_dim=512, num_layers=4, img_size=64)\n",
    "\n",
    "# Test with random latent codes\n",
    "test_latents = tf.random.normal((4, 512))\n",
    "stylegan_outputs = stylegan_gen(test_latents, truncation_psi=0.7)\n",
    "\n",
    "print(f\"StyleGAN Generator:\")\n",
    "print(f\"Input latent shape: {test_latents.shape}\")\n",
    "print(f\"Output image shape: {stylegan_outputs.shape}\")\n",
    "print(f\"Parameters: {stylegan_gen.count_params():,}\")\n",
    "\n",
    "# Visualize StyleGAN outputs\n",
    "def visualize_stylegan_outputs(generator, n_samples=8):\n",
    "    \"\"\"Visualize StyleGAN generated images\"\"\"\n",
    "    \n",
    "    latent_codes = tf.random.normal((n_samples, generator.latent_dim))\n",
    "    generated_images = generator(latent_codes, truncation_psi=0.7)\n",
    "    \n",
    "    # Rescale from [-1, 1] to [0, 1]\n",
    "    generated_images = (generated_images + 1.0) / 2.0\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(2, 4, i + 1)\n",
    "        plt.imshow(generated_images[i])\n",
    "        plt.title(f'Generated {i+1}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('StyleGAN Generated Images')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_stylegan_outputs(stylegan_gen, n_samples=8)\n",
    "\n",
    "# Test Progressive GAN\n",
    "print(\"\\n=== Progressive GAN Test ===\")\n",
    "\n",
    "progressive_gan = ProgressiveGAN(latent_dim=512, max_resolution=32)\n",
    "\n",
    "print(\"Progressive GAN Architecture:\")\n",
    "for res in [4, 8, 16, 32]:\n",
    "    if res in progressive_gan.generators:\n",
    "        gen_params = progressive_gan.generators[res].count_params()\n",
    "        disc_params = progressive_gan.discriminators[res].count_params()\n",
    "        print(f\"  {res}x{res}: Generator {gen_params:,}, Discriminator {disc_params:,}\")\n",
    "\n",
    "# Test generation at different resolutions\n",
    "test_latents = tf.random.normal((4, 512))\n",
    "\n",
    "for resolution in [4, 8, 16, 32]:\n",
    "    if resolution in progressive_gan.generators:\n",
    "        gen_images = progressive_gan.generators[resolution](test_latents)\n",
    "        print(f\"Resolution {resolution}x{resolution}: Output shape {gen_images.shape}\")\n",
    "```\n",
    "\n",
    "## 4. CycleGAN Implementation\n",
    "\n",
    "```python\n",
    "# CycleGAN for unpaired image-to-image translation\n",
    "class CycleGAN:\n",
    "    \"\"\"CycleGAN for unpaired image-to-image translation\"\"\"\n",
    "    \n",
    "    def __init__(self, img_shape=(64, 64, 3)):\n",
    "        self.img_shape = img_shape\n",
    "        \n",
    "        # Build generators and discriminators\n",
    "        self.G_AB = self.build_generator(name='G_AB')  # A to B\n",
    "        self.G_BA = self.build_generator(name='G_BA')  # B to A\n",
    "        self.D_A = self.build_discriminator(name='D_A')\n",
    "        self.D_B = self.build_discriminator(name='D_B')\n",
    "        \n",
    "        # Optimizers\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        \n",
    "        # Loss weights\n",
    "        self.lambda_cycle = 10.0\n",
    "        self.lambda_identity = 0.5\n",
    "    \n",
    "    def build_generator(self, name):\n",
    "        \"\"\"Build U-Net style generator\"\"\"\n",
    "        \n",
    "        inputs = layers.Input(shape=self.img_shape)\n",
    "        \n",
    "        # Encoder\n",
    "        e1 = layers.Conv2D(64, 4, strides=2, padding='same')(inputs)\n",
    "        e1 = layers.LeakyReLU(alpha=0.2)(e1)\n",
    "        \n",
    "        e2 = layers.Conv2D(128, 4, strides=2, padding='same')(e1)\n",
    "        e2 = layers.BatchNormalization()(e2)\n",
    "        e2 = layers.LeakyReLU(alpha=0.2)(e2)\n",
    "        \n",
    "        e3 = layers.Conv2D(256, 4, strides=2, padding='same')(e2)\n",
    "        e3 = layers.BatchNormalization()(e3)\n",
    "        e3 = layers.LeakyReLU(alpha=0.2)(e3)\n",
    "        \n",
    "        e4 = layers.Conv2D(512, 4, strides=2, padding='same')(e3)\n",
    "        e4 = layers.BatchNormalization()(e4)\n",
    "        e4 = layers.LeakyReLU(alpha=0.2)(e4)\n",
    "        \n",
    "        # Bottleneck with residual blocks\n",
    "        bottleneck = e4\n",
    "        for _ in range(6):\n",
    "            bottleneck = self.residual_block(bottleneck, 512)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        d4 = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(bottleneck)\n",
    "        d4 = layers.BatchNormalization()(d4)\n",
    "        d4 = layers.ReLU()(d4)\n",
    "        d4 = layers.Concatenate()([d4, e3])\n",
    "        \n",
    "        d3 = layers.Conv2DTranspose(128, 4, strides=2, padding='same')(d4)\n",
    "        d3 = layers.BatchNormalization()(d3)\n",
    "        d3 = layers.ReLU()(d3)\n",
    "        d3 = layers.Concatenate()([d3, e2])\n",
    "        \n",
    "        d2 = layers.Conv2DTranspose(64, 4, strides=2, padding='same')(d3)\n",
    "        d2 = layers.BatchNormalization()(d2)\n",
    "        d2 = layers.ReLU()(d2)\n",
    "        d2 = layers.Concatenate()([d2, e1])\n",
    "        \n",
    "        outputs = layers.Conv2DTranspose(3, 4, strides=2, padding='same', activation='tanh')(d2)\n",
    "        \n",
    "        return tf.keras.Model(inputs, outputs, name=name)\n",
    "    \n",
    "    def residual_block(self, x, filters):\n",
    "        \"\"\"Residual block for generator\"\"\"\n",
    "        \n",
    "        shortcut = x\n",
    "        \n",
    "        x = layers.Conv2D(filters, 3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        \n",
    "        x = layers.Conv2D(filters, 3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        return layers.Add()([shortcut, x])\n",
    "    \n",
    "    def build_discriminator(self, name):\n",
    "        \"\"\"Build PatchGAN discriminator\"\"\"\n",
    "        \n",
    "        inputs = layers.Input(shape=self.img_shape)\n",
    "        \n",
    "        x = layers.Conv2D(64, 4, strides=2, padding='same')(inputs)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.Conv2D(128, 4, strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.Conv2D(256, 4, strides=2, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        x = layers.Conv2D(512, 4, strides=1, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "        outputs = layers.Conv2D(1, 4, strides=1, padding='same')(x)\n",
    "        \n",
    "        return tf.keras.Model(inputs, outputs, name=name)\n",
    "    \n",
    "    def generator_loss(self, fake_output):\n",
    "        \"\"\"Adversarial loss for generator\"\"\"\n",
    "        return tf.keras.losses.binary_crossentropy(\n",
    "            tf.ones_like(fake_output), fake_output, from_logits=True\n",
    "        )\n",
    "    \n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        \"\"\"Adversarial loss for discriminator\"\"\"\n",
    "        real_loss = tf.keras.losses.binary_crossentropy(\n",
    "            tf.ones_like(real_output), real_output, from_logits=True\n",
    "        )\n",
    "        fake_loss = tf.keras.losses.binary_crossentropy(\n",
    "            tf.zeros_like(fake_output), fake_output, from_logits=True\n",
    "        )\n",
    "        return real_loss + fake_loss\n",
    "    \n",
    "    def cycle_consistency_loss(self, real_image, reconstructed_image):\n",
    "        \"\"\"Cycle consistency loss\"\"\"\n",
    "        return tf.reduce_mean(tf.abs(real_image - reconstructed_image))\n",
    "    \n",
    "    def identity_loss(self, real_image, same_image):\n",
    "        \"\"\"Identity loss\"\"\"\n",
    "        return tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, real_A, real_B):\n",
    "        \"\"\"Training step for CycleGAN\"\"\"\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Forward cycle: A -> B -> A\n",
    "            fake_B = self.G_AB(real_A, training=True)\n",
    "            reconstructed_A = self.G_BA(fake_B, training=True)\n",
    "            \n",
    "            # Backward cycle: B -> A -> B\n",
    "            fake_A = self.G_BA(real_B, training=True)\n",
    "            reconstructed_B = self.G_AB(fake_A, training=True)\n",
    "            \n",
    "            # Identity mapping\n",
    "            same_A = self.G_BA(real_A, training=True)\n",
    "            same_B = self.G_AB(real_B, training=True)\n",
    "            \n",
    "            # Discriminator outputs\n",
    "            disc_real_A = self.D_A(real_A, training=True)\n",
    "            disc_fake_A = self.D_A(fake_A, training=True)\n",
    "            \n",
    "            disc_real_B = self.D_B(real_B, training=True)\n",
    "            disc_fake_B = self.D_B(fake_B, training=True)\n",
    "            \n",
    "            # Generator losses\n",
    "            gen_AB_loss = tf.reduce_mean(self.generator_loss(disc_fake_B))\n",
    "            gen_BA_loss = tf.reduce_mean(self.generator_loss(disc_fake_A))\n",
    "            \n",
    "            # Cycle consistency losses\n",
    "            cycle_A_loss = self.cycle_consistency_loss(real_A, reconstructed_A)\n",
    "            cycle_B_loss = self.cycle_consistency_loss(real_B, reconstructed_B)\n",
    "            \n",
    "            # Identity losses\n",
    "            identity_A_loss = self.identity_loss(real_A, same_A)\n",
    "            identity_B_loss = self.identity_loss(real_B, same_B)\n",
    "            \n",
    "            # Total generator loss\n",
    "            total_gen_loss = (gen_AB_loss + gen_BA_loss + \n",
    "                            self.lambda_cycle * (cycle_A_loss + cycle_B_loss) + \n",
    "                            self.lambda_identity * (identity_A_loss + identity_B_loss))\n",
    "            \n",
    "            # Discriminator losses\n",
    "            disc_A_loss = tf.reduce_mean(self.discriminator_loss(disc_real_A, disc_fake_A))\n",
    "            disc_B_loss = tf.reduce_mean(self.discriminator_loss(disc_real_B, disc_fake_B))\n",
    "        \n",
    "        # Calculate gradients\n",
    "        gen_AB_grads = tape.gradient(total_gen_loss, self.G_AB.trainable_variables)\n",
    "        gen_BA_grads = tape.gradient(total_gen_loss, self.G_BA.trainable_variables)\n",
    "        \n",
    "        disc_A_grads = tape.gradient(disc_A_loss, self.D_A.trainable_variables)\n",
    "        disc_B_grads = tape.gradient(disc_B_loss, self.D_B.trainable_variables)\n",
    "        \n",
    "        # Apply gradients\n",
    "        self.g_optimizer.apply_gradients(zip(gen_AB_grads, self.G_AB.trainable_variables))\n",
    "        self.g_optimizer.apply_gradients(zip(gen_BA_grads, self.G_BA.trainable_variables))\n",
    "        \n",
    "        self.d_optimizer.apply_gradients(zip(disc_A_grads, self.D_A.trainable_variables))\n",
    "        self.d_optimizer.apply_gradients(zip(disc_B_grads, self.D_B.trainable_variables))\n",
    "        \n",
    "        return {\n",
    "            'gen_loss': total_gen_loss,\n",
    "            'disc_A_loss': disc_A_loss,\n",
    "            'disc_B_loss': disc_B_loss,\n",
    "            'cycle_loss': cycle_A_loss + cycle_B_loss\n",
    "        }\n",
    "\n",
    "# Test CycleGAN\n",
    "print(\"=== CycleGAN Implementation Test ===\")\n",
    "\n",
    "cycle_gan = CycleGAN(img_shape=(64, 64, 3))\n",
    "\n",
    "print(\"CycleGAN Architecture:\")\n",
    "print(f\"Generator AB: {cycle_gan.G_AB.count_params():,} parameters\")\n",
    "print(f\"Generator BA: {cycle_gan.G_BA.count_params():,} parameters\")\n",
    "print(f\"Discriminator A: {cycle_gan.D_A.count_params():,} parameters\")\n",
    "print(f\"Discriminator B: {cycle_gan.D_B.count_params():,} parameters\")\n",
    "\n",
    "# Create synthetic test data (normally you'd use two different image domains)\n",
    "test_A = tf.random.normal((4, 64, 64, 3))\n",
    "test_B = tf.random.normal((4, 64, 64, 3))\n",
    "\n",
    "# Test forward pass\n",
    "fake_B = cycle_gan.G_AB(test_A)\n",
    "fake_A = cycle_gan.G_BA(test_B)\n",
    "\n",
    "print(f\"\\nCycleGAN Forward Pass:\")\n",
    "print(f\"Real A shape: {test_A.shape}\")\n",
    "print(f\"Fake B shape: {fake_B.shape}\")\n",
    "print(f\"Real B shape: {test_B.shape}\")\n",
    "print(f\"Fake A shape: {fake_A.shape}\")\n",
    "\n",
    "# Visualize CycleGAN translations\n",
    "def visualize_cycle_translations(cycle_gan, real_A, real_B, n_samples=4):\n",
    "    \"\"\"Visualize CycleGAN image translations\"\"\"\n",
    "    \n",
    "    real_A_batch = real_A[:n_samples]\n",
    "    real_B_batch = real_B[:n_samples]\n",
    "    \n",
    "    # Generate translations\n",
    "    fake_B = cycle_gan.G_AB(real_A_batch)\n",
    "    fake_A = cycle_gan.G_BA(real_B_batch)\n",
    "    \n",
    "    # Reconstruct\n",
    "    reconstructed_A = cycle_gan.G_BA(fake_B)\n",
    "    reconstructed_B = cycle_gan.G_AB(fake_A)\n",
    "    \n",
    "    # Normalize for visualization\n",
    "    def normalize_for_display(images):\n",
    "        return (images + 1.0) / 2.0\n",
    "    \n",
    "    real_A_vis = normalize_for_display(real_A_batch)\n",
    "    real_B_vis = normalize_for_display(real_B_batch)\n",
    "    fake_A_vis = normalize_for_display(fake_A)\n",
    "    fake_B_vis = normalize_for_display(fake_B)\n",
    "    reconstructed_A_vis = normalize_for_display(reconstructed_A)\n",
    "    reconstructed_B_vis = normalize_for_display(reconstructed_B)\n",
    "    \n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # A -> B -> A cycle\n",
    "        plt.subplot(n_samples, 6, i * 6 + 1)\n",
    "        plt.imshow(real_A_vis[i])\n",
    "        plt.title('Real A')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(n_samples, 6, i * 6 + 2)\n",
    "        plt.imshow(fake_B_vis[i])\n",
    "        plt.title('Fake B')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(n_samples, 6, i * 6 + 3)\n",
    "        plt.imshow(reconstructed_A_vis[i])\n",
    "        plt.title('Reconstructed A')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # B -> A -> B cycle\n",
    "        plt.subplot(n_samples, 6, i * 6 + 4)\n",
    "        plt.imshow(real_B_vis[i])\n",
    "        plt.title('Real B')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(n_samples, 6, i * 6 + 5)\n",
    "        plt.imshow(fake_A_vis[i])\n",
    "        plt.title('Fake A')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(n_samples, 6, i * 6 + 6)\n",
    "        plt.imshow(reconstructed_B_vis[i])\n",
    "        plt.title('Reconstructed B')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('CycleGAN Image Translation Results')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test visualization with synthetic data\n",
    "visualize_cycle_translations(cycle_gan, test_A, test_B, n_samples=2)\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "This comprehensive notebook demonstrated advanced generative modeling techniques using tf.keras:\n",
    "\n",
    "### Key Implementations\n",
    "\n",
    "**1. Variational Autoencoders (VAEs):**\n",
    "- Reparameterization trick for differentiable sampling\n",
    "- β-VAE for disentangled representations\n",
    "- Convolutional VAE architectures\n",
    "- Latent space analysis and manipulation tools\n",
    "\n",
    "**2. StyleGAN Architecture:**\n",
    "- Mapping network and style modulation\n",
    "- AdaIN (Adaptive Instance Normalization)\n",
    "- Progressive synthesis with learned features\n",
    "- Truncation trick for sample quality\n",
    "\n",
    "**3. Progressive GAN:**\n",
    "- Growing network architecture during training\n",
    "- Multi-resolution training strategies\n",
    "- Stable training for high-resolution generation\n",
    "\n",
    "**4. CycleGAN:**\n",
    "- Unpaired image-to-image translation\n",
    "- Cycle consistency and identity losses\n",
    "- U-Net generator with skip connections\n",
    "- PatchGAN discriminator architecture\n",
    "\n",
    "### Technical Achievements\n",
    "\n",
    "- **Latent Space Control**: VAE enables smooth interpolation and semantic editing\n",
    "- **High-Quality Generation**: StyleGAN produces photorealistic images\n",
    "- **Stable Training**: Progressive training prevents mode collapse\n",
    "- **Domain Transfer**: CycleGAN enables cross-domain translation\n",
    "\n",
    "### Advanced Techniques Demonstrated\n",
    "\n",
    "- **Disentanglement**: β-VAE separates semantic factors\n",
    "- **Style Transfer**: StyleGAN modulates style at multiple scales\n",
    "- **Multi-scale Training**: Progressive GAN grows complexity gradually\n",
    "- **Adversarial Training**: Multiple discriminators for robust learning\n",
    "\n",
    "### Performance Insights\n",
    "\n",
    "- **VAE**: Fast inference, smooth latent space, good for representation learning\n",
    "- **StyleGAN**: Superior image quality, controllable generation\n",
    "- **Progressive GAN**: Stable training, scalable to high resolutions\n",
    "- **CycleGAN**: No paired data required, preserves content structure\n",
    "\n",
    "### Applications Covered\n",
    "\n",
    "- Image reconstruction and generation\n",
    "- Latent space exploration and editing\n",
    "- Style transfer and domain adaptation\n",
    "- Unpaired image translation tasks\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue to notebook 15 (Diffusion Models with tf.keras) to explore the latest breakthrough in generative modeling, where you'll implement DDPM, DDIM, and other state-of-the-art diffusion-based approaches that are revolutionizing image generation.\n",
    "\n",
    "These advanced generative models provide powerful tools for creative applications, data augmentation, and understanding the structure of high-dimensional data distributions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
