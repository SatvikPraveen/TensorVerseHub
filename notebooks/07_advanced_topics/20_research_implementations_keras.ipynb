{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 research implementations keras\n",
    "**Location: TensorVerseHub/notebooks/07_advanced_topics/20_research_implementations_keras.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Implementations with tf.keras\n",
    "\n",
    "**File Location:** `notebooks/07_advanced_topics/20_research_implementations_keras.ipynb`\n",
    "\n",
    "Implement cutting-edge research techniques and state-of-the-art models using tf.keras custom components. Explore advanced architectures, novel training methods, and experimental approaches from recent machine learning research.\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement attention mechanisms beyond standard transformers\n",
    "- Build custom training loops with advanced optimization techniques\n",
    "- Create novel architectural components and layers\n",
    "- Apply recent research findings to practical implementations\n",
    "- Experiment with cutting-edge regularization and normalization methods\n",
    "- Develop research-grade reproducible experiments\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Advanced Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Linear Attention Implementation\n",
    "class LinearAttention(layers.Layer):\n",
    "    \"\"\"Linear attention mechanism for efficient long-sequence processing\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads=8, feature_dim=64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.feature_dim = feature_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        # Linear projections\n",
    "        self.query_proj = layers.Dense(embed_dim, use_bias=False)\n",
    "        self.key_proj = layers.Dense(embed_dim, use_bias=False)\n",
    "        self.value_proj = layers.Dense(embed_dim, use_bias=False)\n",
    "        self.out_proj = layers.Dense(embed_dim)\n",
    "        \n",
    "        # Feature mapping for linear attention\n",
    "        self.feature_map = self._get_feature_map()\n",
    "        \n",
    "    def _get_feature_map(self):\n",
    "        \"\"\"ELU feature map for linear attention\"\"\"\n",
    "        def elu_feature_map(x):\n",
    "            return tf.nn.elu(x) + 1.0\n",
    "        return elu_feature_map\n",
    "    \n",
    "    def call(self, inputs, mask=None, training=None):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.query_proj(inputs)\n",
    "        K = self.key_proj(inputs)\n",
    "        V = self.value_proj(inputs)\n",
    "        \n",
    "        # Reshape for multi-head\n",
    "        Q = tf.reshape(Q, [batch_size, seq_len, self.num_heads, self.head_dim])\n",
    "        K = tf.reshape(K, [batch_size, seq_len, self.num_heads, self.head_dim])\n",
    "        V = tf.reshape(V, [batch_size, seq_len, self.num_heads, self.head_dim])\n",
    "        \n",
    "        # Apply feature map\n",
    "        Q_prime = self.feature_map(Q)\n",
    "        K_prime = self.feature_map(K)\n",
    "        \n",
    "        # Linear attention computation: O(n) complexity\n",
    "        # Compute K^T V\n",
    "        KV = tf.einsum('bshd,bshv->bhdv', K_prime, V)\n",
    "        \n",
    "        # Compute normalization\n",
    "        K_sum = tf.reduce_sum(K_prime, axis=1, keepdims=True)  # [B, 1, H, D]\n",
    "        \n",
    "        # Compute attention output\n",
    "        attention_output = tf.einsum('bshd,bhdv->bshv', Q_prime, KV)\n",
    "        attention_norm = tf.einsum('bshd,bhd->bsh', Q_prime, tf.squeeze(K_sum, axis=1))\n",
    "        \n",
    "        # Normalize\n",
    "        attention_output = attention_output / (tf.expand_dims(attention_norm, -1) + 1e-8)\n",
    "        \n",
    "        # Reshape back\n",
    "        attention_output = tf.reshape(attention_output, [batch_size, seq_len, self.embed_dim])\n",
    "        \n",
    "        return self.out_proj(attention_output)\n",
    "\n",
    "# Sparse Attention Implementation\n",
    "class SparseAttention(layers.Layer):\n",
    "    \"\"\"Sparse attention with configurable sparsity patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads=8, block_size=64, sparsity_pattern='local', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.block_size = block_size\n",
    "        self.sparsity_pattern = sparsity_pattern\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.query_proj = layers.Dense(embed_dim, use_bias=False)\n",
    "        self.key_proj = layers.Dense(embed_dim, use_bias=False)\n",
    "        self.value_proj = layers.Dense(embed_dim, use_bias=False)\n",
    "        self.out_proj = layers.Dense(embed_dim)\n",
    "        \n",
    "    def _create_sparse_mask(self, seq_len):\n",
    "        \"\"\"Create sparse attention mask based on pattern\"\"\"\n",
    "        \n",
    "        if self.sparsity_pattern == 'local':\n",
    "            # Local attention pattern\n",
    "            mask = tf.zeros((seq_len, seq_len), dtype=tf.bool)\n",
    "            for i in range(seq_len):\n",
    "                start = max(0, i - self.block_size // 2)\n",
    "                end = min(seq_len, i + self.block_size // 2 + 1)\n",
    "                mask = tf.tensor_scatter_nd_update(\n",
    "                    mask, [[i, j] for j in range(start, end)], \n",
    "                    [True] * (end - start)\n",
    "                )\n",
    "            \n",
    "        elif self.sparsity_pattern == 'strided':\n",
    "            # Strided attention pattern\n",
    "            mask = tf.zeros((seq_len, seq_len), dtype=tf.bool)\n",
    "            stride = self.block_size\n",
    "            \n",
    "            for i in range(seq_len):\n",
    "                # Local connections\n",
    "                local_indices = list(range(max(0, i - 32), min(seq_len, i + 33)))\n",
    "                # Strided connections\n",
    "                strided_indices = list(range(0, seq_len, stride))\n",
    "                \n",
    "                all_indices = list(set(local_indices + strided_indices))\n",
    "                for j in all_indices:\n",
    "                    if j < seq_len:\n",
    "                        mask = tf.tensor_scatter_nd_update(mask, [[i, j]], [True])\n",
    "        \n",
    "        else:  # 'random'\n",
    "            # Random sparse pattern\n",
    "            sparsity_ratio = 0.1\n",
    "            num_connections = int(seq_len * sparsity_ratio)\n",
    "            \n",
    "            mask = tf.zeros((seq_len, seq_len), dtype=tf.bool)\n",
    "            for i in range(seq_len):\n",
    "                indices = tf.random.uniform([num_connections], 0, seq_len, dtype=tf.int32)\n",
    "                updates = tf.ones([num_connections], dtype=tf.bool)\n",
    "                mask = tf.tensor_scatter_nd_update(\n",
    "                    mask, tf.stack([tf.fill([num_connections], i), indices], axis=1), updates\n",
    "                )\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def call(self, inputs, mask=None, training=None):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        \n",
    "        # Create sparse mask\n",
    "        sparse_mask = self._create_sparse_mask(seq_len)\n",
    "        \n",
    "        # Standard attention computation with sparse mask\n",
    "        Q = self.query_proj(inputs)\n",
    "        K = self.key_proj(inputs)\n",
    "        V = self.value_proj(inputs)\n",
    "        \n",
    "        # Reshape for multi-head\n",
    "        Q = tf.reshape(Q, [batch_size, seq_len, self.num_heads, self.head_dim])\n",
    "        K = tf.reshape(K, [batch_size, seq_len, self.num_heads, self.head_dim])\n",
    "        V = tf.reshape(V, [batch_size, seq_len, self.num_heads, self.head_dim])\n",
    "        \n",
    "        Q = tf.transpose(Q, [0, 2, 1, 3])\n",
    "        K = tf.transpose(K, [0, 2, 1, 3])\n",
    "        V = tf.transpose(V, [0, 2, 1, 3])\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = tf.matmul(Q, K, transpose_b=True) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply sparse mask\n",
    "        sparse_mask_expanded = tf.expand_dims(tf.expand_dims(sparse_mask, 0), 0)\n",
    "        scores = tf.where(sparse_mask_expanded, scores, -1e9)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores += (mask * -1e9)\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        attention_output = tf.matmul(attention_weights, V)\n",
    "        \n",
    "        # Reshape back\n",
    "        attention_output = tf.transpose(attention_output, [0, 2, 1, 3])\n",
    "        attention_output = tf.reshape(attention_output, [batch_size, seq_len, self.embed_dim])\n",
    "        \n",
    "        return self.out_proj(attention_output)\n",
    "\n",
    "# Cross-Modal Attention\n",
    "class CrossModalAttention(layers.Layer):\n",
    "    \"\"\"Cross-modal attention for vision-language tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads=8, temperature=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = temperature\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Separate projections for different modalities\n",
    "        self.vision_proj = layers.Dense(embed_dim, use_bias=False, name='vision_proj')\n",
    "        self.text_proj = layers.Dense(embed_dim, use_bias=False, name='text_proj')\n",
    "        \n",
    "        # Cross-modal attention projections\n",
    "        self.cross_query = layers.Dense(embed_dim, use_bias=False)\n",
    "        self.cross_key = layers.Dense(embed_dim, use_bias=False)\n",
    "        self.cross_value = layers.Dense(embed_dim, use_bias=False)\n",
    "        \n",
    "        self.output_proj = layers.Dense(embed_dim)\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        vision_features, text_features = inputs\n",
    "        \n",
    "        batch_size = tf.shape(vision_features)[0]\n",
    "        vision_len = tf.shape(vision_features)[1]\n",
    "        text_len = tf.shape(text_features)[1]\n",
    "        \n",
    "        # Project modalities to common space\n",
    "        vision_proj = self.vision_proj(vision_features)\n",
    "        text_proj = self.text_proj(text_features)\n",
    "        \n",
    "        # Cross-modal attention: text attends to vision\n",
    "        Q = self.cross_query(text_proj)\n",
    "        K = self.cross_key(vision_proj)\n",
    "        V = self.cross_value(vision_proj)\n",
    "        \n",
    "        # Reshape for multi-head\n",
    "        Q = tf.reshape(Q, [batch_size, text_len, self.num_heads, self.head_dim])\n",
    "        K = tf.reshape(K, [batch_size, vision_len, self.num_heads, self.head_dim])\n",
    "        V = tf.reshape(V, [batch_size, vision_len, self.num_heads, self.head_dim])\n",
    "        \n",
    "        Q = tf.transpose(Q, [0, 2, 1, 3])\n",
    "        K = tf.transpose(K, [0, 2, 1, 3])\n",
    "        V = tf.transpose(V, [0, 2, 1, 3])\n",
    "        \n",
    "        # Cross-modal attention computation\n",
    "        scores = tf.matmul(Q, K, transpose_b=True) / (math.sqrt(self.head_dim) * self.temperature)\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        \n",
    "        cross_attended = tf.matmul(attention_weights, V)\n",
    "        cross_attended = tf.transpose(cross_attended, [0, 2, 1, 3])\n",
    "        cross_attended = tf.reshape(cross_attended, [batch_size, text_len, self.embed_dim])\n",
    "        \n",
    "        return self.output_proj(cross_attended)\n",
    "\n",
    "# Test attention mechanisms\n",
    "print(\"=== Testing Advanced Attention Mechanisms ===\")\n",
    "\n",
    "# Create test data\n",
    "batch_size, seq_len, embed_dim = 2, 512, 256\n",
    "test_input = tf.random.normal([batch_size, seq_len, embed_dim])\n",
    "\n",
    "# Test Linear Attention\n",
    "linear_attn = LinearAttention(embed_dim=embed_dim, num_heads=8)\n",
    "linear_output = linear_attn(test_input)\n",
    "print(f\"Linear Attention - Input: {test_input.shape}, Output: {linear_output.shape}\")\n",
    "\n",
    "# Test Sparse Attention\n",
    "sparse_attn = SparseAttention(embed_dim=embed_dim, num_heads=8, sparsity_pattern='local')\n",
    "sparse_output = sparse_attn(test_input)\n",
    "print(f\"Sparse Attention - Input: {test_input.shape}, Output: {sparse_output.shape}\")\n",
    "\n",
    "# Test Cross-Modal Attention\n",
    "vision_features = tf.random.normal([batch_size, 196, embed_dim])  # 14x14 patches\n",
    "text_features = tf.random.normal([batch_size, 77, embed_dim])     # text tokens\n",
    "\n",
    "cross_modal_attn = CrossModalAttention(embed_dim=embed_dim, num_heads=8)\n",
    "cross_output = cross_modal_attn([vision_features, text_features])\n",
    "print(f\"Cross-Modal Attention - Vision: {vision_features.shape}, Text: {text_features.shape}, Output: {cross_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Novel Architectural Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze-and-Excitation Block\n",
    "class SEBlock(layers.Layer):\n",
    "    \"\"\"Squeeze-and-Excitation block for channel attention\"\"\"\n",
    "    \n",
    "    def __init__(self, reduction_ratio=16, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        reduced_channels = max(1, channels // self.reduction_ratio)\n",
    "        \n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.fc1 = layers.Dense(reduced_channels, activation='relu')\n",
    "        self.fc2 = layers.Dense(channels, activation='sigmoid')\n",
    "        self.reshape = layers.Reshape((1, 1, channels))\n",
    "        \n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Squeeze\n",
    "        squeezed = self.global_pool(inputs)\n",
    "        \n",
    "        # Excitation\n",
    "        excited = self.fc1(squeezed)\n",
    "        excited = self.fc2(excited)\n",
    "        excited = self.reshape(excited)\n",
    "        \n",
    "        # Scale\n",
    "        return inputs * excited\n",
    "\n",
    "# Spatial Pyramid Pooling\n",
    "class SPPBlock(layers.Layer):\n",
    "    \"\"\"Spatial Pyramid Pooling for multi-scale feature extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, pool_sizes=[1, 2, 3, 6], **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pool_sizes = pool_sizes\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        \n",
    "        self.pools = []\n",
    "        self.convs = []\n",
    "        \n",
    "        for pool_size in self.pool_sizes:\n",
    "            self.pools.append(layers.AveragePooling2D(pool_size, strides=1, padding='same'))\n",
    "            self.convs.append(layers.Conv2D(channels // len(self.pool_sizes), 1, activation='relu'))\n",
    "        \n",
    "        self.final_conv = layers.Conv2D(channels, 1, activation='relu')\n",
    "        \n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        h, w = tf.shape(inputs)[1], tf.shape(inputs)[2]\n",
    "        \n",
    "        pooled_features = []\n",
    "        \n",
    "        for pool, conv in zip(self.pools, self.convs):\n",
    "            pooled = pool(inputs)\n",
    "            pooled = conv(pooled)\n",
    "            # Resize to original size\n",
    "            pooled = tf.image.resize(pooled, [h, w])\n",
    "            pooled_features.append(pooled)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        concat_features = tf.concat(pooled_features, axis=-1)\n",
    "        \n",
    "        return self.final_conv(concat_features)\n",
    "\n",
    "# Feature Pyramid Network\n",
    "class FPNBlock(layers.Layer):\n",
    "    \"\"\"Feature Pyramid Network block for multi-scale features\"\"\"\n",
    "    \n",
    "    def __init__(self, channels=256, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.channels = channels\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Lateral connections\n",
    "        self.lateral_convs = []\n",
    "        self.output_convs = []\n",
    "        \n",
    "        for _ in range(len(input_shape)):\n",
    "            self.lateral_convs.append(layers.Conv2D(self.channels, 1, use_bias=False))\n",
    "            self.output_convs.append(layers.Conv2D(self.channels, 3, padding='same', use_bias=False))\n",
    "        \n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs should be a list of feature maps from different levels\n",
    "        assert isinstance(inputs, list), \"FPN requires list of feature maps\"\n",
    "        \n",
    "        # Top-down pathway\n",
    "        lateral_features = []\n",
    "        for i, feature_map in enumerate(inputs):\n",
    "            lateral = self.lateral_convs[i](feature_map)\n",
    "            lateral_features.append(lateral)\n",
    "        \n",
    "        # Start from the top (smallest resolution)\n",
    "        fpn_features = [lateral_features[-1]]\n",
    "        \n",
    "        for i in range(len(lateral_features) - 2, -1, -1):\n",
    "            # Upsample and add\n",
    "            upsampled = tf.image.resize(fpn_features[-1], tf.shape(lateral_features[i])[1:3])\n",
    "            fpn_feature = lateral_features[i] + upsampled\n",
    "            fpn_features.append(fpn_feature)\n",
    "        \n",
    "        # Reverse to match input order\n",
    "        fpn_features.reverse()\n",
    "        \n",
    "        # Apply output convolutions\n",
    "        outputs = []\n",
    "        for i, fpn_feature in enumerate(fpn_features):\n",
    "            output = self.output_convs[i](fpn_feature)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Efficient Channel Attention (ECA)\n",
    "class ECABlock(layers.Layer):\n",
    "    \"\"\"Efficient Channel Attention without dimensionality reduction\"\"\"\n",
    "    \n",
    "    def __init__(self, k_size=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.k_size = k_size\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.conv = layers.Conv1D(1, kernel_size=self.k_size, padding='same', use_bias=False)\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Global average pooling\n",
    "        gap = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)  # [B, 1, 1, C]\n",
    "        \n",
    "        # Reshape for 1D convolution\n",
    "        gap = tf.squeeze(gap, axis=1)  # [B, 1, C]\n",
    "        \n",
    "        # 1D convolution\n",
    "        y = self.conv(gap)  # [B, 1, 1]\n",
    "        y = tf.nn.sigmoid(y)\n",
    "        \n",
    "        # Reshape back\n",
    "        y = tf.expand_dims(y, axis=1)  # [B, 1, 1, C]\n",
    "        \n",
    "        return inputs * y\n",
    "\n",
    "# Depthwise Separable Convolution\n",
    "class DepthwiseSeparableConv(layers.Layer):\n",
    "    \"\"\"Depthwise Separable Convolution for efficient computation\"\"\"\n",
    "    \n",
    "    def __init__(self, filters, kernel_size=3, strides=1, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.activation = activation\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.depthwise_conv = layers.DepthwiseConv2D(\n",
    "            kernel_size=self.kernel_size,\n",
    "            strides=self.strides,\n",
    "            padding='same',\n",
    "            use_bias=False\n",
    "        )\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        \n",
    "        self.pointwise_conv = layers.Conv2D(\n",
    "            filters=self.filters,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            use_bias=False\n",
    "        )\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            self.act = layers.ReLU()\n",
    "        elif self.activation == 'swish':\n",
    "            self.act = layers.Lambda(lambda x: x * tf.nn.sigmoid(x))\n",
    "        else:\n",
    "            self.act = layers.Activation(self.activation)\n",
    "        \n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.depthwise_conv(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        x = self.pointwise_conv(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.act(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test architectural components\n",
    "print(\"\\n=== Testing Novel Architectural Components ===\")\n",
    "\n",
    "# Test data for image processing components\n",
    "test_image = tf.random.normal([2, 64, 64, 128])\n",
    "\n",
    "# Test SE Block\n",
    "se_block = SEBlock(reduction_ratio=16)\n",
    "se_output = se_block(test_image)\n",
    "print(f\"SE Block - Input: {test_image.shape}, Output: {se_output.shape}\")\n",
    "\n",
    "# Test SPP Block\n",
    "spp_block = SPPBlock(pool_sizes=[1, 2, 4])\n",
    "spp_output = spp_block(test_image)\n",
    "print(f\"SPP Block - Input: {test_image.shape}, Output: {spp_output.shape}\")\n",
    "\n",
    "# Test ECA Block\n",
    "eca_block = ECABlock(k_size=3)\n",
    "eca_output = eca_block(test_image)\n",
    "print(f\"ECA Block - Input: {test_image.shape}, Output: {eca_output.shape}\")\n",
    "\n",
    "# Test Depthwise Separable Conv\n",
    "dsconv = DepthwiseSeparableConv(filters=256, kernel_size=3)\n",
    "dsconv_output = dsconv(test_image)\n",
    "print(f\"Depthwise Separable Conv - Input: {test_image.shape}, Output: {dsconv_output.shape}\")\n",
    "\n",
    "# Test FPN with multiple feature levels\n",
    "feature_maps = [\n",
    "    tf.random.normal([2, 32, 32, 512]),  # High resolution, low-level features\n",
    "    tf.random.normal([2, 16, 16, 1024]), # Medium resolution\n",
    "    tf.random.normal([2, 8, 8, 2048])    # Low resolution, high-level features\n",
    "]\n",
    "\n",
    "fpn_block = FPNBlock(channels=256)\n",
    "fpn_outputs = fpn_block(feature_maps)\n",
    "print(f\"FPN Block - Inputs: {[fm.shape for fm in feature_maps]}\")\n",
    "print(f\"FPN Outputs: {[out.shape for out in fpn_outputs]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Training Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Supervised Learning Components\n",
    "class SimCLRLoss(layers.Layer):\n",
    "    \"\"\"SimCLR contrastive loss for self-supervised learning\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=0.07, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def call(self, z_i, z_j):\n",
    "        \"\"\"\n",
    "        z_i, z_j: [batch_size, embedding_dim] - augmented pairs\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(z_i)[0]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        z_i = tf.nn.l2_normalize(z_i, axis=1)\n",
    "        z_j = tf.nn.l2_normalize(z_j, axis=1)\n",
    "        \n",
    "        # Concatenate all embeddings\n",
    "        z = tf.concat([z_i, z_j], axis=0)  # [2*batch_size, embedding_dim]\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        sim_matrix = tf.matmul(z, z, transpose_b=True) / self.temperature\n",
    "        \n",
    "        # Create labels (positive pairs)\n",
    "        labels = tf.range(2 * batch_size)\n",
    "        labels = tf.where(labels < batch_size, labels + batch_size, labels - batch_size)\n",
    "        \n",
    "        # Exclude self-similarities\n",
    "        mask = tf.eye(2 * batch_size, dtype=tf.bool)\n",
    "        sim_matrix = tf.where(mask, -1e9, sim_matrix)\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=labels, logits=sim_matrix\n",
    "        )\n",
    "        \n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "# Momentum Encoder for MoCo\n",
    "class MomentumEncoder(layers.Layer):\n",
    "    \"\"\"Momentum encoder for MoCo self-supervised learning\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, momentum=0.999, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.momentum = momentum\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        # Create momentum encoder (copy of encoder)\n",
    "        self.momentum_encoder = tf.keras.models.clone_model(encoder)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        \n",
    "        # Initialize momentum encoder with encoder weights\n",
    "        for target, source in zip(self.momentum_encoder.weights, self.encoder.weights):\n",
    "            target.assign(source)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        # Update momentum encoder weights\n",
    "        if training:\n",
    "            for target, source in zip(self.momentum_encoder.weights, self.encoder.weights):\n",
    "                target.assign(self.momentum * target + (1 - self.momentum) * source)\n",
    "        \n",
    "        return self.momentum_encoder(inputs, training=False)\n",
    "\n",
    "# Advanced Regularization Techniques\n",
    "class DropBlock(layers.Layer):\n",
    "    \"\"\"DropBlock regularization for convolutional networks\"\"\"\n",
    "    \n",
    "    def __init__(self, drop_rate=0.1, block_size=7, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        if not training:\n",
    "            return inputs\n",
    "        \n",
    "        # Get input dimensions\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        height = tf.shape(inputs)[1]\n",
    "        width = tf.shape(inputs)[2]\n",
    "        channels = tf.shape(inputs)[3]\n",
    "        \n",
    "        # Calculate gamma (drop probability for each location)\n",
    "        gamma = (self.drop_rate * height * width) / (self.block_size ** 2 * \n",
    "                (height - self.block_size + 1) * (width - self.block_size + 1))\n",
    "        \n",
    "        # Generate random mask\n",
    "        mask_shape = [batch_size, height - self.block_size + 1, \n",
    "                     width - self.block_size + 1, channels]\n",
    "        mask = tf.random.uniform(mask_shape) < gamma\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        \n",
    "        # Expand mask to block size\n",
    "        mask = tf.image.resize(mask, [height, width], method='nearest')\n",
    "        \n",
    "        # Apply mask\n",
    "        mask = 1.0 - mask\n",
    "        outputs = inputs * mask\n",
    "        \n",
    "        # Rescale to maintain expected value\n",
    "        outputs = outputs * tf.reduce_numel(mask) / tf.reduce_sum(mask)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Stochastic Depth\n",
    "class StochasticDepth(layers.Layer):\n",
    "    \"\"\"Stochastic depth regularization\"\"\"\n",
    "    \n",
    "    def __init__(self, survival_probability=0.8, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.survival_probability = survival_probability\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        if not training:\n",
    "            return inputs\n",
    "        \n",
    "        # Randomly drop the entire layer\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        random_tensor = self.survival_probability + tf.random.uniform([batch_size, 1, 1, 1])\n",
    "        binary_tensor = tf.floor(random_tensor)\n",
    "        \n",
    "        # Scale by survival probability during training\n",
    "        return inputs * binary_tensor / self.survival_probability\n",
    "\n",
    "# Mixup Data Augmentation\n",
    "class MixupCallback(keras.callbacks.Callback):\n",
    "    \"\"\"Mixup data augmentation callback\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # This would be implemented in the data pipeline\n",
    "        pass\n",
    "    \n",
    "    def mixup_batch(self, x, y, alpha=None):\n",
    "        \"\"\"Apply mixup to a batch\"\"\"\n",
    "        if alpha is None:\n",
    "            alpha = self.alpha\n",
    "            \n",
    "        batch_size = tf.shape(x)[0]\n",
    "        \n",
    "        # Sample lambda from Beta distribution\n",
    "        lam = tf.random.gamma([batch_size], alpha, alpha)\n",
    "        lam = tf.maximum(lam, 1 - lam)  # Ensure lambda >= 0.5\n",
    "        lam = tf.reshape(lam, [-1, 1, 1, 1])\n",
    "        \n",
    "        # Shuffle indices\n",
    "        indices = tf.random.shuffle(tf.range(batch_size))\n",
    "        \n",
    "        # Mix inputs and labels\n",
    "        mixed_x = lam * x + (1 - lam) * tf.gather(x, indices)\n",
    "        \n",
    "        if len(y.shape) == 1:  # Sparse labels\n",
    "            y_onehot = tf.one_hot(y, depth=tf.reduce_max(y) + 1)\n",
    "        else:\n",
    "            y_onehot = y\n",
    "            \n",
    "        mixed_y = lam[:, 0, 0, 0, tf.newaxis] * y_onehot + \\\n",
    "                  (1 - lam[:, 0, 0, 0, tf.newaxis]) * tf.gather(y_onehot, indices)\n",
    "        \n",
    "        return mixed_x, mixed_y\n",
    "\n",
    "# Label Smoothing\n",
    "class LabelSmoothing(layers.Layer):\n",
    "    \"\"\"Label smoothing regularization\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, smoothing=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "    def call(self, labels):\n",
    "        # Convert to one-hot if necessary\n",
    "        if len(labels.shape) == 1:\n",
    "            labels = tf.one_hot(labels, self.num_classes)\n",
    "        \n",
    "        # Apply smoothing\n",
    "        smooth_labels = labels * (1 - self.smoothing) + \\\n",
    "                       self.smoothing / self.num_classes\n",
    "        \n",
    "        return smooth_labels\n",
    "\n",
    "# Test advanced training techniques\n",
    "print(\"\\n=== Testing Advanced Training Techniques ===\")\n",
    "\n",
    "# Test SimCLR Loss\n",
    "embedding_dim = 128\n",
    "z_i = tf.random.normal([32, embedding_dim])\n",
    "z_j = tf.random.normal([32, embedding_dim])\n",
    "\n",
    "simclr_loss = SimCLRLoss(temperature=0.07)\n",
    "contrastive_loss = simclr_loss(z_i, z_j)\n",
    "print(f\"SimCLR Loss: {contrastive_loss.numpy():.4f}\")\n",
    "\n",
    "# Test DropBlock\n",
    "drop_block = DropBlock(drop_rate=0.1, block_size=7)\n",
    "test_input = tf.random.normal([4, 32, 32, 64])\n",
    "dropped_output = drop_block(test_input, training=True)\n",
    "print(f\"DropBlock - Input: {test_input.shape}, Output: {dropped_output.shape}\")\n",
    "\n",
    "# Test Stochastic Depth\n",
    "stoch_depth = StochasticDepth(survival_probability=0.8)\n",
    "stoch_output = stoch_depth(test_input, training=True)\n",
    "print(f\"Stochastic Depth - Survival rate: {tf.reduce_mean(tf.cast(stoch_output != 0, tf.float32)).numpy():.2f}\")\n",
    "\n",
    "# Test Mixup\n",
    "mixup = MixupCallback(alpha=0.2)\n",
    "test_images = tf.random.normal([8, 32, 32, 3])\n",
    "test_labels = tf.random.uniform([8], 0, 10, dtype=tf.int32)\n",
    "\n",
    "mixed_x, mixed_y = mixup.mixup_batch(test_images, test_labels)\n",
    "print(f\"Mixup - Original: {test_images.shape}, Mixed: {mixed_x.shape}\")\n",
    "print(f\"Label mixing - Original: {test_labels.shape}, Mixed: {mixed_y.shape}\")\n",
    "\n",
    "# Test Label Smoothing\n",
    "label_smoother = LabelSmoothing(num_classes=10, smoothing=0.1)\n",
    "sparse_labels = tf.constant([0, 1, 2, 3, 4])\n",
    "smoothed_labels = label_smoother(sparse_labels)\n",
    "print(f\"Label Smoothing - Original: {sparse_labels.shape}, Smoothed: {smoothed_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Optimization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Optimizer with Lookahead\n",
    "class LookaheadOptimizer(keras.optimizers.Optimizer):\n",
    "    \"\"\"Lookahead optimizer wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, k=5, alpha=0.5, name=\"Lookahead\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self._build_index = None\n",
    "        \n",
    "        # Lookahead variables\n",
    "        self.slow_weights = []\n",
    "        self.step_count = 0\n",
    "        \n",
    "    def build(self, variables):\n",
    "        super().build(variables)\n",
    "        self.optimizer.build(variables)\n",
    "        \n",
    "        # Initialize slow weights\n",
    "        self.slow_weights = []\n",
    "        for var in variables:\n",
    "            self.slow_weights.append(\n",
    "                self.add_variable(\n",
    "                    shape=var.shape,\n",
    "                    dtype=var.dtype,\n",
    "                    name=f\"slow_{var.name.split(':')[0]}\",\n",
    "                    initializer=\"zeros\"\n",
    "                )\n",
    "            )\n",
    "            # Copy initial values\n",
    "            self.slow_weights[-1].assign(var)\n",
    "    \n",
    "    def update_step(self, gradient, variable):\n",
    "        # Update fast weights using base optimizer\n",
    "        self.optimizer.update_step(gradient, variable)\n",
    "        \n",
    "        # Update step count\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Lookahead update every k steps\n",
    "        if self.step_count % self.k == 0:\n",
    "            for i, var in enumerate(self.trainable_variables):\n",
    "                if var in self.variables:\n",
    "                    idx = self.variables.index(var)\n",
    "                    slow_var = self.slow_weights[idx]\n",
    "                    \n",
    "                    # Lookahead update: slow = slow + alpha * (fast - slow)\n",
    "                    slow_var.assign(slow_var + self.alpha * (var - slow_var))\n",
    "                    # Update fast weights to slow weights\n",
    "                    var.assign(slow_var)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"optimizer\": keras.optimizers.serialize(self.optimizer),\n",
    "            \"k\": self.k,\n",
    "            \"alpha\": self.alpha,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Cosine Annealing with Warm Restarts\n",
    "class CosineAnnealingWarmRestarts(keras.callbacks.Callback):\n",
    "    \"\"\"Cosine annealing with warm restarts scheduler\"\"\"\n",
    "    \n",
    "    def __init__(self, T_0=10, T_mult=2, eta_min=1e-6, eta_max=1e-3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.eta_min = eta_min\n",
    "        self.eta_max = eta_max\n",
    "        \n",
    "        self.T_cur = 0\n",
    "        self.T_i = T_0\n",
    "        self.restart_count = 0\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Calculate learning rate\n",
    "        lr = self.eta_min + (self.eta_max - self.eta_min) * \\\n",
    "             (1 + math.cos(math.pi * self.T_cur / self.T_i)) / 2\n",
    "        \n",
    "        # Set learning rate\n",
    "        self.model.optimizer.learning_rate.assign(lr)\n",
    "        \n",
    "        # Update counters\n",
    "        self.T_cur += 1\n",
    "        \n",
    "        # Check for restart\n",
    "        if self.T_cur >= self.T_i:\n",
    "            self.T_cur = 0\n",
    "            self.T_i *= self.T_mult\n",
    "            self.restart_count += 1\n",
    "            \n",
    "        print(f\"Epoch {epoch}: Learning rate = {lr:.6f}\")\n",
    "\n",
    "# Gradient Centralization\n",
    "class GradientCentralization(keras.callbacks.Callback):\n",
    "    \"\"\"Gradient centralization for improved optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        # This would be implemented in a custom training loop\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def centralize_gradients(gradients):\n",
    "        \"\"\"Centralize gradients\"\"\"\n",
    "        centralized_gradients = []\n",
    "        \n",
    "        for grad in gradients:\n",
    "            if grad is not None and len(grad.shape) > 1:\n",
    "                # Centralize gradient by subtracting mean\n",
    "                grad_mean = tf.reduce_mean(grad, axis=tuple(range(len(grad.shape) - 1)), \n",
    "                                         keepdims=True)\n",
    "                centralized_grad = grad - grad_mean\n",
    "                centralized_gradients.append(centralized_grad)\n",
    "            else:\n",
    "                centralized_gradients.append(grad)\n",
    "                \n",
    "        return centralized_gradients\n",
    "\n",
    "# Custom Learning Rate Schedule\n",
    "def cyclical_learning_rate(step, base_lr=1e-5, max_lr=1e-2, step_size=1000, mode='triangular'):\n",
    "    \"\"\"Cyclical learning rate schedule\"\"\"\n",
    "    cycle = tf.floor(1 + step / (2 * step_size))\n",
    "    x = tf.abs(step / step_size - 2 * cycle + 1)\n",
    "    \n",
    "    if mode == 'triangular':\n",
    "        lr = base_lr + (max_lr - base_lr) * tf.maximum(0.0, 1 - x)\n",
    "    elif mode == 'triangular2':\n",
    "        lr = base_lr + (max_lr - base_lr) * tf.maximum(0.0, 1 - x) / (2 ** (cycle - 1))\n",
    "    elif mode == 'exp_range':\n",
    "        gamma = 0.99994\n",
    "        lr = base_lr + (max_lr - base_lr) * tf.maximum(0.0, 1 - x) * (gamma ** step)\n",
    "    else:\n",
    "        lr = base_lr + (max_lr - base_lr) * tf.maximum(0.0, 1 - x)\n",
    "    \n",
    "    return lr\n",
    "\n",
    "# Test optimization techniques\n",
    "print(\"\\n=== Testing Advanced Optimization Techniques ===\")\n",
    "\n",
    "# Create a simple model for testing\n",
    "test_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Test Lookahead optimizer\n",
    "base_optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "lookahead_optimizer = LookaheadOptimizer(base_optimizer, k=5, alpha=0.5)\n",
    "\n",
    "# Test cyclical learning rate\n",
    "steps = tf.range(0, 5000, dtype=tf.float32)\n",
    "clr_schedule = [cyclical_learning_rate(step) for step in steps[:100]]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(clr_schedule)\n",
    "plt.title('Cyclical Learning Rate Schedule')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"CLR - Min: {min(clr_schedule):.6f}, Max: {max(clr_schedule):.6f}\")\n",
    "\n",
    "# Test gradient centralization\n",
    "sample_gradients = [\n",
    "    tf.random.normal([32, 64]),  # Weight gradient\n",
    "    tf.random.normal([64]),      # Bias gradient\n",
    "    tf.random.normal([64, 32])   # Another weight gradient\n",
    "]\n",
    "\n",
    "centralized_grads = GradientCentralization.centralize_gradients(sample_gradients)\n",
    "print(f\"Gradient centralization applied to {len(centralized_grads)} gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Interpretability and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient-based Attribution Methods\n",
    "class IntegratedGradients:\n",
    "    \"\"\"Integrated Gradients for model interpretability\"\"\"\n",
    "    \n",
    "    def __init__(self, model, baseline=None):\n",
    "        self.model = model\n",
    "        self.baseline = baseline\n",
    "        \n",
    "    def compute_gradients(self, inputs, target_class=None):\n",
    "        \"\"\"Compute gradients with respect to inputs\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(inputs)\n",
    "            predictions = self.model(inputs)\n",
    "            \n",
    "            if target_class is not None:\n",
    "                predictions = predictions[:, target_class]\n",
    "            else:\n",
    "                predictions = tf.reduce_max(predictions, axis=1)\n",
    "                \n",
    "        gradients = tape.gradient(predictions, inputs)\n",
    "        return gradients\n",
    "    \n",
    "    def integrated_gradients(self, inputs, target_class=None, steps=50):\n",
    "        \"\"\"Compute integrated gradients\"\"\"\n",
    "        if self.baseline is None:\n",
    "            baseline = tf.zeros_like(inputs)\n",
    "        else:\n",
    "            baseline = self.baseline\n",
    "            \n",
    "        # Generate path from baseline to input\n",
    "        alphas = tf.linspace(0.0, 1.0, steps + 1)\n",
    "        \n",
    "        gradients = []\n",
    "        for alpha in alphas:\n",
    "            interpolated = baseline + alpha * (inputs - baseline)\n",
    "            grad = self.compute_gradients(interpolated, target_class)\n",
    "            gradients.append(grad)\n",
    "        \n",
    "        # Average gradients and multiply by path\n",
    "        avg_gradients = tf.reduce_mean(gradients, axis=0)\n",
    "        integrated_grads = (inputs - baseline) * avg_gradients\n",
    "        \n",
    "        return integrated_grads\n",
    "\n",
    "# LIME-like Local Interpretability\n",
    "class LocalLinearApproximation:\n",
    "    \"\"\"Local linear approximation for interpretability\"\"\"\n",
    "    \n",
    "    def __init__(self, model, num_samples=1000, kernel_width=0.25):\n",
    "        self.model = model\n",
    "        self.num_samples = num_samples\n",
    "        self.kernel_width = kernel_width\n",
    "        \n",
    "    def explain_instance(self, instance, feature_mask=None):\n",
    "        \"\"\"Explain a single instance using local linear approximation\"\"\"\n",
    "        if feature_mask is None:\n",
    "            feature_mask = tf.ones_like(instance)\n",
    "        \n",
    "        # Generate perturbed samples\n",
    "        samples = []\n",
    "        weights = []\n",
    "        \n",
    "        for _ in range(self.num_samples):\n",
    "            # Random binary mask for features\n",
    "            mask = tf.cast(tf.random.uniform(tf.shape(instance)) > 0.5, tf.float32)\n",
    "            \n",
    "            # Create perturbed sample\n",
    "            perturbed = instance * mask\n",
    "            samples.append(perturbed)\n",
    "            \n",
    "            # Calculate weight based on distance\n",
    "            distance = tf.reduce_sum(tf.square(instance - perturbed))\n",
    "            weight = tf.exp(-distance / (2 * self.kernel_width ** 2))\n",
    "            weights.append(weight)\n",
    "        \n",
    "        samples = tf.stack(samples)\n",
    "        weights = tf.stack(weights)\n",
    "        \n",
    "        # Get model predictions\n",
    "        predictions = self.model(samples)\n",
    "        if len(predictions.shape) > 1:\n",
    "            predictions = tf.reduce_max(predictions, axis=1)\n",
    "        \n",
    "        # Fit linear model\n",
    "        X = tf.cast(samples, tf.float32)\n",
    "        y = predictions\n",
    "        w = weights\n",
    "        \n",
    "        # Weighted least squares solution\n",
    "        X_weighted = X * tf.expand_dims(tf.sqrt(w), 1)\n",
    "        y_weighted = y * tf.sqrt(w)\n",
    "        \n",
    "        # Add bias term\n",
    "        X_bias = tf.concat([tf.ones((tf.shape(X)[0], 1)), X_weighted], axis=1)\n",
    "        \n",
    "        # Solve normal equation\n",
    "        XTX = tf.matmul(X_bias, X_bias, transpose_a=True)\n",
    "        XTy = tf.matmul(X_bias, tf.expand_dims(y_weighted, 1), transpose_a=True)\n",
    "        \n",
    "        coefficients = tf.linalg.solve(XTX, XTy)\n",
    "        \n",
    "        return coefficients[1:]  # Exclude bias term\n",
    "\n",
    "# Attention Visualization\n",
    "class AttentionVisualizer:\n",
    "    \"\"\"Visualize attention weights in transformer models\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def extract_attention_weights(self, inputs, layer_names=None):\n",
    "        \"\"\"Extract attention weights from transformer layers\"\"\"\n",
    "        attention_weights = {}\n",
    "        \n",
    "        # Create a model that outputs attention weights\n",
    "        outputs = []\n",
    "        x = inputs\n",
    "        \n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            if hasattr(layer, 'attention') or 'attention' in layer.name.lower():\n",
    "                # Modify layer to return attention weights\n",
    "                if hasattr(layer, '__call__'):\n",
    "                    x = layer(x)\n",
    "                    # This is a simplified version - actual implementation\n",
    "                    # would need to modify the attention layer\n",
    "                    attention_weights[f'layer_{i}'] = tf.random.uniform([1, 8, 100, 100])\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        \n",
    "        return attention_weights\n",
    "    \n",
    "    def visualize_attention(self, attention_weights, tokens=None, head_idx=0):\n",
    "        \"\"\"Visualize attention weights as heatmap\"\"\"\n",
    "        for layer_name, weights in attention_weights.items():\n",
    "            if len(weights.shape) == 4:  # [batch, heads, seq, seq]\n",
    "                attn_matrix = weights[0, head_idx].numpy()\n",
    "                \n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.heatmap(attn_matrix, cmap='Blues', \n",
    "                           xticklabels=tokens if tokens else False,\n",
    "                           yticklabels=tokens if tokens else False)\n",
    "                plt.title(f'{layer_name} - Head {head_idx} Attention')\n",
    "                plt.xlabel('Key Positions')\n",
    "                plt.ylabel('Query Positions')\n",
    "                plt.show()\n",
    "\n",
    "# Feature Importance Analysis\n",
    "class FeatureImportanceAnalyzer:\n",
    "    \"\"\"Analyze feature importance using various methods\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def permutation_importance(self, X, y, n_repeats=10, metric='accuracy'):\n",
    "        \"\"\"Compute permutation importance\"\"\"\n",
    "        baseline_score = self._evaluate_model(X, y, metric)\n",
    "        \n",
    "        importances = []\n",
    "        \n",
    "        for feature_idx in range(X.shape[-1]):\n",
    "            scores = []\n",
    "            \n",
    "            for _ in range(n_repeats):\n",
    "                # Permute feature values\n",
    "                X_permuted = X.numpy().copy()\n",
    "                np.random.shuffle(X_permuted[:, feature_idx])\n",
    "                X_permuted = tf.constant(X_permuted)\n",
    "                \n",
    "                # Calculate score with permuted feature\n",
    "                score = self._evaluate_model(X_permuted, y, metric)\n",
    "                scores.append(baseline_score - score)\n",
    "            \n",
    "            importances.append(np.mean(scores))\n",
    "        \n",
    "        return np.array(importances)\n",
    "    \n",
    "    def _evaluate_model(self, X, y, metric):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        predictions = self.model(X)\n",
    "        \n",
    "        if metric == 'accuracy':\n",
    "            if len(predictions.shape) > 1 and predictions.shape[1] > 1:\n",
    "                pred_classes = tf.argmax(predictions, axis=1)\n",
    "                true_classes = y if len(y.shape) == 1 else tf.argmax(y, axis=1)\n",
    "            else:\n",
    "                pred_classes = tf.cast(predictions > 0.5, tf.int32)\n",
    "                true_classes = tf.cast(y, tf.int32)\n",
    "            \n",
    "            accuracy = tf.reduce_mean(tf.cast(pred_classes == true_classes, tf.float32))\n",
    "            return accuracy.numpy()\n",
    "        \n",
    "        elif metric == 'mse':\n",
    "            mse = tf.reduce_mean(tf.square(predictions - y))\n",
    "            return mse.numpy()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric: {metric}\")\n",
    "\n",
    "# Test interpretability methods\n",
    "print(\"\\n=== Testing Model Interpretability ===\")\n",
    "\n",
    "# Create a simple model for testing\n",
    "interpretation_model = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(10,)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')  # 3-class classification\n",
    "])\n",
    "\n",
    "interpretation_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generate test data\n",
    "test_X = tf.random.normal([100, 10])\n",
    "test_y = tf.random.uniform([100], 0, 3, dtype=tf.int32)\n",
    "\n",
    "# Test Integrated Gradients\n",
    "ig = IntegratedGradients(interpretation_model)\n",
    "sample_input = test_X[:1]\n",
    "integrated_grads = ig.integrated_gradients(sample_input, target_class=0)\n",
    "print(f\"Integrated Gradients shape: {integrated_grads.shape}\")\n",
    "print(f\"Attribution scores (first 5): {integrated_grads[0, :5].numpy()}\")\n",
    "\n",
    "# Test Local Linear Approximation\n",
    "lla = LocalLinearApproximation(interpretation_model)\n",
    "local_explanation = lla.explain_instance(sample_input[0])\n",
    "print(f\"Local explanation shape: {local_explanation.shape}\")\n",
    "\n",
    "# Test Feature Importance\n",
    "fi_analyzer = FeatureImportanceAnalyzer(interpretation_model)\n",
    "feature_importance = fi_analyzer.permutation_importance(test_X[:50], test_y[:50], n_repeats=5)\n",
    "print(f\"Permutation importance: {feature_importance}\")\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importance)), feature_importance)\n",
    "plt.title('Feature Importance Analysis')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experimental Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Training Loop with Advanced Features\n",
    "class ExperimentalTrainer:\n",
    "    \"\"\"Advanced experimental training loop\"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, loss_fn, metrics=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metrics = metrics or []\n",
    "        \n",
    "        # Training state\n",
    "        self.epoch = 0\n",
    "        self.step = 0\n",
    "        self.train_loss = keras.metrics.Mean()\n",
    "        self.val_loss = keras.metrics.Mean()\n",
    "        \n",
    "        # Advanced features\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.use_mixed_precision = False\n",
    "        \n",
    "        # Tracking\n",
    "        self.train_metrics = [keras.metrics.Mean() for _ in self.metrics]\n",
    "        self.val_metrics = [keras.metrics.Mean() for _ in self.metrics]\n",
    "        \n",
    "        # EMA for model weights\n",
    "        self.use_ema = False\n",
    "        self.ema_decay = 0.9999\n",
    "        self.ema_weights = []\n",
    "        \n",
    "    def setup_mixed_precision(self):\n",
    "        \"\"\"Setup mixed precision training\"\"\"\n",
    "        self.use_mixed_precision = True\n",
    "        policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "        keras.mixed_precision.set_global_policy(policy)\n",
    "        \n",
    "        # Wrap optimizer for mixed precision\n",
    "        self.optimizer = keras.mixed_precision.LossScaleOptimizer(self.optimizer)\n",
    "        \n",
    "    def setup_ema(self, decay=0.9999):\n",
    "        \"\"\"Setup Exponential Moving Average\"\"\"\n",
    "        self.use_ema = True\n",
    "        self.ema_decay = decay\n",
    "        \n",
    "        # Initialize EMA weights\n",
    "        self.ema_weights = []\n",
    "        for weight in self.model.trainable_weights:\n",
    "            self.ema_weights.append(tf.Variable(weight, trainable=False))\n",
    "    \n",
    "    def update_ema(self):\n",
    "        \"\"\"Update EMA weights\"\"\"\n",
    "        if not self.use_ema:\n",
    "            return\n",
    "            \n",
    "        for ema_weight, weight in zip(self.ema_weights, self.model.trainable_weights):\n",
    "            ema_weight.assign(self.ema_decay * ema_weight + (1 - self.ema_decay) * weight)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, x, y):\n",
    "        \"\"\"Single training step with advanced features\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            predictions = self.model(x, training=True)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.loss_fn(y, predictions)\n",
    "            \n",
    "            # Scale loss for mixed precision\n",
    "            if self.use_mixed_precision:\n",
    "                scaled_loss = self.optimizer.get_scaled_loss(loss)\n",
    "            else:\n",
    "                scaled_loss = loss\n",
    "        \n",
    "        # Compute gradients\n",
    "        if self.use_mixed_precision:\n",
    "            scaled_gradients = tape.gradient(scaled_loss, self.model.trainable_weights)\n",
    "            gradients = self.optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "        else:\n",
    "            gradients = tape.gradient(scaled_loss, self.model.trainable_weights)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if self.max_grad_norm > 0:\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, self.max_grad_norm)\n",
    "        \n",
    "        # Apply gradients\n",
    "        if self.step % self.gradient_accumulation_steps == 0:\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "            \n",
    "            # Update EMA\n",
    "            self.update_ema()\n",
    "        \n",
    "        # Update metrics\n",
    "        self.train_loss(loss)\n",
    "        for i, metric in enumerate(self.metrics):\n",
    "            self.train_metrics[i](metric(y, predictions))\n",
    "        \n",
    "        self.step += 1\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def val_step(self, x, y):\n",
    "        \"\"\"Validation step\"\"\"\n",
    "        # Use EMA weights if available\n",
    "        if self.use_ema:\n",
    "            # Temporarily swap weights\n",
    "            original_weights = []\n",
    "            for i, weight in enumerate(self.model.trainable_weights):\n",
    "                original_weights.append(tf.Variable(weight))\n",
    "                weight.assign(self.ema_weights[i])\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = self.model(x, training=False)\n",
    "        loss = self.loss_fn(y, predictions)\n",
    "        \n",
    "        # Restore original weights\n",
    "        if self.use_ema:\n",
    "            for weight, orig in zip(self.model.trainable_weights, original_weights):\n",
    "                weight.assign(orig)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.val_loss(loss)\n",
    "        for i, metric in enumerate(self.metrics):\n",
    "            self.val_metrics[i](metric(y, predictions))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train_epoch(self, train_dataset, val_dataset=None, verbose=True):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        # Reset metrics\n",
    "        self.train_loss.reset_states()\n",
    "        for metric in self.train_metrics:\n",
    "            metric.reset_states()\n",
    "        \n",
    "        # Training loop\n",
    "        for step, (x, y) in enumerate(train_dataset):\n",
    "            loss = self.train_step(x, y)\n",
    "            \n",
    "            if verbose and step % 100 == 0:\n",
    "                print(f\"Step {step}: Loss = {loss:.4f}\")\n",
    "        \n",
    "        # Validation loop\n",
    "        if val_dataset is not None:\n",
    "            self.val_loss.reset_states()\n",
    "            for metric in self.val_metrics:\n",
    "                metric.reset_states()\n",
    "            \n",
    "            for x, y in val_dataset:\n",
    "                self.val_step(x, y)\n",
    "        \n",
    "        # Print epoch results\n",
    "        if verbose:\n",
    "            train_results = f\"Epoch {self.epoch}: Train Loss = {self.train_loss.result():.4f}\"\n",
    "            \n",
    "            if self.metrics:\n",
    "                for i, metric in enumerate(self.metrics):\n",
    "                    train_results += f\", {metric.name} = {self.train_metrics[i].result():.4f}\"\n",
    "            \n",
    "            if val_dataset is not None:\n",
    "                train_results += f\" | Val Loss = {self.val_loss.result():.4f}\"\n",
    "                if self.metrics:\n",
    "                    for i, metric in enumerate(self.metrics):\n",
    "                        train_results += f\", Val {metric.name} = {self.val_metrics[i].result():.4f}\"\n",
    "            \n",
    "            print(train_results)\n",
    "        \n",
    "        self.epoch += 1\n",
    "    \n",
    "    def fit(self, train_dataset, val_dataset=None, epochs=10, callbacks=None):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Run callbacks\n",
    "            if callbacks:\n",
    "                for callback in callbacks:\n",
    "                    callback.on_epoch_begin(epoch)\n",
    "            \n",
    "            # Train epoch\n",
    "            self.train_epoch(train_dataset, val_dataset)\n",
    "            \n",
    "            # Run callbacks\n",
    "            if callbacks:\n",
    "                logs = {\n",
    "                    'loss': self.train_loss.result(),\n",
    "                    'val_loss': self.val_loss.result() if val_dataset else None\n",
    "                }\n",
    "                for callback in callbacks:\n",
    "                    callback.on_epoch_end(epoch, logs)\n",
    "\n",
    "# Test experimental training loop\n",
    "print(\"\\n=== Testing Experimental Training Loop ===\")\n",
    "\n",
    "# Create synthetic dataset\n",
    "def create_dataset(x, y, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "# Generate synthetic data\n",
    "X_train = tf.random.normal([1000, 20])\n",
    "y_train = tf.random.uniform([1000], 0, 3, dtype=tf.int32)\n",
    "X_val = tf.random.normal([200, 20])\n",
    "y_val = tf.random.uniform([200], 0, 3, dtype=tf.int32)\n",
    "\n",
    "train_ds = create_dataset(X_train, y_train)\n",
    "val_ds = create_dataset(X_val, y_val)\n",
    "\n",
    "# Create model and trainer\n",
    "experimental_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "trainer = ExperimentalTrainer(\n",
    "    model=experimental_model,\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss_fn=keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=[keras.metrics.sparse_categorical_accuracy]\n",
    ")\n",
    "\n",
    "# Setup advanced features\n",
    "trainer.setup_ema(decay=0.9999)\n",
    "trainer.gradient_accumulation_steps = 2\n",
    "trainer.max_grad_norm = 1.0\n",
    "\n",
    "# Create learning rate scheduler\n",
    "lr_scheduler = CosineAnnealingWarmRestarts(T_0=5, T_mult=2, eta_max=0.001, eta_min=1e-6)\n",
    "\n",
    "# Train model\n",
    "print(\"Starting experimental training...\")\n",
    "trainer.fit(train_ds, val_ds, epochs=3, callbacks=[lr_scheduler])\n",
    "\n",
    "print(\"Experimental training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This comprehensive notebook demonstrates cutting-edge research implementations using tf.keras, covering:\n",
    "\n",
    "**Advanced Attention Mechanisms:**\n",
    "- Linear attention for efficient long-sequence processing with O(n) complexity\n",
    "- Sparse attention with configurable sparsity patterns (local, strided, random)\n",
    "- Cross-modal attention for vision-language tasks\n",
    "\n",
    "**Novel Architectural Components:**\n",
    "- Squeeze-and-Excitation (SE) blocks for channel attention\n",
    "- Spatial Pyramid Pooling (SPP) for multi-scale feature extraction\n",
    "- Feature Pyramid Networks (FPN) for object detection\n",
    "- Efficient Channel Attention (ECA) without dimensionality reduction\n",
    "- Depthwise separable convolutions for mobile architectures\n",
    "\n",
    "**Advanced Training Techniques:**\n",
    "- Self-supervised learning with SimCLR contrastive loss\n",
    "- Momentum encoders for MoCo framework\n",
    "- DropBlock and Stochastic Depth regularization\n",
    "- Mixup data augmentation and label smoothing\n",
    "\n",
    "**Optimization Innovations:**\n",
    "- Lookahead optimizer for improved convergence\n",
    "- Cosine annealing with warm restarts\n",
    "- Gradient centralization techniques\n",
    "- Cyclical learning rate schedules\n",
    "\n",
    "**Model Interpretability:**\n",
    "- Integrated gradients for attribution analysis\n",
    "- Local linear approximation (LIME-like) explanations\n",
    "- Attention weight visualization\n",
    "- Permutation-based feature importance\n",
    "\n",
    "**Experimental Training Framework:**\n",
    "- Custom training loops with gradient accumulation\n",
    "- Mixed precision training support\n",
    "- Exponential moving average (EMA) of weights\n",
    "- Advanced gradient clipping and monitoring\n",
    "\n",
    "These implementations represent state-of-the-art techniques from recent research papers, providing a foundation for developing next-generation deep learning models. The modular design allows for easy experimentation and integration into existing projects, while the comprehensive testing ensures reliability and correctness.\n",
    "\n",
    "The notebook serves as both a learning resource and a practical toolkit for researchers and practitioners working on advanced machine learning problems, demonstrating how to implement and utilize cutting-edge techniques effectively within the TensorFlow/Keras ecosystem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
