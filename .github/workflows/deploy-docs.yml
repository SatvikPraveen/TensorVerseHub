# Location: /.github/workflows/deploy-docs.yml

name: Deploy Documentation

on:
  push:
    branches: [ main ]
    paths: 
      - 'docs/**'
      - 'notebooks/**'
      - 'src/**'
      - 'scripts/generate_docs.py'
  pull_request:
    branches: [ main ]
    paths:
      - 'docs/**'
      - 'notebooks/**'
      - 'scripts/generate_docs.py'
  workflow_dispatch:
    inputs:
      force_rebuild:
        description: 'Force rebuild all documentation'
        required: false
        default: false
        type: boolean
  schedule:
    # Rebuild docs daily at 3 AM UTC to catch any updates
    - cron: '0 3 * * *'

env:
  PYTHON_VERSION: '3.9'
  NODE_VERSION: '18'

jobs:
  generate-docs:
    name: Generate Documentation
    runs-on: ubuntu-latest
    
    outputs:
      docs-changed: ${{ steps.changes.outputs.docs }}
      
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full history for git diff
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y graphviz pandoc
    
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-docs-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-docs-
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install tensorflow==2.13.0
        pip install nbformat nbconvert jupyter
        pip install markdown beautifulsoup4
        pip install -r requirements.txt
    
    - name: Install Node.js dependencies for documentation
      run: |
        npm install -g mermaid-cli
        npm install -g @mermaid-js/mermaid-cli
    
    - name: Check for documentation changes
      id: changes
      run: |
        if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.event.inputs.force_rebuild }}" == "true" ]]; then
          echo "docs=true" >> $GITHUB_OUTPUT
          echo "Force rebuild requested"
        elif [[ "${{ github.event_name }}" == "schedule" ]]; then
          echo "docs=true" >> $GITHUB_OUTPUT
          echo "Scheduled rebuild"
        elif git diff --name-only HEAD~1 | grep -E "(docs/|notebooks/|src/|scripts/generate_docs.py)"; then
          echo "docs=true" >> $GITHUB_OUTPUT
          echo "Documentation-related files changed"
        else
          echo "docs=false" >> $GITHUB_OUTPUT
          echo "No documentation changes detected"
        fi
    
    - name: Generate API documentation
      if: steps.changes.outputs.docs == 'true'
      run: |
        echo "üöÄ Generating comprehensive documentation..."
        python scripts/generate_docs.py \
          --project-root . \
          --output-dir generated_docs
    
    - name: Process notebooks for documentation
      if: steps.changes.outputs.docs == 'true'
      run: |
        echo "üìö Processing notebooks..."
        find notebooks/ -name "*.ipynb" -not -path "*/.*" | while read notebook; do
          echo "Processing: $notebook"
          # Convert notebooks to HTML for web display
          output_dir="generated_docs/notebooks/$(dirname ${notebook#notebooks/})"
          mkdir -p "$output_dir"
          
          jupyter nbconvert \
            --to html \
            --output-dir="$output_dir" \
            --ExecutePreprocessor.timeout=300 \
            --ExecutePreprocessor.kernel_name=python3 \
            "$notebook" \
            2>/dev/null || echo "Warning: Failed to execute $notebook"
        done
    
    - name: Generate Mermaid diagrams
      if: steps.changes.outputs.docs == 'true'
      run: |
        echo "üé® Generating diagrams..."
        # Find and convert Mermaid diagrams in markdown files
        find generated_docs/ -name "*.md" -exec grep -l "```mermaid" {} \; | while read file; do
          echo "Processing diagrams in: $file"
          # Extract and convert mermaid diagrams (simplified)
          python scripts/process_mermaid.py "$file" || echo "Warning: Mermaid processing failed for $file"
        done
    
    - name: Create documentation index
      if: steps.changes.outputs.docs == 'true'
      run: |
        echo "üìÑ Creating documentation index..."
        cat > generated_docs/sitemap.xml << 'EOF'
        <?xml version="1.0" encoding="UTF-8"?>
        <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
          <url>
            <loc>https://tensorversehub.github.io/</loc>
            <lastmod>$(date -I)</lastmod>
            <changefreq>daily</changefreq>
            <priority>1.0</priority>
          </url>
        EOF
        
        find generated_docs -name "*.html" | while read file; do
          path=${file#generated_docs/}
          echo "  <url>" >> generated_docs/sitemap.xml
          echo "    <loc>https://tensorversehub.github.io/$path</loc>" >> generated_docs/sitemap.xml
          echo "    <lastmod>$(date -I)</lastmod>" >> generated_docs/sitemap.xml
          echo "    <changefreq>weekly</changefreq>" >> generated_docs/sitemap.xml
          echo "  </url>" >> generated_docs/sitemap.xml
        done
        
        echo "</urlset>" >> generated_docs/sitemap.xml
    
    - name: Optimize documentation assets
      if: steps.changes.outputs.docs == 'true'
      run: |
        echo "‚ö° Optimizing documentation assets..."
        
        # Compress images if any
        find generated_docs -name "*.png" -exec optipng -o7 {} \; 2>/dev/null || true
        find generated_docs -name "*.jpg" -o -name "*.jpeg" | head -10 | xargs -I {} jpegoptim --max=85 {} 2>/dev/null || true
        
        # Minify HTML files
        find generated_docs -name "*.html" | head -20 | while read file; do
          python -c "
import re
with open('$file', 'r') as f: content = f.read()
content = re.sub(r'\s+', ' ', content)
content = re.sub(r'> <', '><', content)
with open('$file', 'w') as f: f.write(content)
" 2>/dev/null || echo "HTML minification skipped for $file"
        done
    
    - name: Generate search index
      if: steps.changes.outputs.docs == 'true'
      run: |
        echo "üîç Generating search index..."
        python << 'EOF'
import os
import json
import re
from pathlib import Path

search_index = []

for html_file in Path('generated_docs').rglob('*.html'):
    try:
        with open(html_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Extract title
        title_match = re.search(r'<title[^>]*>([^<]+)</title>', content, re.IGNORECASE)
        title = title_match.group(1) if title_match else html_file.stem
        
        # Extract text content (simplified)
        text_content = re.sub(r'<[^>]+>', ' ', content)
        text_content = ' '.join(text_content.split())[:500]  # First 500 chars
        
        search_index.append({
            'title': title,
            'url': str(html_file.relative_to('generated_docs')),
            'content': text_content
        })
    except Exception as e:
        print(f"Error processing {html_file}: {e}")

with open('generated_docs/search_index.json', 'w') as f:
    json.dump(search_index, f, indent=2)

print(f"Search index created with {len(search_index)} entries")
EOF
    
    - name: Validate generated documentation
      if: steps.changes.outputs.docs == 'true'
      run: |
        echo "‚úÖ Validating documentation..."
        
        # Check if main index exists
        if [[ ! -f "generated_docs/index.html" ]]; then
          echo "‚ùå Missing main index.html"
          exit 1
        fi
        
        # Count generated files
        html_count=$(find generated_docs -name "*.html" | wc -l)
        echo "üìä Generated $html_count HTML files"
        
        if [[ $html_count -lt 5 ]]; then
          echo "‚ö†Ô∏è  Warning: Very few HTML files generated"
        fi
        
        # Check for broken internal links (simplified)
        find generated_docs -name "*.html" -exec grep -H "href=\"[^http]" {} \; | head -10 | while read link; do
          echo "üîó Found internal link: $link"
        done
    
    - name: Upload documentation artifacts
      if: steps.changes.outputs.docs == 'true'
      uses: actions/upload-artifact@v3
      with:
        name: documentation
        path: generated_docs/
        retention-days: 30
    
    - name: Upload documentation to Pages staging
      if: steps.changes.outputs.docs == 'true' && github.event_name == 'pull_request'
      uses: actions/upload-pages-artifact@v2
      with:
        name: documentation-preview
        path: generated_docs/

  deploy-github-pages:
    name: Deploy to GitHub Pages
    runs-on: ubuntu-latest
    needs: generate-docs
    if: needs.generate-docs.outputs.docs-changed == 'true' && github.ref == 'refs/heads/main'
    
    permissions:
      pages: write
      id-token: write
    
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
    - name: Download documentation
      uses: actions/download-artifact@v3
      with:
        name: documentation
        path: docs_output
    
    - name: Setup Pages
      uses: actions/configure-pages@v3
    
    - name: Upload to GitHub Pages
      uses: actions/upload-pages-artifact@v2
      with:
        path: docs_output
    
    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v2

  deploy-netlify:
    name: Deploy to Netlify
    runs-on: ubuntu-latest
    needs: generate-docs
    if: needs.generate-docs.outputs.docs-changed == 'true'
    
    steps:
    - name: Download documentation
      uses: actions/download-artifact@v3
      with:
        name: documentation
        path: docs_output
    
    - name: Deploy to Netlify
      uses: nwtgck/actions-netlify@v2
      with:
        publish-dir: './docs_output'
        production-branch: main
        github-token: ${{ secrets.GITHUB_TOKEN }}
        deploy-message: "Deploy from GitHub Actions - ${{ github.event.head_commit.message }}"
        enable-pull-request-comment: true
        enable-commit-comment: true
        overwrites-pull-request-comment: true
      env:
        NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
        NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}
      timeout-minutes: 10

  lighthouse-audit:
    name: Lighthouse Performance Audit
    runs-on: ubuntu-latest
    needs: [deploy-github-pages]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Wait for deployment
      run: sleep 60  # Wait for GitHub Pages deployment to be live
    
    - name: Run Lighthouse CI
      uses: treosh/lighthouse-ci-action@v10
      with:
        urls: |
          https://tensorversehub.github.io/
          https://tensorversehub.github.io/docs/
          https://tensorversehub.github.io/notebooks/
        configPath: '.github/lighthouse/lighthouse-ci.json'
        uploadArtifacts: true
        temporaryPublicStorage: true

  update-search-engine:
    name: Update Search Engine
    runs-on: ubuntu-latest
    needs: [deploy-github-pages]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Submit sitemap to search engines
      run: |
        # Submit to Google
        curl -X GET "https://www.google.com/ping?sitemap=https://tensorversehub.github.io/sitemap.xml" || echo "Google sitemap submission failed"
        
        # Submit to Bing
        curl -X GET "https://www.bing.com/ping?sitemap=https://tensorversehub.github.io/sitemap.xml" || echo "Bing sitemap submission failed"
        
        echo "Search engine submissions completed"

  documentation-preview:
    name: Documentation Preview
    runs-on: ubuntu-latest
    needs: generate-docs
    if: needs.generate-docs.outputs.docs-changed == 'true' && github.event_name == 'pull_request'
    
    steps:
    - name: Download documentation
      uses: actions/download-artifact@v3
      with:
        name: documentation
        path: docs_preview
    
    - name: Create preview comment
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Count files
          function countFiles(dir, ext) {
            const files = fs.readdirSync(dir, { recursive: true });
            return files.filter(file => file.endsWith(ext)).length;
          }
          
          const htmlFiles = countFiles('docs_preview', '.html');
          const totalSize = fs.statSync('docs_preview').size;
          
          const comment = `## üìö Documentation Preview
          
          Documentation has been generated successfully!
          
          ### üìä Statistics
          - **HTML Files**: ${htmlFiles}
          - **Total Size**: ~${Math.round(totalSize/1024)}KB
          
          ### üîó Preview Links
          - [Main Documentation](https://deploy-preview-${context.issue.number}--tensorversehub.netlify.app/)
          - [API Reference](https://deploy-preview-${context.issue.number}--tensorversehub.netlify.app/api/)
          - [Notebook Gallery](https://deploy-preview-${context.issue.number}--tensorversehub.netlify.app/notebooks/)
          
          ### ‚úÖ Checks
          - [x] Documentation generated
          - [x] HTML validation passed
          - [x] Search index created
          - [x] Assets optimized
          
          *Documentation will be updated automatically when you push changes to this PR.*`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  cleanup:
    name: Cleanup Artifacts
    runs-on: ubuntu-latest
    needs: [deploy-github-pages, deploy-netlify]
    if: always()
    
    steps:
    - name: Delete old artifacts
      uses: actions/github-script@v6
      with:
        script: |
          const artifacts = await github.rest.actions.listArtifactsForRepo({
            owner: context.repo.owner,
            repo: context.repo.repo,
            per_page: 100
          });
          
          const oldArtifacts = artifacts.data.artifacts
            .filter(artifact => artifact.name === 'documentation')
            .filter(artifact => {
              const createdAt = new Date(artifact.created_at);
              const weekAgo = new Date(Date.now() - 7 * 24 * 60 * 60 * 1000);
              return createdAt < weekAgo;
            });
          
          for (const artifact of oldArtifacts) {
            console.log(`Deleting artifact: ${artifact.name} (${artifact.created_at})`);
            await github.rest.actions.deleteArtifact({
              owner: context.repo.owner,
              repo: context.repo.repo,
              artifact_id: artifact.id
            });
          }

  notify-team:
    name: Notify Team
    runs-on: ubuntu-latest
    needs: [deploy-github-pages]
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
    - name: Send Slack notification
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ needs.deploy-github-pages.result }}
        channel: '#tensorversehub-docs'
        fields: repo,message,commit,author,took
        custom_payload: |
          {
            attachments: [{
              color: '${{ needs.deploy-github-pages.result }}' === 'success' ? 'good' : 'danger',
              title: 'Documentation Deployment',
              title_link: 'https://tensorversehub.github.io/',
              text: '${{ needs.deploy-github-pages.result }}' === 'success' ? 
                    'üìö Documentation updated successfully!' : 
                    '‚ùå Documentation deployment failed',
              fields: [{
                title: 'Repository',
                value: '${{ github.repository }}',
                short: true
              }, {
                title: 'Commit',
                value: '${{ github.sha }}',
                short: true
              }]
            }]
          }
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}